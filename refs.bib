@article{goto2008anatomy,
author = {Goto, Kazushige and Geijn, Robert A. van de},
title = {Anatomy of High-Performance Matrix Multiplication},
year = {2008},
issue_date = {May 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/1356052.1356053},
doi = {10.1145/1356052.1356053},
journal = {ACM Trans. Math. Softw.},
month = may,
articleno = {Article 12},
numpages = {25},
keywords = {matrix multiplication, Linear algebra, basic linear algebra subprogrms}
}

@online{ibmarchive,
title = {{IBM - Archives - History of IBM - United States}},
organization = {International Business Machines},
date = {2003-01-23},
urldate = {2020-09-15},
url = {https://www.ibm.com/ibm/history/history/history_intro.html}
}

@article{montoye1990design,
author={R. K. {Montoye} and E. {Hokenek} and S. L. {Runyon}},
journal={IBM Journal of Research and Development},
title={Design of the IBM RISC System/6000 floating-point execution unit},
year={1990},
volume={34},
number={1},
pages={59-70}
}

@article{tomasulo1967efficient,
author={R. M. {Tomasulo}},
journal={IBM Journal of Research and Development},
title={An Efficient Algorithm for Exploiting Multiple Arithmetic Units},
year={1967},
volume={11},
number={1},
pages={25-33}
}

@inproceedings{blue1992training,
author = {James L. Blue and Patrick J. Grother},
title = {{Training feed-forward neural networks using conjugate gradients}},
volume = {1661},
booktitle = {Machine Vision Applications in Character Recognition and Industrial Inspection},
editor = {Donald P. D'Amato and Wolf-Ekkehard Blanz and Byron E. Dom and Sargur N. Srihari},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {179 -- 190},
year = {1992},
doi = {10.1117/12.130286},
URL = {https://doi.org/10.1117/12.130286}
}

@book{rojas1996neural,
title={Neural networks: a systematic introduction},
author={Rojas, Ra{\'u}l},
year={1996},
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
isbn="978-3-642-61068-4",
doi="10.1007/978-3-642-61068-4_7",
url="https://doi.org/10.1007/978-3-642-61068-4_7"
}

@article{flynn1972some,
author={M. J. {Flynn}},
journal={IEEE Transactions on Computers},
title={Some Computer Organizations and Their Effectiveness},
year={1972},
volume={C-21},
number={9},
pages={948-960},
doi={10.1109/TC.1972.5009071}
}

@article{barnes1968illiac,
author={G. H. {Barnes} and R. M. {Brown} and M. {Kato} and D. J. {Kuck} and D. L. {Slotnick} and R. A. {Stokes}},
journal={IEEE Transactions on Computers},
title={{The ILLIAC IV Computer}},
year={1968},
volume={C-17},
number={8},
pages={746-757},
doi={10.1109/TC.1968.229158}
}

@INPROCEEDINGS{tyler1999altivec,
author={J. {Tyler} and J. {Lent} and A. {Mather} and  {Huy Nguyen}},
booktitle={1999 IEEE International Performance, Computing and Communications Conference (Cat. No.99CH36305)},
title={{AltiVec/sup TM/: bringing vector technology to the PowerPC/sup TM/ processor family}},
year={1999},
volume={},
number={},
pages={437-444},
doi={10.1109/PCCC.1999.749469}
}

@online{llvmLangref,
  author = {{\relax LLVM} Foundation},
  title = {{LLVM} Language Reference Manual},
  year = 2020,
  url = {https://llvm.org/docs/LangRef.html},
  urldate = {2021-01-04}
}

@online{llvmCheckArith,
  author = {{\relax LLVM} Foundation},
  title = {Clang Language Extensions},
  year = 2020,
  url = {https://clang.llvm.org/docs/LanguageExtensions.html#checked-arithmetic-builtins},
  urldate = {2021-01-04}
}

@online{gccOtherBuiltins,
  author = {{GNU}},
  title = {Other Built-in Functions Provided by GCC},
  year = 2020,
  url = {https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html},
  urldate = {2021-01-04}
}

@unsubmitted{kuzma2021fast,
  author={Braedy Kuzma and Ivan Korostelev and João P. L. de Carvalho and José Moreira and Christopher Barton and Guido Araujo and José Nelson Amaral},
  title = {Fast Matrix Multiplication via Compiler-only Layered Data Reorganization and Intrinsic Lowering},
  note = {submitted}
}

@ARTICLE{eisen2007ibm,
  author={Eisen, L. and Ward, J. W. and Tast, H.-W. and Mading, N. and Leenstra, J. and Mueller, S. M. and Jacobi, C. and Preiss, J. and Schwarz, E. M. and Carlough, S. R.},
  journal={IBM Journal of Research and Development},
  title={IBM POWER6 accelerators: VMX and DFU},
  year={2007},
  volume={51},
  number={6},
  pages={1-21},
  doi={10.1147/rd.516.0663}
}

@MastersThesis{lattner2002llvm,
  author  = {Chris Lattner},
  title   = "{LLVM: An Infrastructure for Multi-Stage Optimization}",
  school  = "{Computer Science Dept., University of Illinois at Urbana-Champaign}",
  year    = {2002},
  address = {Urbana, IL},
  month   = {Dec},
  note    = {{\em See {\tt http://llvm.cs.uiuc.edu}.}}
}

@INPROCEEDINGS{lattner2004llvm,
  author={Lattner, C. and Adve, V.},
  booktitle={International Symposium on Code Generation and Optimization, 2004. CGO 2004.},
  title={LLVM: a compilation framework for lifelong program analysis transformation},
  year={2004},
  volume={},
  number={},
  pages={75-86},
  doi={10.1109/CGO.2004.1281665}
}

@inproceedings{grosser2011polly,
  title={Polly-Polyhedral optimization in LLVM},
  author={Grosser, Tobias and Zheng, Hongbin and Aloor, Raghesh and Simb{\"u}rger, Andreas and Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l},
  booktitle={Proceedings of the First International Workshop on Polyhedral Compilation Techniques (IMPACT)},
  volume={2011},
  pages={1},
  year={2011}
}

@inproceedings{10.1145/2814270.2814285,
author = {Alves, P\'{e}ricles and Gruber, Fabian and Doerfert, Johannes and Lamprineas, Alexandros and Grosser, Tobias and Rastello, Fabrice and Pereira, Fernando Magno Quint\~{a}o},
title = {Runtime Pointer Disambiguation},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814285},
doi = {10.1145/2814270.2814285},
abstract = { To optimize code effectively, compilers must deal with memory dependencies. However, the state-of-the-art heuristics available in the literature to track memory dependencies are inherently imprecise and computationally expensive. Consequently, the most advanced code transformations that compilers have today are ineffective when applied on real-world programs. The goal of this paper is to solve this conundrum through dynamic disambiguation of pointers. We provide different ways to determine at runtime when two memory locations can overlap. We then produce two versions of a code region: one that is aliasing-free - hence, easy to optimize - and another that is not. Our checks let us safely branch to the optimizable region. We have applied these ideas on Polly-LLVM, a loop optimizer built on top of the LLVM compilation infrastructure. Our experiments indicate that our method is precise, effective and useful: we can disambiguate every pair of pointer in the loop intensive Polybench benchmark suite. The result of this precision is code quality: the binaries we generate are 10% faster than those that Polly-LLVM produces without our optimization, at the -O3 optimization level of LLVM. },
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {589–606},
numpages = {18},
keywords = {optimization, dynamic guards, Alias analysis},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}

@article{alves2015runtime,
author = {Alves, P\'{e}ricles and Gruber, Fabian and Doerfert, Johannes and Lamprineas, Alexandros and Grosser, Tobias and Rastello, Fabrice and Pereira, Fernando Magno Quint\~{a}o},
title = {Runtime Pointer Disambiguation},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858965.2814285},
doi = {10.1145/2858965.2814285},
journal = {SIGPLAN Not.},
month = oct,
pages = {589–606},
numpages = {18},
keywords = {Alias analysis, dynamic guards, optimization}
}

@inproceedings{sui2016interprocedural,
author = {Sui, Yulei and Xue, Jingling},
title = {SVF: Interprocedural Static Value-Flow Analysis in LLVM},
year = {2016},
isbn = {9781450342414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892208.2892235},
doi = {10.1145/2892208.2892235},
booktitle = {Proceedings of the 25th International Conference on Compiler Construction},
pages = {265–266},
numpages = {2},
keywords = {Pointer Analysis, Value-Flow, SVF},
location = {Barcelona, Spain},
series = {CC 2016}
}

@inproceedings{hardekopf2009semi,
author = {Hardekopf, Ben and Lin, Calvin},
title = {Semi-Sparse Flow-Sensitive Pointer Analysis},
year = {2009},
isbn = {9781605583792},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1480881.1480911},
doi = {10.1145/1480881.1480911},
booktitle = {Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {226–238},
numpages = {13},
keywords = {pointer analysis, alias analysis},
location = {Savannah, GA, USA},
series = {POPL '09}
}

@article{lozano2019combinatorial,
author = {Lozano, Roberto Casta\~{n}eda and Carlsson, Mats and Blindell, Gabriel Hjort and Schulte, Christian},
title = {Combinatorial Register Allocation and Instruction Scheduling},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/3332373},
doi = {10.1145/3332373},
journal = {ACM Trans. Program. Lang. Syst.},
month = jul,
articleno = {17},
numpages = {53},
keywords = {register allocation, instruction scheduling, Combinatorial optimization}
}

@inproceedings{10.1145/1375581.1375609,
author = {Quint\~{a}o Pereira, Fernando Magno and Palsberg, Jens},
title = {Register Allocation by Puzzle Solving},
year = {2008},
isbn = {9781595938602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1375581.1375609},
doi = {10.1145/1375581.1375609},
abstract = {We show that register allocation can be viewed as solving a collection of puzzles. We model the register file as a puzzle board and the program variables as puzzle pieces; pre-coloring and register aliasing fit in naturally. For architectures such as PowerPC, x86, and StrongARM, we can solve the puzzles in polynomial time, and we have augmented the puzzle solver with a simple heuristic for spilling. For SPEC CPU2000, the compilation time of our implementation is as fast as that of the extended version of linear scan used by LLVM, which is the JIT compiler in the openGL stack of Mac OS 10.5. Our implementation produces x86 code that is of similar quality to the code produced by the slower, state-of-the-art iterated register coalescing of George and Appel with the extensions proposed by Smith, Ramsey, and Holloway in 2004.},
booktitle = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {216–226},
numpages = {11},
keywords = {puzzle solving, register allocation, register aliasing},
location = {Tucson, AZ, USA},
series = {PLDI '08}
}

@article{pereira2008register,
author = {Quint\~{a}o Pereira, Fernando Magno and Palsberg, Jens},
title = {Register Allocation by Puzzle Solving},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1379022.1375609},
doi = {10.1145/1379022.1375609},
abstract = {We show that register allocation can be viewed as solving a collection of puzzles. We model the register file as a puzzle board and the program variables as puzzle pieces; pre-coloring and register aliasing fit in naturally. For architectures such as PowerPC, x86, and StrongARM, we can solve the puzzles in polynomial time, and we have augmented the puzzle solver with a simple heuristic for spilling. For SPEC CPU2000, the compilation time of our implementation is as fast as that of the extended version of linear scan used by LLVM, which is the JIT compiler in the openGL stack of Mac OS 10.5. Our implementation produces x86 code that is of similar quality to the code produced by the slower, state-of-the-art iterated register coalescing of George and Appel with the extensions proposed by Smith, Ramsey, and Holloway in 2004.},
journal = {SIGPLAN Not.},
month = jun,
pages = {216–226},
numpages = {11},
keywords = {register allocation, puzzle solving, register aliasing}
}

@inproceedings{cytron1989efficient,
  title={An efficient method of computing static single assignment form},
  author={Cytron, Ron and Ferrante, Jeanne and Rosen, Barry K and Wegman, Mark N and Zadeck, F Kenneth},
  booktitle={Proceedings of the 16th ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
  pages={25--35},
  year={1989}
}

@inproceedings{rosen1988global,
  title={Global value numbers and redundant computations},
  author={Rosen, Barry K and Wegman, Mark N and Zadeck, F Kenneth},
  booktitle={Proceedings of the 15th ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
  pages={12--27},
  year={1988}
}

@inproceedings{alpern1988detecting,
  title={Detecting equality of variables in programs},
  author={Alpern, Bowen and Wegman, Mark N and Zadeck, F Kenneth},
  booktitle={Proceedings of the 15th ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
  pages={1--11},
  year={1988}
}

@article{cytron1991efficiently,
author = {Cytron, Ron and Ferrante, Jeanne and Rosen, Barry K. and Wegman, Mark N. and Zadeck, F. Kenneth},
title = {Efficiently Computing Static Single Assignment Form and the Control Dependence Graph},
year = {1991},
issue_date = {Oct. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/115372.115320},
doi = {10.1145/115372.115320},
journal = {ACM Trans. Program. Lang. Syst.},
month = oct,
pages = {451–490},
numpages = {40},
keywords = {optimizing compilers, def-use chain, dominator, control dependence, control flow graph}
}

@article{brandis1994single,
author = {Brandis, Marc M. and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Single-Pass Generation of Static Single-Assignment Form for Structured Languages},
year = {1994},
issue_date = {Nov. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {6},
issn = {0164-0925},
url = {https://doi.org/10.1145/197320.197331},
doi = {10.1145/197320.197331},
journal = {ACM Trans. Program. Lang. Syst.},
month = nov,
pages = {1684–1698},
numpages = {15},
keywords = {static single-assignment form, structured languages, dominator tree}
}

@inproceedings{10.1145/800028.808480,
author = {Cocke, John},
title = {Global Common Subexpression Elimination},
year = {1970},
isbn = {9781450373869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800028.808480},
doi = {10.1145/800028.808480},
abstract = {When considering compiler optimization, there are two questions that immediately come to mind; one, why and to what extent is optimization necessary and two, to what extent is it possible.When considering the second question, one might immediately become discouraged since it is well known that the program equivalency problem is recursively unsolvable. It is, of course, clear from this that there will never be techniques for generating a completely optimum program. These unsolvability results, however, do not preclude the possibility of ad hoc techniques for program improvement or even a partial theory which produces a class of equivalent programs optimized in varying degrees.The reasons why optimization is required seem to me to fall in two major categories. The first I will call “local” and the second “global”.},
booktitle = {Proceedings of a Symposium on Compiler Optimization},
pages = {20–24},
numpages = {5},
location = {Urbana-Champaign, Illinois}
}

@article{cocke1970global,
author = {Cocke, John},
title = {Global Common Subexpression Elimination},
year = {1970},
issue_date = {July 1970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {7},
issn = {0362-1340},
url = {https://doi.org/10.1145/390013.808480},
doi = {10.1145/390013.808480},
journal = {SIGPLAN Not.},
month = jul,
pages = {20–24},
numpages = {5}
}

@article{domke2020matrix,
  author    = {Jens Domke and
               Emil Vatai and
               Aleksandr Drozd and
               Peng Chen and
               Yosuke Oyama and
               Lingqi Zhang and
               Shweta Salaria and
               Daichi Mukunoki and
               Artur Podobas and
               Mohamed Wahib and
               Satoshi Matsuoka},
  title     = {Matrix Engines for High Performance Computing: {A} Paragon of Performance
               or Grasping at Straws?},
  journal   = {CoRR},
  volume    = {abs/2010.14373},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.14373},
  archivePrefix = {arXiv},
  eprint    = {2010.14373},
  timestamp = {Mon, 02 Nov 2020 18:17:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-14373.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2019bfloat16,
  title={Bfloat16: the secret to high performance on cloud tpus},
  author={Wang, Shibo and Kanwar, Pankaj},
  journal={Google Cloud Blog},
  year={2019}
}

@article{nakasato2011fast,
author = {Nakasato, Naohito},
title = {A Fast GEMM Implementation on the Cypress GPU},
year = {2011},
issue_date = {March 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/1964218.1964227},
doi = {10.1145/1964218.1964227},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = mar,
pages = {50–55},
numpages = {6}
}

@InProceedings{yu2020toward,
author="Yu, Tan
and Cai, Yunfeng
and Li, Ping",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Toward Faster and Simpler Matrix Normalization via Rank-1 Update",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="203--219",
isbn="978-3-030-58529-7"
}

@INPROCEEDINGS{pal2018outerspace,
author={Pal, Subhankar and Beaumont, Jonathan and Park, Dong-Hyeon and Amarnath, Aporva and Feng, Siying and Chakrabarti, Chaitali and Kim, Hun-Seok and Blaauw, David and Mudge, Trevor and Dreslinski, Ronald},
booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
title={OuterSPACE: An Outer Product Based Sparse Matrix Multiplication Accelerator},
year={2018},
volume={},
number={},
pages={724-736},
doi={10.1109/HPCA.2018.00067}
}

@INPROCEEDINGS{srivastava2020matraptor,
author={Srivastava, Nitish and Jin, Hanchen and Liu, Jie and Albonesi, David and Zhang, Zhiru},
booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
title={MatRaptor: A Sparse-Sparse Matrix Multiplication Accelerator Based on Row-Wise Product},
year={2020},
volume={},
number={},
pages={766-780},
doi={10.1109/MICRO50266.2020.00068}
}

@article{wu2016achieving,
author = {Wu, Jing and Jaja, Joseph},
title = {Achieving Native GPU Performance for Out-of-Card Large Dense Matrix Multiplication},
journal = {Parallel Processing Letters},
volume = {26},
number = {02},
pages = {1650007},
year = {2016},
doi = {10.1142/S0129626416500079},
URL = {https://doi.org/10.1142/S0129626416500079},
eprint = {https://doi.org/10.1142/S0129626416500079}
}

@InProceedings{waugh2020use,
author="Waugh, Harry
and McIntosh-Smith, Simon",
editor="Nichols, Jeffrey
and Verastegui, Becky
and Maccabe, Arthur `Barney'
and Hernandez, Oscar
and Parete-Koon, Suzanne
and Ahearn, Theresa",
title="On the Use of BLAS Libraries in Modern Scientific Codes at Scale",
booktitle="Driving Scientific and Engineering Discoveries Through the Convergence of HPC, Big Data and AI",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="67--79",
abstract="As we approach the Exascale era, computer architectures are evolving ever-greater vector and matrix acceleration units---NVIDIA's Ampere Tensor Cores, Intel's AMX, and Arm's SVE vector instruction set developments are just three recent examples [1, 2, 10]. To exploit these, it is expected that optimised math libraries such as those for dense and sparse linear algebra, will play an increasing role in achieving optimal performance. It is therefore useful to understand which of these functions dominate an application's runtime, and in particular how this changes with increasing scale. This work aims to provide a contemporary dataset regarding how much dense linear algebra (BLAS) is used in HPC codes at scale. We have analysed several science codes widely used on the UK HPC service, ARCHER (https://www.archer.ac.uk), including CASTEP, CP2K, QuantumESPRESSO, and Nektar++. To capture demands from the AI community, we have additionally traced the training stage of the Convolutional Neural Network (CNN), AlexNet [7]. HPLinpack is also included as a reference, as it exhibits a well-understood BLAS usage pattern. Results from across all the codes show that, unlike HPLinpack, BLAS usage is never more than 25{\%} of the total runtime, even when running at a modest scale (32 nodes of the Arm-based supercomputer, Isambard). This presents limited speedup opportunity when considering Amdahl's law, and suggests that application developers may need to adjust their algorithms to spend more time in optimised BLAS libraries to capitalise on new architectures and accelerators.",
isbn="978-3-030-63393-6"
}

@inproceedings{abadi2016tensorflow,
author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
title = {TensorFlow: A System for Large-Scale Machine Learning},
booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {265--283},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
publisher = {{USENIX} Association},
month = nov,
}

@article{lawson1979basic,
  title={Basic linear algebra subprograms for Fortran usage},
  author={Lawson, Chuck L and Hanson, Richard J. and Kincaid, David R and Krogh, Fred T.},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={5},
  number={3},
  pages={308--323},
  year={1979},
  publisher={ACM New York, NY, USA}
}

@INPROCEEDINGS{2012xianyi,
author={Z. {Xianyi} and W. {Qian} and Z. {Yunquan}},
booktitle={2012 IEEE 18th International Conference on Parallel and Distributed Systems},
title={Model-driven Level 3 BLAS Performance Optimization on Loongson 3A Processor},
year={2012},
volume={},
number={},
pages={684-691},
doi={10.1109/ICPADS.2012.97},
ISSN={1521-9097},
month={Dec}
}


@article{zee2016blis,
author = {Zee, Field G. Van and Smith, Tyler M. and Marker, Bryan and Low, Tze Meng and Geijn, Robert A. Van De and Igual, Francisco D. and Smelyanskiy, Mikhail and Zhang, Xianyi and Kistler, Michael and Austel, Vernon and Gunnels, John A. and Killough, Lee},
title = {The BLIS Framework: Experiments in Portability},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/2755561},
doi = {10.1145/2755561},
abstract = {BLIS is a new software framework for instantiating high-performance BLAS-like dense linear algebra libraries. We demonstrate how BLIS acts as a productivity multiplier by using it to implement the level-3 BLAS on a variety of current architectures. The systems for which we demonstrate the framework include state-of-the-art general-purpose, low-power, and many-core architectures. We show, with very little effort, how the BLIS framework yields sequential and parallel implementations that are competitive with the performance of ATLAS, OpenBLAS (an effort to maintain and extend the GotoBLAS), and commercial vendor implementations such as AMD’s ACML, IBM’s ESSL, and Intel’s MKL libraries. Although most of this article focuses on single-core implementation, we also provide compelling results that suggest the framework’s leverage extends to the multithreaded domain.},
journal = {ACM Trans. Math. Softw.},
month = jun,
articleno = {12},
numpages = {19},
keywords = {Linear algebra, matrix, BLAS, multiplication, libraries, high performance}
}

@article{vanzee2015blis,
author = {Van Zee, Field G. and van de Geijn, Robert A.},
title = {BLIS: A Framework for Rapidly Instantiating BLAS Functionality},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/2764454},
doi = {10.1145/2764454},
abstract = {The BLAS-like Library Instantiation Software (BLIS) framework is a new infrastructure for rapidly instantiating Basic Linear Algebra Subprograms (BLAS) functionality. Its fundamental innovation is that virtually all computation within level-2 (matrix-vector) and level-3 (matrix-matrix) BLAS operations can be expressed and optimized in terms of very simple kernels. While others have had similar insights, BLIS reduces the necessary kernels to what we believe is the simplest set that still supports the high performance that the computational science community demands. Higher-level framework code is generalized and implemented in ISO C99 so that it can be reused and/or reparameterized for different operations (and different architectures) with little to no modification. Inserting high-performance kernels into the framework facilitates the immediate optimization of any BLAS-like operations which are cast in terms of these kernels, and thus the framework acts as a productivity multiplier. Users of BLAS-dependent applications are given a choice of using the traditional Fortran-77 BLAS interface, a generalized C interface, or any other higher level interface that builds upon this latter API. Preliminary performance of level-2 and level-3 operations is observed to be competitive with two mature open source libraries (OpenBLAS and ATLAS) as well as an established commercial product (Intel MKL).},
journal = {ACM Trans. Math. Softw.},
month = jun,
articleno = {14},
numpages = {33},
keywords = {libraries, BLAS, matrix, Linear algebra, high-performance}
}

@article{low2016analytical,
  title={Analytical modeling is enough for high-performance BLIS},
  author={Low, Tze Meng and Igual, Francisco D and Smith, Tyler M and Quintana-Orti, Enrique S},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={43},
  number={2},
  pages={1--18},
  year={2016},
  publisher={ACM New York, NY, USA}
}
