% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Background}
\label{cha:background}
This chapter introduces concepts that are necessary for subsequent chapters.
\gls{simd} architectures and vectorisation are concepts critical to every facet of the work.
This chapter also explains the \gls{llvm} framework, in which this work is implemented, as well as the \gls{ir} used in the \gls{llvm} framework.

\section{SIMD Architecture}
\label{sec:simd}
First introduced as part of the ILLIAC IV~\autocite{barnes1968illiac}, processors capable of \gls{simd} computation were a crucial advancement in high-performance computing history.
Flynn characterised a \gls{simd} processor as having the ability to apply a single ``master'' instruction over a vector of related operands~\autocite{flynn1972some}.
Such an architecture is desirable for its ability to increase the throughput of repeated operations through data-level parallelism and is thus highly applicable to one of the most standard computing control flow mechanisms: loops.
This process of increasing the throughput of loops, through what is essentially a reduction in the number of instructions executed, has developed into what is now the well-known and well-studied process of ``\gls{vectorisation}''.

\Gls{vectorisation} is the process by which a compiler merges multiple scalar operations into a single operation on a vector.
Unrolling a loop exposes repeated operations that can be rescheduled and grouped.
This extends loads and stores which, when combined, produce only a single (wider) memory operation.
Repeated memory requests are queued in a memory controller, waiting for earlier requests to complete before being started.
Requiring only a single request for the same amount of data can significantly reduce memory latency.

\section{The IBM Power Architecture}
\Gls{ibm} has decades of history in computing, tracing their roots back to the 1880s; first incorporated as the Computing-Tabulating-Recording Company in 1911, it was eventually renamed to \gls{ibm} in 1924~\autocite{ibmarchive}.
Throughout this history, \gls{ibm} has made a point of innovating and pioneering numerous technologies in hardware, software, and the intermingling of the two.
One such innovation was the \gls{power} \gls{isa}.

The \gls{power} \gls{isa} was first announced in 1990 along with its primary instantiation in the \gls{ibm} System/6000~\autocite{montoye1990design}.
The System/6000, a \gls{risc}, implemented new features such as register renaming and out-of-order execution via the Tomasulo algorithm~\autocite{tomasulo1967efficient}.
Previously these features were only available in the \gls{ibm} System/360 mainframe.

Years later, in August 2020, \gls{ibm} remains competitive in its technology offerings with the announcement of the tenth generation of the \gls{power} \gls{isa}, aptly named \gls{power10}.

\subsection{SIMD History in IBM Power}
Initially, the \gls{power} architecture implemented its \gls{simd} capabilities through a common standard named \gls{altivec}.
\Gls{altivec} was designed through a collaboration between \gls{ibm}, Apple, and Motorola and described a \gls{simd} instruction set for floating-point and integer values.
This implementation, presented under the name \gls{vmx}, was first instantiated by \gls{ibm} as part of \gls{power}6 (\gls{power} \gls{isa} v2.03) in 2007~\autocite{eisen2007ibm} despite the standard being presented in 1999~\autocite{tyler1999altivec}.

Improvements for \gls{power}7 (\gls{power} \gls{isa} v2.06) added a new facility, called \gls{vsx}, designed to add even further manipulation capabilities when dealing with vectors.
This included support for up to 64 vector registers, 64-bit integers, and double-precision floating point values.
Both \gls{vmx} and \gls{vsx} exist to this day in the most recent version of the \gls{isa} (\gls{power} \gls{isa} v3.1), though many refer to them collectively under the second name, \gls{vsx}.

\subsection{Matrix Math Assist}
\label{sec:mmaintro}
As part of the new \gls{power10}'s offerings, \gls{ibm} has implemented a new facility, dubbed \gls{mma}, into the \gls{isa}, continuing \gls{ibm}'s commitment to providing cutting edge hardware for the software needs of the current era.
\gls{mma} is an addition to \gls{power}'s preexisting \gls{vmx} and \gls{vsx} \gls{simd} facilities.
It provides a high-throughput and low-latency method for calculating matrix multiplications.

The state of the art for hardware that performs matrix multiplication falls into two categories:
\begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}, after={.}]
  \item via external components (\eg \gls{gpu}, \gls{fpga}, or \gls{asic} such as Google's \gls{tpu})
  \item via \gls{cpu} \gls{isa} extensions (\eg x86's \gls{amx} or ARM's NEON/\gls{sve})
\end{enumerate*}
\Gls{mma} belongs to the second category given that the facility is built directly into the \gls{cpu}.
Thus, the facility is amenable to tasks that would normally incur avoidable overhead, for example, while sending data to an external component such as a \gls{gpu}.
See \rcha{mma} for further discussion.

\section{LLVM}
\label{sec:llvm}
\Gls{llvm}, originally an initialism for Low-Level Virtual Machine, is a compilation framework devised by Chris Lattner~\autocite{lattner2002llvm,lattner2004llvm}.
Lattner originally positions \gls{llvm} as a ``unique multi-stage optimisation system'' that aims to ``support extensive inter-procedural and profile-driven optimisations, while being efficient enough for use in commercial compiler systems''~\autocite{lattner2002llvm}.
\Gls{llvm}, now a mononym, has evolved into a project covering a wide range of compilation-related tools including frontends for many languages, an optimiser, backends for many platforms, a \glslink{linking}{linker}, debugger, and several other projects.

Frontends exist for a multitude of languages including C/C++ (\code{clang}), Fortran (\code{flang}), Swift (\code{swiftc}), and Rust (\code{rustc}).
Backends also exist for a wide variety of platforms, such as X86, ARM, PowerPC, and WebAssembly.
The \gls{llvm} optimiser, named \code{opt}, is a popular target compiler research of all varieties, seeing advancements in various areas like register allocation~\autocite{lozano2019combinatorial,pereira2008register}, pointer analysis~\autocite{hardekopf2009semi,sui2016interprocedural}, and polyhedral optimisation~\autocite{grosser2011polly,alves2015runtime}.
The framework has also been adopted by several large companies who have based their own products off the ever-improving set of open-source tools.
These include \gls{ibm}'s XL C/C++ compiler and Nvidia's CUDA compiler.

\subsection{LLVM's Intermediate Representation}
\label{sec:ir}
The main pillar upon which the \gls{llvm}'s compilation pipeline is built is its \gls{ir}.
An \gls{ir} is a programming language in its own right, though its intended use is a frontend-language-agnostic and backend-target-agnostic intermediary language.
In this way, all frontends may target their \gls{lowering} toward producing a single shared language and all backends may consume a single shared language to produce their platform specific assembly.
Given this shared middle point, it is easy to create an optimiser which both consumes and produces the single intermediate language.
Thus, any combination of high-level language and destination platform may benefit from new or improved optimisations in the optimiser.

\Gls{llvm} \gls{ir} is known for being strongly typed and maintaining much of the high level type information from the original input.
It can also be annotated with large amounts of debugging information all while remaining easily serialisable.
A critical feature of the \gls{ir} is that it is in \gls{ssa} form.

\Gls{ssa} was formalised by Cytron \etal~\autocite{cytron1989efficient} though its use was seen as a side effect in previous works~\autocite{rosen1988global,alpern1988detecting}.
Efficient methods for its construction followed shortly after~\autocite{cytron1991efficiently,brandis1994single}.
Intuitively, the main property of a program in \gls{ssa} form is that each variable in the program text (static) is assigned to exactly once (single assignment); this property does not preclude executing an assignment multiple times at runtime, potentially with different values.

\noindent
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[caption={[Example C Program, Pre-SSA Conversion.]An simple example C program, pre-conversion to SSA.},
      label=lst:cSSA,numbers=none,language=c,columns=flexible]
int foo(int a) {
  int x = a + 2;
  int y = x + 10;
  x = y * 4;
  return x;
}
\end{lstlisting}
\end{minipage}
\hspace{.025\linewidth}
\noindent
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[caption={[Example LLVM IR Program, Post-SSA Conversion.]The same simple program, converted to LLVM IR in SSA format.},
      label=lst:llSSA,numbers=none,language=llvm,columns=flexible]
define i32 @foo(i32 %a) {
entry:
  %x.1 = add i32 %a, 2
  %y.1 = add i32 %x.1, 10
  %x.2 = mul i32 %y.1, 4
  ret i32 %x.2
}
\end{lstlisting}
\end{minipage}

A simple C program, shown in \rlst{cSSA} is converted to the SSA format in \rlst{llSSA} via a simple renaming scheme.
The first reference to a variable is suffixed with a one, and all future references increment the counter by one.
This process continues for the entirety of a function body, regardless of any control flow.
There are further complications in the construction algorithm in the presence of control flow but because the process presented in this thesis functions within a single \gls{basic block}, this explanation is left to Cytron \etal's work.
This format is significantly more conducive to analysis and optimisation than the source language, allowing for a much easier data flow analysis.

\end{document}
