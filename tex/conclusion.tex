% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Conclusion}
\label{cha:conclusion}
Beginning by highlighting code-improvement opportunities that are unrealisable while using an external library for computation, this thesis presents a performant solution to the attractive alternative of a compiler-only path to generate code to compute matrix-multiplication.
A comprehensive review of state-of-the-art matrix-multiplication micro-kernel implementation strategies in \rcha{matmul} reveals an opportunity for significant performance improvements in the inner kernel.
This improvement comes in the form of extending operation dimensions in order to perform a full $4 \times 4$ outer-product, an operation that is supported by \gls{power10}'s \gls{mma} extension, as discussed in \rcha{mma}.
The \gls{matrix engine} offers significant speedup for matrix multiplication when compared with previous \gls{simd} methods using \gls{vsx}.
Further insight in the form of the use of the \code{llvm.matrix.multiply.*} \gls{intrinsic} enables the proposed method to be well positioned to enter common use as a standardised \gls{lowering} for any frontend that compiles matrix-multiplication operations.

\rcha{method} details a procedure for emitting a matrix-multiplication kernel in \gls{llvm} \gls{ir} that makes use of all observations made above.
Deep understanding of the requirements of the hardware and the software led to the creation of several constraints that were used to create an efficient and performant \gls{lowering} strategy.
Each of these constraints is addressed throughout \rsec{alternateLowering} and directly influences the \gls{lowering} algorithm presented in \ralg{intrinsic}.
The implemented lowering generates an efficient kernel for operands of any dimension, data orientation, and several data types.

A thorough performance evaluation and analysis of the algorithm follows in \rcha{evaluation}.
An investigation and discussion of several shortcomings in the compiler backend for \gls{power} indicates where performance can be improved in future iterations of the \gls{llvm} framework.
Accounting for these deficiencies, the remaining experiments narrow kernel parameters to find an optimal setting.
This optimal setting is compared with the closest point of reference: the vectorised lowering of the same \gls{intrinsic} which makes use of the \gls{power} \glslink{isa}{ISA's} \gls{simd} capabilities in \gls{vsx}.
The comparison shows a speedup of at least 3.1 times for \code{half}, which is expected to improve drastically with improvements to the backend, up to a maximum of 15.8 times for \code{i8}.
Moreover, a human-crafted kernel is only twice as fast as the current best \gls{lowering} of the \code{llvm.matrix.multiply.*} \gls{intrinsic}, a significant achievement.
Furthering this comparison, there are strong indications that, after the noted \gls{spill} issues are resolved, the handwritten and compiler-generated kernels will have have identical performance.

The contribution of this thesis is part of a methodology that relies on a multi-level code generation strategy that separates the efficient utilization of the memory hierarchy at the macro level and the efficient utilization of the processing units and vector registers in the micro level.
The design of the micro-kernel code generation presented in this thesis uses the \code{llvm.matrix.multiply.*} \gls{intrinsic} in \gls{llvm} as an interface.
This solution is likely to also be the best methodology when targeting upcoming matrix-multiplication engines that will soon be available from other vendors.
Thus, the methodology presented in this thesis shall be relevant for these other machine architectures as well.
\end{document}
