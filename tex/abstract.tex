% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

% environment for abstract.
\begin{abstract} % Enforces spacing rules already.
  % \begin{itemize}
  %   \item $\leq 700$ words.
  %   \item Text only.
  % \end{itemize}
\Gls{gemm} is used widely in many high-performance application domains.
In many cases, these applications repeatedly execute their matrix-multiplication subroutine, as is the case in the implementation of a particle-physics simulator.
The construction of deep-learning models in machine learning also often relies on multiple executions of  \Gls{gemm} to compute a convolution operation.
This reliance on repeated executions causes matrix-multiplication operations to be a computational bottleneck in these applications, creating a strong motivation to improve the performance of \Gls{gemm}.

The state of the art for the efficient computation of \Gls{gemm} consists of manual, programmer-directed replacement of matrix multiplication with calls to highly optimised \gls{blas}-like libraries.
Such implementations contain kernels painstakingly written by hand in assembly.
Beyond a clear expertise barrier for porting each kernel to each iteration of a specific platform -- and thus a maintenance issue -- such a replacement creates a dependency on external code over which a developer has no control.
Moreover, calls to an unknowable library function disables critical optimisations such as inlining that can enable further optimisations in the calling code.

A less common, yet critical, opportunity for code tranformation is operation combination via loop fusion.
When two matrix operations directly follow each other with the input data of one operation being the output of the previous, they can be combined in such a way that as the results of one operation is immediately passed to the next.
In this way, all operations over a single piece of input data can be computed before writing it back to memory.
\nelson{I am not sure what you mean by "memory synchronisation" in this sentence.}
Function calls induce artificial memory synchronisation points which in turn cause performance degradations that cannot be alleviated by a compiler.

The solution to these issues is to provide an alternative for the computation of matrix-multiplication, with competitive performance, directly within the compiler.
An implementation in this style automatically generate a matrix-multiplication kernel that benefits from all applicable code transformations available in the compiler.
This thesis addresses the lack of an efficient compiler-only path to generate code for \Gls{gemm} by investigating and implementing a high-performance matrix-multiplication kernel implementation directly within the \gls{llvm} compiler framework.
Furthermore,  the proposed solution integrates emerging technologies, namely the \gls{matrix engine} that provides hardware assistance for the computation of matrix multiplication.

\Glspl{matrix engine} are new facilities added in several architectures as part of the most-recent \gls{cpu} generation.
These extensions focus on enabling matrix operations by extending the preexisting \gls{simd} capabilities of the \gls{isa}.
The most recent processor in the POWER architecture, \glslink{power10}{POWER10}, from \gls{ibm} features one such extension named \gls{mma}.
Its unique design choice to implement matrix multiplication through the computation of outer products presents new opportunities to improve the performance of computing a matrix-multiplication kernel.

The generation of efficient code for matrix multiplication in the  \gls{llvm} compiler framework is divided into two levels: the macro kernel and the micro kernel. 
The main goal of the macro-kernel code generation is to make the best use of the memory hierarchy when bringing the operands all the way from the main memory to the highest-level of cache memory.
The central ideas for the macro-kernel code generation are {\it blocking} to create the appropriate unit of transfer between the various levels of the memory hierarchy and {\it packing} to reorganize the data into a temporary buffer so that it is laid out to improve locality.
The focus of the micro-kernel code generation is to make efficient use of the vector functional units in the processor and to reduce the memory-register data-transfer requirements by increasing the reuse of data that has been transferred to the vector registers.
This thesis focuses on the micro-kernel code generation and uses an implementation of a compiler-only macro-kernel code generation developed in collaboration with other members of the same research team.

This thesis also contributes a detailed performance study that indicates that this new code-generation strategy results in speed improvements between 3.1 and 12.9 times when compared with the closest alternative compiler-only code-generation implementation for some data types.
The thesis also features a detailed analysis of the experimental results --- specially for some data types for which the performance is surprising or lower than expected --- that reveals opportunities for changes in the compiler that have the potential to lead to improvements in the entire \gls{power} compilation stack.

\end{abstract}

\end{document}
