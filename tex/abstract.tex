% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\begin{abstract}
\Gls{gemm} is used widely in many high-performance application domains.
In many cases, these applications repeatedly execute their matrix-multiplication subroutine, as is the case in the implementation of a particle-physics simulator or the repeated convolutions of many deep-learning models.
This reliance on repeated executions causes matrix-multiplication operations to be a computational bottleneck in these applications, creating a strong motivation to improve the performance of \gls{gemm}.

The state of the art for the efficient computation of \gls{gemm} consists of manual, programmer-directed replacement of matrix multiplication with calls to highly optimised \gls{blas}-like libraries which contain kernels painstakingly written in assembly.
Beyond a clear expertise barrier for porting each kernel to each iteration of a specific platform -- and thus a maintenance issue -- such a replacement creates a dependency on external code over which a developer has no control.
Moreover, calls to an unknowable library function disable critical optimisations such as inlining and loop fusion that can enable further optimisations in the calling code.

The solution to these issues is to provide an alternative for the computation of matrix-multiplication, with competitive performance, directly within the compiler.
An implementation in this style automatically generates a matrix-multiplication kernel that benefits from all applicable code transformations available in the compiler.
This thesis addresses the lack of an efficient compiler-only path to generate code for \gls{gemm} by investigating and implementing a high-performance matrix-multiplication kernel implementation directly within the \gls{llvm}\texttrademark{} compiler framework.
Furthermore, the proposed solution integrates emerging technologies, namely the \gls{matrix engine}, that provide hardware assistance for the computation of matrix multiplication.
In particular, the recent \gls{power10} processor features one such extension named \gls{mma}.
Its unique design choice to implement matrix multiplication through the computation of outer products presents new opportunities to improve performance.

The generation of efficient code for matrix multiplication in the \gls{llvm} compiler framework is divided into two levels: the macro kernel and the micro kernel.
The main goal of the macro-kernel code generation is to make the best use of the memory hierarchy when bringing the operands from the main memory to the highest-level of cache memory.
The focus of the micro-kernel code generation is to make efficient use of \gls{simd} functional units and to reduce the memory-register data-transfer requirements by increasing data reuse.
This thesis focuses on the micro-kernel code generation, though a compiler-only macro-kernel code generation developed as part of a large work is available.

This thesis also contributes a detailed performance study that indicates that this new code-generation strategy results in speed improvements between 3.1 and 15.8 times when compared with the closest alternative compiler-only code-generation implementation for some data types.
There is also strong indication that, given several improvements in the compiler assembly-code generation, the compiler-generated kernel can match the performance of an expert's handcrafted solution.
This thesis also features a detailed analysis of the experimental results that reveals opportunities for changes in the compiler that have the potential to lead to improvements in the entire \gls{power} compilation stack.
\end{abstract}

\end{document}
