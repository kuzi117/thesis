% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Related Work}
\label{cha:related}
\bk{
  I'm not sure if this section should go early or late.
  I know we've discussed that it's typically early but our area has a history of putting it as the second-to-last section (before conclusion).
  Seems better to have it late so I won't have to explain a bunch of things before the sections they're explained in.
}
Each of the following sections contains text derived from work submitted to PACT 2021~\autocite{kuzma2021fast}.

% comparitive and analytical, not descriptive.
\section{Matrix Multiplication}
In a seminal work, Goto and Van D. Geijn detail a layered approach to improve cache and vector-register utilization on \glspl{cpu}s~\autocite{goto2008anatomy}.
Using this approach, modern linear algebra libraries, such as Eigen and OpenBLAS, achieve high performance on \gls{hpc} workloads.
Goto and Van D. Geijn show that modelling both L2 cache and \gls{tlb} --- and not only L1 cache as considered earlier --- is crucial for cache performance.
Their work is seminal because it publicly explained practical strategies for optimal cache and vector register utilization on \glspl{cpu}; these strategies were previously only available in proprietary libraries.
The layered strategy features two stages:
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}, label=\textbf{(\arabic*)}, after={.}]
  \item blocking input matrices and packing tiles of these blocks in such a way that tiles lay in main memory in the order that they will be accessed
  \item computing a small \gls{gemm} at the register level
\end{enumerate*}
Kuzma \etal~\autocite{kuzma2021fast} is the first work to create a compiler-only code generation for the layered approach and adapts blocking, tiling, and packing to create a data layout that is suitable for computing with \gls{mma} and to also improve utilization of the L3 cache.
This thesis is an in-depth presentation of the method behind \textbf{(2)}.

Gareev et al.~\autocite{gareev2018high} implement tiling and packing within Polly~\autocite{grosser2011polly,grosser2012polly} without the need for an external library or automatic tuning.
Their approach with a hand-optimized SSE kernel reports performance on par with BLIS on Intel Sandy Bridge.
When not relying on an assembly kernel, their pass uses the default \gls{llvm} \glslink{vectorisation}{vectoriser} that delivers only a small speedup over naive code.
The work in this thesis presents a compiler-only kernel that can replace Gareev \etal's SSE kernel or the default \gls{llvm} \glslink{vectorisation}{vectoriser} on \gls{power10}.
Furthermore, this work is more modular and can be reused by new passes or other code generation paths as drop-in inner-kernel.

Uday Bondhugula presents an implementation of the BLAS strategy within the emerging MLIR framework~\autocite{bondhugula2020high}.
He demonstrates that blocking, packing, register tiling, and unroll/jam yields code that achieves 34\% of OpenBLAS' performance on Intel's Coffee Lake~\autocite{bondhugula2020high}.
\bk{Numbers in this next sentence need to be verified.}
Bondhugula also implemented a custom \gls{vectorisation} pass to replace the default \gls{llvm} \glslink{vectorisation}{vectoriser} thus reaching 91\% of the performance of OpenBLAS.
This work also shows the weakness of the default \gls{llvm} \glslink{vectorisation}{vectoriser}.
As well, because \gls{llvm} \gls{ir} is a dialect within MLIR, using \gls{llvm} \gls{ir} \glspl{intrinsic} is inherently an accessible method of implementing an inner kernel in MLIR.

Carvalho \etal introduce KernelFaRer, a robust pattern recognition system that can identify matrix-multiplication patterns in the \gls{llvm} \gls{ir} level and can replace these with library calls~\autocite{carvalho2021kernelfarer}.
While this approach can lead to speedups on the order of 1000s in comparison with non-optimized code, it has the drawback of requiring the integration of libraries into a computer system that may not have it.
Moreover, their experimental results, as in this work, indicate that, for smaller matrices, the overhead of invoking functions in the libraries leads to performance degradations.
The solution in this paper is orthogonal to Carvalho \etal: their pattern recognition can identify GEMM kernels at the intermediate-level representation and then replace the inner most kernel with the solution presented here.

\section{Code Generation for New Hardware}
When presenting the ILLIAC IV, one of the first SIMD machines, Barnes \etal advocated that data parallelism would be crucial for progress~\autocite{barnes1968illiac}, citing matrix operations as a critical target~\autocite{kuck1968illiac}.
Nearly 50 years later, Barnes' direction culminated in the inclusion of vector extensions in all mainstream \glspl{cpu} such as \gls{ibm}'s \gls{vsx}~\autocite{PowerISA}, Intel's AVX-512~\autocite{IntelISA}, and ARM's NEON/\gls{sve}~\autocite{ArmISA}.
Although fast \gls{vectorisation} is powerful, matrix-multiplication performance could be improved further with specialized hardware units.
This possibility is now realized with the introduction of what Domke \etal have dubbed ``matrix engines''~\autocite{domke2020matrix}, now available in \gls{ibm}'s \gls{mma}~\autocite{PowerISA}, Intel's \gls{amx}~\autocite{IntelISA}, and ARM's NEON/\gls{sve}~\autocite{ArmISA}.
This thesis, in the same vein as Barnes \etal, focuses on bringing high performance to new hardware facilities.

\section{Other Matrix Engines}
The advent of the ``general purpose'' GPUs quickly saw study and performance analysis of matrix computations~\autocite{larsen2001fast,fatahalian2004understanding}.
This evolved into implementations of matrix multiplications on GPUs: manually~\autocite{li2011strassens}, through libraries like BLAS~\autocite{nath2011accelerating}, and through frameworks such as DistME~\autocite{han2019distme}.
Matrix multiplication is also central to the design of hardware for tensor-operation acceleration such as Google's Tensor Processing Unit~\autocite{jouppi2017datacenter}, Nvidia's Tensor Core~\autocite{markidis2018nvidia}, and Huawei's Cube Unit~\autocite{liao2019davinci}.
Performance evaluations of GEMM in tensor hardware are difficult to find because the studies of these devices focus on the benchmarking of various flavours of neural network~\autocite{jouppi2017datacenter,wang2019benchmarking}.
Matrix-multiplication acceleration in standalone accelerators is not the focus of this thesis.

\section{Performance Evaluation}
Robust performance benchmarking is critical for the evaluation of vector extensions.
While there is extensive performance evaluation of matrix multiplication on vector extensions for Intel architectures~\autocite{hassan20161performance,hemeida2020optimizing,alappat2020understanding}, to the best of the author's knowledge, similar studies do not exist for the PowerPC or ARM platforms.
Moreover, the introduction of matrix engines is recent in all platforms and therefore only simulated or theorized performance estimates exist for AMX, SVE, or MMA~\autocite{poenaru2020evaluating,domke2020matrix}.
Therefore, this work is among the first to present performance evaluation of a matrix engine on actual hardware.

Sections:
\begin{itemize}
  \item Matrix multiplication performance improvement.
  \begin{itemize}
    \item
      Many works don't discuss their implementation of the matmul kernel.
      This work specifically implements rank-1 update~\autocite{nakasato2011fast}.
    \item
      Another GPU paper specifically using outer product~\autocite{wu2016achieving}.
      I'd never heard of world scientific (the hosting website) or the publishing journal ``parallel processing letters'' so I'm slightly dubious of the peer review.
    \item
      Potentially related work.
      Focuses on rank-1 update as part of matrix normalisation using matrix-vector not matrix-matrix multiplication~\autocite{yu2020toward}.
  \end{itemize}
  \item Codegen for new hardware (focus on SIMD).
  \item Performance evaluation methodologies (cross platform, new hardware).
  \item Similar works on Neon/SVE and AMX.
    \begin{itemize}
      \item
        Accelerator design for outer product in sparse matmul~\autocite{pal2018outerspace}.
        Paper which builds on the idea and compares with it~\autocite{srivastava2020matraptor}.
    \end{itemize}
\end{itemize}

\end{document}
