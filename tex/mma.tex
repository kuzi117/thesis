% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Matrix Math Assist}
\label{cha:mma}

\Gls{power10}'s \gls{mma} is what is being called a ``matrix engine''~\autocite{domke2021matrix}.
Specifically, it is a facility in the \gls{cpu} whose function is computing matrix multiplication.
Matrix engines have appeared in multiple architectures, though their implementation and design choices differ.
Likewise, the algorithm for optimally making use of each of these architectures differs.
While this work focuses on \gls{mma}, a discussion of the differences in these architectural implementations follows in \rsec{matrixEngines}.

\gls{mma} adds a new architectural feature alongside the matrix engine: eight accumulator (ACC) registers.
Accumulators and the new instructions for interacting with them enable the efficient matrix-matrix multiplication outlined in \rsec{gotoInner}.
Much of the content in this section can be found throughout the \gls{power10} \gls{isa} document though some of it comes as insight from engineers at \gls{ibm} or external sources.

\Gls{mma}'s design is built around the choice to compute the outer product instead of the inner product.
As discussed in \rsec{productsVs}, the inner product is inferior when compared with the outer product.
\Gls{mma} eschews the overhead revealed at the end of \rsec{gotoInner} by directly providing the outlined outer product capabilities, removing unnecessary overhead, along with the large register necessary for holding the result data.

\section{Accumulator Assembly and Disassembly}
\label{sec:assDis}
An accumulator, in all interactions with \gls{mma}, can be thought of as a single register consisting of $4 \times 4$ 32-bit elements.
Accumulators may be assembled (initialised) in three manners:
\begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}, after={.}]
  \item set to zeroes (\code{xxsetaccz}, Set Acc to Zero)
  \item constructed from four consecutive \glspl{vsr} (\code{xxmtacc}, Move To Acc)
  \item a multiplication instruction that does not accumulate (see \rsec{matMulMMA})
\end{enumerate*}
When constructing from \glspl{vsr}, each \gls{vsr} is moved into the ACC and becomes one row\footnotemark of the resulting accumulator.
\footnotetext{Viewing the vectors as rows is intuitively convenient, but relatively arbitrary in reality. See \rsec{arbitraryOrder} for use as columns.}

In its current iteration, accumulator $n$ is the logical grouping of four 128-bit \glspl{vsr} (\code{VSR[4n:4n+3]}) into a single register.
As such, the four underlying registers used to construct the accumulator are unavailable for use while the accumulator is in an assembled state.
For example, \code{VSR[0:3]} are unavailable while \code{ACC[0]} is assembled.
This blocking effect also applies to the underlying registers when assembling an accumulator to zeroes.

Blocking the underlying registers is viewed as a reasonable restriction in \gls{power10}.
However, the ISA is written such that if, in future versions of the hardware, the blocking restriction is removed, no changes will need to be made to the \gls{isa}.

The reverse operation to assembly is disassembly (\code{xxmfacc}, Move From Acc).
Disassembly moves each row from the accumulator into the same vector registers used for construction.
Therefore, as before, the first row of \code{ACC[0]} is placed into \code{VSR[0]}, the second row is placed into \code{VSR[1]}, \etc.

\section{Matrix Multiplication in MMA}
\label{sec:matMulMMA}

\begin{table}[t]
  \centering
  \begin{tabular}{| c | c | c | c | c |}
    \hline
    Instruction & Input Type & Output Type & Input Dim. & Smallest Computation \\\hline
    \code{xvi4ger8}  & \code{i4}     & \code{i32}    & $4 \times 8$ & \matmul{4}{8}{4} \\\hline
    \code{xvi8ger4}  & \code{i8}     & \code{i32}    & $4 \times 4$ & \matmul{4}{4}{4} \\\hline
    \code{xvi16ger2} & \code{i16}    & \code{i32}    & $4 \times 2$ & \matmul{4}{2}{4} \\\hline
    \code{xvf16ger2} & \code{half}   & \code{float}  & $4 \times 2$ & \matmul{4}{2}{4} \\\hline
    \code{xvbf16ger2} & \code{bfloat16}\footnotemark & \code{float}  & $4 \times 2$ & \matmul{4}{2}{4} \\\hline
    \code{xvf32ger}  & \code{float}  & \code{float}  & $4 \times 1$ & \matmul{4}{1}{4} \\\hline
    \code{xvf64ger}  & \code{double} & \code{double} & $4 \times 1$ & \matmul{4}{1}{2} \\\hline
  \end{tabular}
  \caption[MMA Instruction Description]{A description of \gls{mma} instructions investigated. Input dimension is transposed for second argument.}
  \label{tab:mmaInsts}
\end{table}
\footnotetext{Google's Brain floating point format~\autocite{wang2019bfloat16}.}

\rtab{mmaInsts} shows all seven of the datatypes usable with \gls{mma}.
Instructions are given three arguments: an accumulator and two \glspl{vsr}.
Considering the simplest example, single-precision floating point, a four-element row or column fills the entire \gls{vsr} ($32\text{bits} \times 4 = 128\text{bits}$).
Thus, two length-four vectors are multiplied in an outer product, creating the computation \matmul{4}{1}{4}.
Recalling \rsec{outerProduct}, this is a rank-one update to the accumulator.

Halving the size of the type to 16 bits means the argument \glspl{vsr} are underutilised at half capacity.
Accumulators cannot be expanded and are always $4 \times 4$ matrices of 32-bit elements.
Thus, to increase utilisation of \glspl{vsr}, the number of elements contained in each \gls{vsr} is doubled resulting in doubling the number of rows or columns in a \gls{vsr}.
Therefore, when using 16-bit values, the calculation performed is \matmul{4}{2}{4}, now performing a rank-two update to the accumulator.
The same logic applies for eight- and four-bit types, computing rank-four and rank-eight updates respectively.

\subsection{Arguments in Register}
\label{sec:registerArguments}
Once the instruction is performing a rank-two update or greater, the order of the elements in the register must be considered.
The \gls{power} \gls{isa} specifies that the first argument must have elements in row-major order while the second argument must have elements in column-major order.
This enables simple circuit pathways where, for a rank-$r$ update, element $(i, j)$\footnotemark in $C$ accumulates with the dot product of the $r$-length group of elements in $A$ at offset $i \times r$ and in $B$ at $j \times r$.
\footnotetext{Typical matrix notation uses one-indexing; this example uses zero-indexing.}

\begin{figure}[t]
  \hfill
  \begin{subfigure}{.40\linewidth}
    \centering
    \begin{tikzpicture}[scale=1/2]
      % A
      \draw[pattern=north west lines, shift={(0, 3)}] (0, 0) rectangle +(2, 1);
      \draw[xstep=1, ystep=1, shift={(0, 0)}, color={black!20!white}] (0, 0) grid +(2, 4);
      \draw[xstep=2, ystep=1, shift={(0, 0)}] (0, 0) grid +(2, 4);

      % B
      \draw[pattern=dots, shift={(5, 5)}] (0, 0) rectangle +(1, 2);
      \draw[xstep=1, ystep=1, shift={(3, 5)}, color={black!20!white}] (0, 0) grid +(4, 2);
      \draw[xstep=1, ystep=2, shift={(3, 5)}] (0, 0) grid +(4, 2);

      % C
      \draw[pattern=crosshatch, shift={(5, 3)}] (0, 0) rectangle +(1, 1);
      % \draw[pattern={Hatch[angle=45,distance={8pt},xshift=.2pt, line width=1.25]}, xstep=2, ystep=2, shift={(3, 6)}] (0, 0) rectangle +(8, 2);
      \draw[xstep=1, ystep=1, shift={(3, 0)}, color={black!20!white}] (0, 0) grid +(4, 4);
      \draw[xstep=4, ystep=4, shift={(3, 0)}] (0, 0) grid +(4, 4);

      % Matrix labels.
      \node[above] at (0, 4) {$A$};
      \node[left] at (3, 7) {$B$};
    \end{tikzpicture}
    \caption{Arguments for an \code{i16} matrix shown contextually.}
    \label{fig:registerArgumentsMatrix}
  \end{subfigure}
  \hfill
  \begin{subfigure}{.4\linewidth}
    \centering
    \begin{tikzpicture}[scale=1/2]
      % Spacer to move the diagram up.
      \node[shift={(0, -.1)}] {};
      \node[shift={(0, 3.1)}] {};
      % A
      \pgfmathsetmacro{\aOffset}{3.5}
      \draw[pattern=north west lines, shift={(0, \aOffset)}] (0, 0) rectangle +(2, 1);
      \draw[xstep=1, ystep=1, shift={(0, \aOffset)}, color={black!20!white}] (0, 0) grid +(8, 1);
      \draw[xstep=2, ystep=1, shift={(0, \aOffset)}] (0, 0) grid +(8, 1);

      % B
      \draw[pattern=dots, shift={(4, 1)}] (0, 0) rectangle +(2, 1);
      \draw[xstep=1, ystep=1, shift={(0, 1)}, color={black!20!white}] (0, 0) grid +(8, 1);
      \draw[xstep=2, ystep=1, shift={(0, 1)}] (0, 0) grid +(8, 1);

      % Matrix labels.
      \node[left] at (0, 4) {$A$};
      \node[left] at (0, 1.5) {$B$};
    \end{tikzpicture}
    \caption{Arguments for an \code{i16} shown in VSR.}
    \label{fig:registerArgumentsVSR}
  \end{subfigure}
  \hfill
  \caption[Small Data Type Element Order In Register]{Element ordering in types smaller than \code{float} demonstrated using \code{i16}.}
  \label{fig:registerArguments}
\end{figure}

\rfig{registerArgumentsMatrix} shows an example for \code{i16}.
$A$ and $B$ are divided by dark lines according to their access order while lighter lines divide the elements within the vector.
The first two elements of the operand from $A$, highlighted by diagonal lines, form a row while the first two elements from $B$, highlighted by dots, form a column.

In \rfig{registerArgumentsVSR}, the arguments are shown laid out in a \gls{vsr}.
The \gls{vsr} is divided using dark and light lines in the same way as \rfig{registerArgumentsMatrix} to show how the element positions are translated to a \gls{vsr}.
The dot product of these two two-element vectors, from $A$ at offset $0 \times 2 = 0$ and from $B$ at $2 \times 2 = 4$ in their respective \glspl{vsr}, are accumulated with the value in $C$ at $(0, 2)$, highlighted in a crosshatch pattern.

\subsection{Instruction Variants}
\label{sec:instVar}
\begin{table}[t]
  \centering
  \begin{tabular}{| c | c | c |}
    \cline{2-3}
    \multicolumn{1}{c|}{} & \code{-p} & \code{-n}\\\hline
    \code{p-} & $C' = C + AB$ & $C' = -C + AB$\\\hline
    \code{n-} & $C' = C - AB$ & $C' = -C - AB$\\\hline
  \end{tabular}
  \caption[MMA Instruction Suffixes and Associated Computation]{MMA accumulation computations by instruction suffix. $C$ is the accumulator before accumulation while $C'$ is the accumulator after accumulation.}
  \label{tab:accSign}
\end{table}

An instruction without a suffix (first column of \rtab{mmaInsts}) will both assemble the given accumulator and overwrite it with the outer product of the two \glspl{vsr}.
Afterward, to accumulate into the accumulator, each datatype has a ``family'' of instructions offering different semantics.
Four instruction suffixes are available: \code{pp}, \code{pn}, \code{np}, \code{nn}.
The \code{p} and \code{n} stand for positive and negative respectfully.
The first character specifies the sign of multiplication (\ie $\pm AB$) while the second specifies the sign of the accumulator (\ie $\pm C$).
The possible computations are shown in \rtab{accSign}.
Computations with 16-bit integers have an additional suffix, \code{s}, which replaces the regular overflow semantics with saturating semantics (\eg $0\text{xFFFF} + 1 = 0\text{xFFFF}$).

In combination with any of the suffixes, a single prefix exists: \code{pm}.
This prefix indicates a ``prefixed masked'' instruction.
These instructions take an extra three arguments, each of which is a mask that enables fine-grain control over the accumulation.
Each of the masks allows disabling of one of the following:
\begin{enumerate*}[itemjoin*={{ or }}, label=\textbf{(\arabic*)}, after={.}]
  \item any of the rows of the accumulation
  \item any of the columns of the accumulation
  \item any of the ranks of the accumulation
\end{enumerate*}
Masks disable only the application of computed values to the accumulator, not the calculation; therefore, the execution time of a masked instruction is the same as that of an unmasked one.

\subsection{Double Precision Differences}
\label{sec:doubles}
When all other computations produce a 32-bit value, essentially gaining precision or remaining at the same precision, it would be unreasonable to force computations with 64-bit values to lose precision.
Therefore, when working with double-precision floats, accumulators become $4 \times 2$ arrays rather than the usual $4 \times 4$.
The change in data size affects the argument \glspl{vsr} as well: a single \gls{vsr} now fits only two values.
Thus, in order to compute a rank-one update, the first dimension requires four values, spread across two registers, while the second dimension now requires two values in a single register.

\section{Matrix Engine Comparison}
\label{sec:matrixEngines}
Currently, three prominent matrix engines exist, but, given the current importance placed on matrix multiplication, it is unlikely that this list remains small.
Furthermore, each of the architectures have been designed in their own way with extensibility in mind implying that the richness and diversity of features is sure to improve as well.
The features of these three matrix engines are summarised in \rtab{featComp}.

\begin{table}
  \centering
  \begin{tabular}{| c | c | c | c | c |}
    \hline
    Architecture & Location & \parbox[t][28pt][t]{40pt}{\centering Product Style} & Arg/Dest & Supported Types\\\hline
    \parbox[t][][t]{40pt}{\centering Power10 MMA} & core & outer & VSR/ACC & \parbox[t][40pt][t]{3.2cm}{\raggedright\code{i4}, \code{i8}, \code{i16}, \code{bfloat16}, \code{half}, \code{float}, \code{double}}\\\hline
    x86 AMX & off-core & inner & Tile &\parbox[t][11pt][t]{3.3cm}{\raggedright\code{i8}, \code{bfloat16}}\\\hline
    \parbox[t][][t]{60pt}{\centering ARM NEON/SVE} & core & inner & Vector register & \parbox[t][25pt][t]{3.3cm}{\raggedright\code{i8}, \code{bfloat16}, \code{float}, \code{double}}\\\hline
  \end{tabular}
  \caption[Matrix Engine Feature Comparison]{Comparison of features currently offered by cpu-based matrix engines.}
  \label{tab:featComp}
\end{table}

The x86 architecture has placed a strong focus on matrix multiplication by developing \gls{amx}.
It is an off-core accelerator dedicated specifically to matrix multiplication.
Furthermore, the accelerator makes use of an entirely new register file consisting of eight large ($16 \times 64$ bytes) registers dubbed ``tiles''.
The architecture supports only inner product computations and uses tiles as both arguments and destinations .
While it supports less types than \gls{mma}, it does support two important types for artificial-intelligence workloads: \code{i8} and \code{bfloat16}.

ARM's NEON and \gls{sve} do not focus on matrix multiplication, providing them instead as a small part of a larger vector extension.
The facility is implemented on-core and does not provide any new registers, relying on the architecture's already-present vector registers.
These two factors, combined with an inner-product style computation mean relatively small computation sizes when compared with \gls{mma} or \gls{amx}.
NEON, like \gls{amx}, supports \code{i8} and \code{bfloat16} while \gls{sve} adds support for \code{float} and \code{double}.

\section{Summary}
This chapter presented the new \gls{mma} extension in \gls{power10}.
The extension adds a new type of register called an accumulator which can be used to compute matrix multiplication using the outer product.
Moreover, a total of seven types are supported, several of which perform multiple outer products in a single instruction, increasing efficiency
Finally, this chapter compared \gls{mma} compared with the \gls{matrix engine} available in other \glspl{isa}.

\end{document}
