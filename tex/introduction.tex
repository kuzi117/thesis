% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Introduction}
\label{cha:intro}

Matrix multiplication is a critical basic operation in many \gls{hpc} and AI workloads.
Given the surge of popularity in AI and the growing scale of \gls{hpc} tasks and simulations, the optimisation of matrix multiplication can mean an essential reduction in computing time.
Waugh and McIntosh-Smith demonstrate that for small thread counts, \gls{gemm} operations often dominate execution time in a set of benchmarks found to be representative of current workloads~\autocite{waugh2020use}.
While multithreading reduces these functions' contribution to overall runtime significantly, they conclude that, in a soon-to-be-exascale future, applications will adapt to use more \gls{gemm} operations, expanding this portion considerably.
\bk{
  Technically they conclude there will be more BLAS usage, but they've also shown that the majority of time spent in BLAS is spent in some sort of GEMM (ZGEMM, DGEMM, SGEMM, etc.).
  I'm trying to avoid explaining BLAS until I talk about libraries.
  Is this an acceptable summarisation?
}
\nelson{Yes}

At the center of our capability to handle these expanding workloads is the \gls{matrix engine}.
\Glspl{matrix engine} are a new accelerator facility, found in the most recent generation of \glspl{cpu}, that focuses on accelerating matrix multiplication.
Currently, acceleration is done via external-hardware accelerators or via vector extensions to an \gls{isa}.
External-hardware accelerators may be an \gls{asic} specialised to the task (\eg Google's \gls{tpu}~\autocite{abadi2016tensorflow}), a (\eg \glspl{gpu}, or an \glspl{fpga}).
\nelson{Chenged "and" to "or" because a piece of data cannot be used and produced simultaneously.}
Vector extensions are part of the movement towards the \gls{simd} computing paradigm where multiple pieces of data are used or produced simultaneously (see \rsec{simd}).
\bk{These next two sentences might be a tangent and might be better off deleted or moved.}
\nelson{I am rephrasing it.}
%These vector extensions have a specific advantage over external hardware for small matrices given that data must be sent to the external hardware via the slower PCIe connection.
%For vector extensions, these small matrices can be brought entirely into cache allowing them to remove much of the data transfer overhead.
An advantage of vector extensions in relation to external-hardware accelerators is that a vector extension can access data through the same memory hierarchy used by the CPU while most external accelerators require data transfer over the slower PCIe connection.

\nelson{"variety of implementing" sounds odd}
The current state of the art when working with matrix operations is the \gls{blas} interface and any of a variety of implementing libraries (\eg OpenBLAS~\autocite{2012xianyi}, IBM's ESSL, Nvidia's cuBLAS).
\bk{
  Do I need to explain BLAS more or is the glossary sufficient?
  I reference it only several times throughout the current text and only as a passing tangent.
}
\bk{Do I need more examples library implementations and do I need citations for each?}
\nelson{You should include at least MKL. Citation for each one would be good.}
These libraries can provide incredible speedups and, in parallel architectures, automatic parallelisation: a very attractive feature for large workloads.
It has been a natural extension of these libraries to include the usage of matrix engines when targeting \glspl{cpu}.
\nelson{This is a case where there is much more in your mind than in the text or what your reader can know. Be specific on what you mean by "poor habits" and "extra requirements".}
However, these libraries induce poor habits in development and extra requirements on systems.

All high-performance implementations of \gls{blas}-like libraries have handwritten assembly kernels at their core, though the extent varies~\autocite{zee2016blis}.
\bk{Should I emphasise or add more to this next argument?}
Nevertheless, each of these kernels must be produced and hand tuned for each new architecture that the library needs to support.
\bk{
  How compelling is this next argument, really?
  Will it strike a chord with the reader or is it weak?
}
\nelson{"on tightly guarded supercomputers" places you as an outsider. Usually the team that is targeting a machine has access to it. Similar for users of these supercomputers. I do not understand the issue with accessibility here. There are three arguments that you can make to motivate the compiler-only code path: (1) adding a new machine target to a library implementation requires developers that are proficient in assembly for that machine. Maintaining that code is also more difficult; and (2) in some machine installation a specific library may not be installed; (3) for small matrices, the overhead of invoking a library is too high in comparison to the execution time of the matrix multiplication.  }
Moreover, libraries must be available and updated in the target machine for the program to use: a sometimes difficult request to fulfill on tightly guarded supercomputers.

Therefore, as a counterpoint, a method of transparently accelerating matrix multiplication via a compiler-only path is a crucial step forwards in portability and maintainability.
\nelson{What "now" refers to in this paragraph? There is a temporal relation in your mind that did not make it to the text.}
Matrix multiplication kernels, while still architecture \emph{sensitive}\footnotemark, need not be written in assembly and are now immediately available for additional automatic optimisation by the compiler framework's current and future optimisations.
\bk{The footnote feels like the correct choice, but maybe it would make sense to be part of the text.}
\nelson{The footnote should be part of the text. And perhaps you need a paragraph before this one where you explain your distinction between architecture-sensitive kernels and architecture-specific kernels. The footnote needs rephrasing.}
\footnotetext{
  Not architecture \emph{specific}, \eg a kernel may be written in terms of an \gls{isa}'s vector extension, but duplication as part of loop unrolling, which is typically subject to specific architecture features (\eg register count and length, functional units), can be handled automatically by the compiler.
}
\nelson{"These" what?}
These include optimisations such as loop switching, blocking, and unrolling which are known to have significant effects on matrix multiplication speed\todo{citation?}.
\bk{
  Technically loop switching would be possible only at the outer kernel level given that the intrinsic is completely unrolled
}
As well, the kernel is now subject to any future improvements in the compiler's backends, encompassing improvements such as register allocation and instruction reordering, all transparent to the user.

\nelson{What is "the current paradigm"? Do you mean "when using libraries"? Be more specific. Sounds like you have two ways of implementing matrix multiplication in your mind but you have not been explicit about these two ways in the text.}
Furthermore, a crucial optimisation, loop fusion, is impossible with the current paradigm.
Mathematically, consecutive matrix multiplications (\eg $D = ABC$) can be fused so as to perform both computations at once instead of producing a temporary result (\ie $\text{T}=AB;D=TC$).
\nelson{"This" what?}
This significantly reduces the memory movements required by removing the need to store and reload a temporary value that is likely to have been removed from cache if the matrices are large.

\bk{Codegen requirments are next.}

% \nelson{
%   Everything that you have in this chapter should move to the background chapter.
%   The introduction should explain the problem that you are solving.
%   Re-read the introductions to the KernelFarer paper and to the PACT paper to get your mind into what an introduction should say.
%   You can either talk about the same ideas and white your own, or you can reuse some of that text if you have a paragraph saying that the text appears in those multi-author manuscripts.
% }
% \nelson{
%   The introduction should contain a clear problem statement or thesis statement.
%   Someone reading only the introduction should be able to clearly state the contribution of a thesis.
% }

\begin{itemize}
  \item
    Set up issues: matrix multiply is in high demand as HPC and AI tasks grow in size and importance.
  \item
    Present matrix engines as the ``next generation'' solution.
    They don't require extra hardware, specialised (TPU, etc.) or not (GPU, FPGA).
  \item
    Motivate the problem: We need automated code generation, in a compiler, to support these new matrix engines.
    Automated generation of efficient code is more versatile and productive than assembly-level code writing for each new matrix-engine architecture.
  \item
    Set up problem: code generation requires
    \begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}, after={.}]
      \item ease of access to users to facilitate adoption
      \item understanding of hardware (memory heirarchy, ME facility properties)
    \end{enumerate*}
  \item
    Referencing adoption: discuss how libraries
    \begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}, after={.}]
      \item are the default high performance implementation
      \item require installation and maintenance
      \item promote bad development habits (kernel written in assembly, specialised to hardware versions)
    \end{enumerate*}
  \item
    Referencing understanding of hardware: discuss how initial SIMD hardware and algorithms parallel the needs of matrix engines.
    \bk{
      This idea comes from a discussion on the PACT 2021 paper where we wanted to relate say ``codegen for new hardware'' is an important contribution.
      I'm not sure if there's a better hardware example or a better way to go about relating this to codegen.
    }
  \item
    Present first contribution as a lowering algorithm that addresses the above problems by
    \begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}, after={.}]
      \item implementing in a production compiler (LLVM) and implementing a generic lowering under and easy to use intrinsic.
      \item carefully considering memory and hardware constraints to produce high performance code.
    \end{enumerate*}
  \item
    Second contribution is the evaluation of the implementation.
  \item
    Expand slightly and point to sections.
\end{itemize}

% \section{Contributions}
% This work focuses on how \gls{mma} can improve matrix multiplication on PowerPC
% This work addresses multiple questions of interest when considering MMA.
% \begin{enumerate}
%   \item
%     Does an \gls{mma} kernel perform better than the vectorised equivalent?
%   \item
%     Does \gls{mma} offer greater benefits when using different datatypes?
% \end{enumerate}

\end{document}
