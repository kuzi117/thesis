% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Introduction}
\label{cha:intro}

Matrix multiplication is a critical basic operation in many \gls{hpc} and AI workloads.
Given the surge of popularity in AI and the growing scale of \gls{hpc} tasks and simulations, the optimisation of matrix multiplication can mean an essential reduction in computing time.
Waugh and McIntosh-Smith demonstrate that for small thread counts, \gls{gemm} operations often dominate execution time in a set of benchmarks found to be representative of current workloads~\autocite{waugh2020use}.
While multithreading reduces these functions' contribution to overall runtime significantly, they conclude that, in a soon-to-be-exascale future, applications will adapt to use more \gls{gemm} operations, expanding this portion considerably.

At the center of our capability to handle these expanding workloads is the \gls{matrix engine}.
\Glspl{matrix engine} are a new accelerator facility, found in the most recent generation of \glspl{cpu}, that focuses on accelerating matrix multiplication.
Currently, acceleration is done via external accelerators or via vector extensions to an \gls{isa}.
An external accelerators may be an \gls{asic} specialised to the task (\eg Google's \gls{tpu}~\autocite{abadi2016tensorflow}), a \glspl{gpu}, or an \glspl{fpga}.
Vector extensions are part of the movement towards the \gls{simd} computing paradigm where multiple pieces of data are used and produced by a single instruction (see \rsec{simd}).
An advantage of vector extensions in relation to external accelerators is that a vector extension can access data through the same memory hierarchy used by the CPU while most external accelerators require data transfer over the slower PCIe connection.

The current state of the art when working with matrix operations is to choose one of several libraries that implement the \gls{blas} interface (\eg OpenBLAS~\autocite{xianyi2012model}, IBM's ESSL~\autocite{ibm2021engineering}, Intel's MKL~\autocite{wang2014intel,intel2021accelerate}, Nvidia's cuBLAS~\autocite{nvidia2021cublas}).
These libraries can provide incredible speedups and, in parallel architectures, automatic parallelisation: a very attractive feature for large workloads.
It has been a natural extension of these libraries to include the usage of matrix engines when targeting \glspl{cpu}.
However, the development of most of these libraries rely on difficult-to-maintain assembly programs and their usage imposes  extra requirements on systems.

All high-performance implementations of \gls{blas}-like libraries have handwritten assembly kernels at their core, though the extent varies~\autocite{zee2016blis}.
Nevertheless, each of these kernels must be produced and hand tuned for each new \gls{cpu} that the library needs to support.
Creating these kernels requires engineers who are extremely knowledgeable about the \gls{isa} and the architectural details of a target \gls{cpu}.
Thus, maintaining the code surrounding such a kernel as well as the kernel itself requires significant ongoing effort as developers try to obtain greater performance from the implementation.

Additionally, using a library means that user code now has an external dependency.
In applications where performance and correctness are critical and must be tightly controlled by the developer, using libraries may be an impossibility.
Similarly, while finding a \gls{blas} implementation on major platforms is often quite easy, some target platforms may not have an available implementation with which this dependency can be fulfilled~\autocite{zee2016blis}.
This is a direct product of the manual kernel porting difficulties described above.

Therefore, as a counterpoint, a method of transparently accelerating matrix multiplication via a compiler-only path is important for portability and maintainability.
When a matrix-multiplication kernel is created as part of a compiler, it no longer needs to be written in assembly.
Relaxing this requirement means that kernels become \emph{\gls{cpu} agnostic} rather than \emph{\gls{cpu} dependent}.
It will not be \emph{\gls{isa} agnostic} because a kernel, to obtain best performance, will still need to be written in terms of an \gls{isa}'s vector extension, tying it to the architecture, but it will not be tied to a specific version of the \gls{isa}.
The change to \gls{cpu} agnostic does, however, mean that  optimisations that are based on architectural details such as register count or available functional units, which would typically be done manually in a hand-written kernel, can now be automatically performed and tuned to the \gls{cpu} by the compiler.
This automation means that an appropriately written and parameterised kernel will also still be optimal in future hardware iterations.

Kernels written as part of a compiler are also implicitly subject to all of the optimisations available in the compiler, now and in the future.
Currently, these include optimisations such as loop switching, blocking, and unrolling which are known to have significant effects on matrix multiplication speed.
In the backend, improvements to processes such as register allocation and instruction scheduling (reordering/pipelining) will be retroactively available to these kernels as well, completely transparently to the user.

Furthermore, a crucial optimisation, loop fusion, is impossible with the current paradigm of library function calls.
Mathematically, consecutive matrix operations (\eg $D = ABC$) can be fused so as to perform both computations at once instead of producing a temporary result (\ie $\text{T}=AB;D=TC$).
Fusing operations in this manner significantly reduces the memory movements required by removing the need to store and reload the whole-matrix temporary value. That temporary matrix is likely to have been removed from cache if the matrices are large.
A compiler has the foresight and tools to fuse these operations and then further optimise the resulting code.

A compiler-only solution for matrix multiplication, while benefitting from preexisting features of a compiler, nevertheless requires
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}, label={}, afterlabel={}, after={.}]
  \item knowledge of the compiler framework, target operation, and the target architecture
  \item forethought based on the constraints derived from this knowledge
  \item an actionable design built from this forethought
\end{enumerate*}
As in a hand-written solution, required knowledge includes an understanding of the matrix engine and vector extensions of the target architecture as well as how they interact with each other and with the memory hierarchy.
However, because the kernel is written at a higher level within the compiler it counters many of the issues associated with the hand-written version.
Authors are now more productive by only having to focus on writing the kernel; registes and the data in them are now automatically managed, the instruction schedule can be optimised by the compiler to hide latencies, loops are unrolled according to architecture capacity, and more.
The produced kernels are more portable, functioning on \glspl{cpu} with the same \gls{isa} (in some cases cross-\gls{isa}) with much of the infrastructure being portable between architectures.
Finally, maintenance is also easier: higher level languages are more readable with less expertise required to interact with them.

The method of delivering the kernel must also be considered.
Libraries are the currently preferred method of acceleration and therefore any replacement method should aim to require less effort if it hopes to be adopted.
Library download and installation is oftentimes a simple barrier to overcome for the average user whereas the majority of users are loath to find and benchmark combinations of compiler flags in an effort to optimise their program.
They would prefer instead to assume that the preset, curated optimisation levels (\ie \code{-O1}, \code{-O2}, \code{-O3}) are sufficient.
Therefore, a compiler-only solution should strive to be as transparent as possible when it comes to enabling easy adoption.

% \bk{
%   Some possible theses:
% }
% \nelson{Here are alternative statement: \\
% This thesis presents a compiler-only path for the code generation for the general matrix multiplication operation that leverages hardware-supported matrix-multiplication engines.
% An evaluation of this new compiler-only path indicates that it produces code that is .... in comparison with tailored library implementations. \\
% This thesis addresses the following questions:
%   Is it possible to implement a compiler-only code generation path for  the generation of code for the general matrix multiplication operation that leverages hardware-supported matrix-multiplication engines?
%   How efficient is the code generated by this compiler-only path in comparison with tailored library implementations?
%   Is the compiler-only path for code generation portable to multiple architectures? (not sure if the data provides evidence to support claims about the third question here).
% }
% \bk{No, it would only be portable to Power11, which we don't know anything about yet.}
% \begin{itemize}
%   \item
%     \bk{A question as a statement..?}
%     \nelson{Are these two properties, easy-to-adopt and transparent, sufficient? Do users care about performance? I think that both easy-to-adoptness and transparency are quite difficult to measure or to estimate. Thus, the thesis would be posing a question that is very difficult to answer.}
%     This thesis addressed the question of whether or not a compiler-only method of matrix-multiplication acceleration using matrix engines can be implemented in an easy-to-adopt and transparent way.
%   \item
%     \bk{Contribution presentation.}
%     This manuscript presents an easy-to-adopt, transparent method of accelerating matrix-multiplication code using matrix engines.
% \end{itemize}


\begin{quote}
The main contribution of this thesis is a compiler-only path for code generation for the \gls{gemm} operation.
This path leverages hardware support in the form of matrix-multiplication engines.
An evaluation of this compiler-only path shows that the generated code is competitive with handwritten, \gls{cpu}-tailored library implementations.
The presented method is also easily adaptable and transparent to the user: it replaces a pre-existing matrix-multiplication kernel generator automatically on the \gls{power10} platform.
This versatility is shown through an implementation in the \gls{llvm} (\rsec{llvm}), an industry-grade compiler and related tools.
\end{quote}

\rcha{background} explains background information required to understand the rest of the manuscript.
\rcha{related} details related works and research.
\rcha{matmul} discusses the intricacies of contemporary matrix multiplication optimisation and presents a path to improving it.
\rcha{mma} presents \gls{power10}'s \gls{mma}, the facility which enables us to follow the aforementioned path.
\rcha{method} describes the method which uses the \gls{mma} facility to implement an efficient matrix multiplication kernel.
\rcha{evaluation} evaluates the presented method in various use cases and contrasts its performances with a variety of comparison points.
\rcha{conclusion} summarises the manuscript and presents closing thoughts.

% \nelson{
%   Everything that you have in this chapter should move to the background chapter.
%   The introduction should explain the problem that you are solving.
%   Re-read the introductions to the KernelFarer paper and to the PACT paper to get your mind into what an introduction should say.
%   You can either talk about the same ideas and white your own, or you can reuse some of that text if you have a paragraph saying that the text appears in those multi-author manuscripts.
% }
% \nelson{
%   The introduction should contain a clear problem statement or thesis statement.
%   Someone reading only the introduction should be able to clearly state the contribution of a thesis.
% }

% \begin{itemize}
%   \item
%     Set up issues: matrix multiply is in high demand as HPC and AI tasks grow in size and importance.
%   \item
%     Present matrix engines as the ``next generation'' solution.
%     They don't require extra hardware, specialised (TPU, etc.) or not (GPU, FPGA).
%   \item
%     Motivate the problem: We need automated code generation, in a compiler, to support these new matrix engines.
%     Automated generation of efficient code is more versatile and productive than assembly-level code writing for each new matrix-engine architecture.
%   \item
%     Set up problem: code generation requires
%     \begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}, after={.}]
%       \item ease of access to users to facilitate adoption
%       \item understanding of hardware (memory heirarchy, ME facility properties)
%     \end{enumerate*}
%   \item
%     Referencing adoption: discuss how libraries
%     \begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}, after={.}]
%       \item are the default high performance implementation
%       \item require installation and maintenance
%       \item promote bad development habits (kernel written in assembly, specialised to hardware versions)
%     \end{enumerate*}
%   \item
%     Referencing understanding of hardware: discuss how initial SIMD hardware and algorithms parallel the needs of matrix engines.
%     \bk{
%       This idea comes from a discussion on the PACT 2021 paper where we wanted to relate say ``codegen for new hardware'' is an important contribution.
%       I'm not sure if there's a better hardware example or a better way to go about relating this to codegen.
%     }
%   \item
%     Present first contribution as a lowering algorithm that addresses the above problems by
%     \begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}, after={.}]
%       \item implementing in a production compiler (LLVM) and implementing a generic lowering under and easy to use intrinsic.
%       \item carefully considering memory and hardware constraints to produce high performance code.
%     \end{enumerate*}
%   \item
%     Second contribution is the evaluation of the implementation.
%   \item
%     Expand slightly and point to sections.
% \end{itemize}

% \section{Contributions}
% This work focuses on how \gls{mma} can improve matrix multiplication on PowerPC
% This work addresses multiple questions of interest when considering MMA.
% \begin{enumerate}
%   \item
%     Does an \gls{mma} kernel perform better than the vectorised equivalent?
%   \item
%     Does \gls{mma} offer greater benefits when using different datatypes?
% \end{enumerate}

\end{document}
