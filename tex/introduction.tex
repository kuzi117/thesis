% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Introduction}
\label{cha:intro}

\nelson{Everything that you have in this chapter should move to the background chapter. The introduction should explain the problem that you are solving. Re-read the introductions to the KernelFarer paper and to the PACT paper to get your mind into what an introduction should say. You can either talk about the same ideas and white your own, or you can reuse some of that text if you have a paragraph saying that the text appears in those multi-author manuscripts.}

\nelson{The introduction should contain a clear problem statement or thesis statement. Someone reading only the introduction should be able to clearly state the contribution of a thesis.}

\begin{itemize}
  \item Introduction to \gls{power}/\gls{power10}.
  \item Introduction to MMA as an extension to VSX/SIMD.
  \item Introduction to matrix multiplication, optimisation (point to \rcha{related}).
\end{itemize}

\section{SIMD Architecture}
First introduced as part of the ILLIAC IV~\autocite{barnes1968illiac}, processors capable of \gls{simd} computation were a crucial advancement in high-performance computing history.
\todo{Join better.}
Flynn characterised a \gls{simd} processor as having the ability to apply a single ``master'' instruction over a vector of related operands~\autocite{flynn1972some}.
Such an architecture is desirable for its ability to increase the throughput of repeated operations through data-level parallelism and is thus highly applicable to one of the most standard computing control flow mechanisms: loops.
This process of increasing the throughput of loops, through what is essentially a reduction in the number of instructions executed, has developed into what is now the well-known and well-studied process of ``\gls{vectorisation}''.

\section{The IBM Power Architecture}
\Gls{ibm} has decades of history in computing, tracing their roots back to the 1880s; first incorporated as the Computing-Tabulating-Recording Company in 1911, it was eventually renamed to \gls{ibm} in 1924~\autocite{ibmarchive}.
Throughout this history, \gls{ibm} has made a point of innovating and pioneering numerous technologies in hardware, software, and the intermingling of the two.
One such innovation was the \gls{power} \gls{isa}.

The \gls{power} \gls{isa} was first announced in 1990 along with its primary instantiation in the \gls{ibm} System/6000~\autocite{montoye1990design}.
The System/6000, a \gls{risc}, implemented new features such as register renaming and out-of-order execution via the Tomasulo algorithm~\autocite{tomasulo1967efficient}.
Previously these features were only available in the \gls{ibm} System/360 mainframe.

Years later, in August 2020, \gls{ibm} remains competitive in its technology offerings with the announcement of the tenth generation of the \gls{power} \gls{isa}, aptly named \gls{power10}.

\subsection{SIMD History in IBM Power}
Initially, the \gls{power} architecture implemented its \gls{simd} capabilities through a common standard named \gls{altivec}.
\Gls{altivec} was designed through a collaboration between \gls{ibm}, Apple, and Motorola and described a \gls{simd} instruction set for floating-point and integer values.
This implementation, presented under the name \gls{vmx}, was first instantiated by IBM as part of \gls{power}6 (\gls{power} \gls{isa} v2.03) in 2007~\autocite{eisen2007ibm} despite the standard being presented in 1999~\autocite{tyler1999altivec}.

Improvements for \gls{power}7 (\gls{power} \gls{isa} v2.06) added a new facility, called \gls{vsx}, designed to add even further manipulation capabilities when dealing with vectors.
\bk{Expand.}
Both continue to exist to this day in the most recent version of the \gls{isa} (\gls{power} \gls{isa} v3.1), though many refer to them collectively under the second name, \gls{vsx}.

\subsection{Matrix Math Assist}
\label{sec:mmaintro}
As part of the new \gls{power10}'s offerings, \gls{ibm} has implemented a new facility, dubbed \gls{mma}, into the \gls{isa}, continuing \gls{ibm}'s commitment to providing cutting edge hardware for the software needs of the current era.
\gls{mma} is an addition to \gls{power}'s preexisting \gls{vmx} and \gls{vsx} \gls{simd} facilities.
It provides a high-throughput and low-latency method for calculating matrix multiplications.

The state of the art for hardware that performs matrix multiplication falls into two categories:
\begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}, after={.}]
  \item via external components (\eg \gls{gpu}, \gls{fpga}, or \gls{asic} such as Google's \gls{tpu})
  \item via \gls{cpu} \gls{isa} extensions (\eg x86's \gls{amx} or ARM's Neon/\gls{sve})
\end{enumerate*}
\Gls{mma} belongs to the second category given that the facility is built directly into the \gls{cpu}.
Thus, the facility is amenable to tasks that would normally incur avoidable overhead, for example, while sending data to an external component such as a \gls{gpu}.
See \rcha{mma} for further discussion.

\section{Matrix Multiplication}
Matrix multiplication has long been intrinsic to scientific computation and it is well known that many neural-network operations are based on matrix-matrix or vector-matrix multiplication~\autocite{rojas1996neural,blue1992training}.
Optimisation of this operation therefore has the potential to improve many tasks in both \gls{hpc} and business domains.

The operation itself, in its most naive form, is quite simple:
\begin{lstlisting}[caption={[Basic Matrix Multiplication]A basic matrix multiplication.},label=lst:basicmatmul,language=C++,columns=flexible,morekeywords=uint64_t]
for (uint64_t i = 0; i < M; ++i)
  for (uint64_t j = 0; j < N; ++j)
      for (uint64_t k = 0; k < D; ++k)
        C[i][j] += A[i][k] * B[k][j];
\end{lstlisting}
\bk{The above listing and description seem out of place and potentially useless. The listing may be pertinent but the explaining paragraph needs to be improved.}
The \code{i} loop iterates the rows of \code{A}, also choosing a row in \code{C}, while the \code{j} loop iterates the columns of \code{B}, also choosing a cell in the chosen row of \code{C}.
The final \code{k} loop iterates through the elements of the chosen row of \code{A} and column of \code{B}, reducing each of the pair-wise products into the chosen cell of \code{C}.

Due to its simple structure, tight loop layout, but potentially long execution time, matrix multiplication is well positioned to be improved in both software and hardware.
Seminal research such as that by Goto and van de Geijn~\autocite{goto2008anatomy} has resulted in high performance software implementations, \eg EIGEN\todo{cite} and BLAS\todo{cite}.
These implementations often outperform naive implementations of matrix multiplication by hundreds or thousands of times, turning hours of serial work into seconds or less.
This speedup is possible through the use of hardware features, such as \gls{simd}, and software optimisations, such as loop switching.

However, when one looks more closely, it is hard to classify any optimisation as hardware or software only.
\Gls{simd}, though it is a hardware feature, enables software to execute less instructions while loop switching's true function is optimising data reuse in the hardware cache.
It is for this reason that a new feature such as \gls{mma} can be so impactful on the optimisation of this operation.
See \rcha{matmul} for further discussion.

\section{LLVM}
\label{sec:llvm}
\Gls{llvm}, originally an initialism for Low-Level Virtual Machine, is a compilation framework devised by Chris Lattner~\autocite{lattner2002llvm,lattner2004llvm}.
Lattner originally positions \gls{llvm} as a ``unique multi-stage optimisation system'' that aims to ``support extensive inter-procedural and profile-driven optimisations, while being efficient enough for use in commercial compiler systems''~\autocite{lattner2002llvm}.
\Gls{llvm}, now a mononym, has evolved into a project covering a wide range of compilation-related tools including frontends for many languages, an optimiser, backends for many platforms, a \glslink{linking}{linker}, debugger, and several other projects.

Frontends exist for a multitude of languages including C/C++ (\code{clang}), Fortran (\code{flang}), Swift (\code{swiftc}), and Rust (\code{rustc}).
Backends also exist for a wide variety of platforms, such as X86, ARM, PowerPC, and WebAssembly.
The \gls{llvm} optimiser, named \code{opt}, is a popular target compiler research of all varieties, seeing advancements in various areas like register allocation~\autocite{lozano2019combinatorial,pereira2008register}, pointer analysis~\autocite{hardekopf2009semi,sui2016interprocedural}, and polyhedral optimisation~\autocite{grosser2011polly,alves2015runtime}.
\bk{
  There are obviously a ton of papers in each of these areas, I stopped only after two examples.
  If I should, how do I say these are only several examples.
}
The framework has also been adopted by several large companies who have based their own products off the ever-improving set of open-source tools.
These include \gls{ibm}'s XL C/C++ compiler and Nvidia's CUDA compiler.

\subsection{LLVM's Intermediate Representation}
\label{sec:ir}
The main pillar upon which the \gls{llvm}'s compilation pipeline is built is its \gls{ir}.
An \gls{ir} is a programming language in its own right, though its intended use is a frontend-language-agnostic and backend-target-agnostic intermediary language.
In this way, all frontends may target their \gls{lowering} toward producing a single shared language and all backends may consume a single shared language to produce their platform specific assembly.
Given this shared middle point, it is easy to create an optimiser which both consumes and produces the single intermediate language.
Thus, any combination of high-level language and destination platform may benefit from new or improved optimisations in the optimiser.

\Gls{llvm} \gls{ir} is known for being strongly typed and maintaining much of the high level type information from the original input.
It can also be annotated with large amounts of debugging information all while remaining easily serialisable.
A critical feature of the \gls{ir} is that it is in \gls{ssa} form.

\Gls{ssa} was formalised by Cytron \etal~\autocite{cytron1989efficient} though its use was seen as a side effect in previous works~\autocite{rosen1988global,alpern1988detecting}.
Efficient methods for its construction followed shortly after~\autocite{cytron1991efficiently,brandis1994single}.
Intuitively, the main property of a program in \gls{ssa} form is that each variable in the program text (static) is assigned to exactly once (single assignment); this property does not preclude executing an assignment multiple times at runtime, potentially with different values.

\noindent
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[caption={[Example C program, pre-SSA conversion.]An simple example C program, pre-conversion to \gls{ssa}.},
      label=lst:cSSA,numbers=none,language=c,columns=flexible]
int foo(int a) {
  int x = a + 2;
  int y = x + 10;
  x = y * 4;
  return x;
}
\end{lstlisting}
\end{minipage}
\hspace{.025\linewidth}
\noindent
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{lstlisting}[caption={[Example LLVM IR program, post-SSA conversion.]The same simple program, converted to \gls{llvm} \gls{ir} in \gls{ssa} format.},
      label=lst:llSSA,numbers=none,language=llvm,columns=flexible]
define i32 @foo(i32 %a) {
entry:
  %x.1 = add i32 %a, 2
  %y.1 = add i32 %x.1, 10
  %x.2 = mul i32 %y.1, 4
  ret i32 %x.2
}
\end{lstlisting}
\end{minipage}

A simple C program, shown in \rlst{cSSA} is converted to the SSA format in \rlst{llSSA} via a simple renaming scheme.
The first reference to a variable is suffixed with a one, and all future references increment the counter by one.
This process continues for the entirety of a function body, regardless of any control flow.
There are further complications in the construction algorithm in the presence of control flow but as this work presented in this thesis functions within a single \gls{basic block}, this explanation is left to Cytron \etal's work.
This format is significantly more conducive to analysis and optimisation than the source language, allowing for a much easier data flow analysis.
\bk{
  I feel like I should expand here but I'm unsure which direction.
  Should I explain more of why SSA is a good idea?
  Should I explain more properties and the structure of LLVM IR?
  More of SSA seems off topic and the structure of the IR takes a lot longer to describe than is reasonable.
}

\section{Contributions}
This work focuses on how \gls{mma} can improve matrix multiplication on PowerPC
This work addresses multiple questions of interest when considering MMA.
\begin{enumerate}
  \item
    Does an \gls{mma} kernel perform better than the vectorised equivalent?
  \item
    Does \gls{mma} offer greater benefits when using different datatypes?
\end{enumerate}

\end{document}
