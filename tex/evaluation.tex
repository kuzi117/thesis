% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Evaluation}
\label{cha:evaluation}
This chapter presents an evaluation of the implementation developed in \rcha{method}.
The results presented also represent one of the first evaluations of \gls{power10}'s \gls{mma} facility.
Given that the evaluation was performed on first-silicon version of \gls{power10} which runs at a lower frequency than will be featured in commercially available versions, the results are representative of the hardware that will eventually be available to consumers but performance of the commercial machines will be different.
Changes between this version and the commercial versions includes clock frequency, firmware updates or even silicon updates.

\section{Experimental Setup}
This section presents details on resources and processes that are necessary to produce the results in \rsec{caveats} and onwards.

\subsection{Machine Details}
The experimental platform is a pre-commercial \gls{ibm} \gls{power10} machine that was made available through a research collaboration with \gls{ibm}.
The processor is not yet available to the public but relevant details are presented in \rtab{machineInfo}.
The listed core counts are per socket while the thread counts are per core.
The machine runs Linux with a 64-bit kernel at version 5.10.0-17496-g41bc5268c5e8.

\begin{table}[t]
  \centering
  \begin{tabular}{c | c}
    & IBM POWER10\\\hline
    Cores/Threads & 15/8\\
    L1i cache & 720KiB\\
    L1d cache & 480KiB\\
    Frequency & 3.65 GHz\\
  \end{tabular}
  \caption{POWER10 machine info.}
  \label{tab:machineInfo}
\end{table}

\subsection{Compilation}
All binaries are compiled with Clang version 13.0.0\footnotemark   at the highest optimisation level (\code{-O3}) and are tuned to the processor (\code{-mcpu=pwr10}).
\footnotetext{At time of writing this is a development version.}
Because support for \gls{power10} was not available in mainstream \gls{llvm} when this work began, it has been implemented as part of \glslink{ibm}{IBM's} variant of \gls{llvm} which did have support for \gls{mma} instructions.
This variant contains several platform-specific optimisations to improve \gls{power} code; all these specific optimizations have been disabled so that results are representative of mainstream \gls{llvm}.

\subsection{Experimental Methodology}

\nelson{There is lots that is in your mind but not in the text in this section. Sounds like you should have a first paragraph that talks about the Google\texttrademark{} benchmark library\footnotemark~\autocite{googlebench} method in general first. And then a second paragraph that talks about the specific use of that method in this chapter. Sounds like the low variability enabled a simplification of the method but this is mixed with other information and sentences in this text now.}

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[scale=1]
    \draw[decorate, decoration={brace}, align=center, shift={(0, 0)}] (0, 0) -- node[above=2] {$n_0$ iters} +(2, 0);
    \draw[decorate, decoration={brace}, align=center, shift={(2.125, 0)}] (0, 0) -- node[above=2] {$n_1$ iters} +(2, 0);
    \node[shift={(4.5, 0)}] {$\ldots$};
    \draw[decorate, decoration={brace}, align=center, shift={(4.875, 0)}] (0, 0) -- node[above=2] {$n_{24}$ iters} +(2, 0);
    \draw[decorate, decoration={brace}, align=center, shift={(1, .8)}] (0, 0) -- node[above=2] {25 repeats} +(4.875, 0);
    \foreach \x in { 0, 2.125, 4.875 }
    {
      \pgfmathsetmacro{\xStart}{\x + 0.1}
      \foreach \i in {0,...,18}
      {
        \draw[shift={(\xStart + \i * 0.1, -0.1)}] (0, 0) -- +(0, -0.3);
      }
    }
  \end{tikzpicture}
  \caption{Experiment execution process.}
  \label{fig:execution}
\end{figure}

Every measurement presented in this chapter was produced using the Google\texttrademark{} benchmark library\footnotemark~\autocite{googlebench}.
\footnotetext{At commit ab74ae5e104f72fa957c1712707a06a781a974a6.}
A single measurement is the result of timing the execution of a kernel many times until statistical stability is obtained.
The framework implements a method of measuring a kernel by placing it in a loop whose exit condition is dynamically determined by the benchmark being executed.
The loop will only exit when the kernel has been executed enough times.

To implement this method, a loop trip count, $n$, is determined by
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}, label=(\arabic*), after={\footnotemark.}]
  \item timing several executions of the benchmark
  \item \nelson{Why 1.4? Why we are using double thee default of 0.5 s?}multiplying the required time (set at one second)\footnote{One second is double the default, suggested half-second time-limit.} by $1.4$
  \item dividing by the average execution time of those executions
\end{enumerate*}
\footnotetext{\nelson{Which process?}This process can be verified in any upcoming table using $n \times \text{cpuTime} \approx 1.4e9$.}
\nelson{What "multiple values" refer to?}
The true process is more iterative and tries multiple values in order to produce a conservative estimate for benchmarks with variation in their execution times.
For the kernels studied in this chapter, which have very little variation, this is an accurate summary of the process.

\bk{Double check these numbers when they're finalised.}
While a single second may seem short, for the longest-running kernel presented (\atilde\SI{81}{\textit{\micro\second}}), this still results in roughly 17000 executions.
For the shortest-running kernel (\atilde\SI{14}{\textit{\nano\second}}), the kernel is executed nearly 100 million times.

In \rfig{execution}, producing one measurement (\ie completing $n$ iterations of the loop) corresponds to a single of the lower braces labeled ``$n_i$ iters''.
\nelson{What value is reported after measuring 25 times?}
A total of 25 measurements are produced in this way, corresponding to the upper brace in \rfig{execution}, to reduce the impact of system fluctuations in any execution.
Repetitions vary $n$ slightly because the timing iterations may produce slightly different base times, changing the iteration count.

\nelson{which loops?}
The framework presents statistics for each of the 10 loops including wall clock time, \gls{cpu} time, and cycles.
\bk{I think I can add instructions retired if it's a good idea.}
It also presents overall statistics between each of the ten runs: mean, median, and standard deviation.
These overall statistics are the values presented throughout the coming sections.
Vertical bars in plots represent the overall mean while error bars represent the 95\% confidence interval derived from the standard deviation.
Tables accompany each plot with execution count ($n$), mean \gls{cpu} time, mean cycles, and their respective confidence intervals.

\subsection{Types Missing From Analysis}
Three types are missing from the following analysis for different reasons.
The \code{i4} and \code{bfloat} types use the same \gls{ir} code generation code paths as every other type except \code{double}.
Given that the implemented logic works for \code{i8}, \code{i16}, \code{half}, and \code{float}, it is likely that the code generated for them is functional and as performant as the other types.
However, while translating to assembly, the backend produces either incorrect code (\code{i4}) or an error (\code{bfloat}).
Both issues have been acknowledged by the developer team and will be fixed in future versions of \gls{llvm}.

\nelson{What "changed operation dimensions" means?}
For \code{double}, the changed operation dimensions cause a significant divergence in code-generation logic that otherwise applies to all other types usable with \gls{mma}.
Within the scope of this thesis, the development efforts focus on an algorithm that worked for six of the seven types.
The method presented in \rcha{method} applies to \code{float} but its \gls{lowering} implementation will require changes in relation to the original code.

\section{Caveats}
\label{sec:caveats}
There are some unexpected performance results reported in \todo{link future results section}.
The unexpected performance is caused either by issues in the backend code generator or by limitations inherent to the compilation framework, both of which are outside of the scope of this thesis.

Before presenting the results, this section discusses some of the causes for the unexpected performance results.
These issues are addressable in the future and the performance study in this thesis uncovered opportunities for improving code generation for the entire \gls{power} software stack.
The presented \gls{mma} kernel is a new type of kernel never before seen by the backend and therefore the assembly code generation has not been as tuned as other kernels have been.
Only after stripping away certain bottlenecks present in the naive kernel can we learn about new issues that are still to be addressed by the backend design team.

\subsection{Spilling}
In compiler terminology, a \gls{live} value is a value that may be used by a statement in the future.
Ideally, once a value is brought from memory into a register or it is computed and placed into a register, it should remain in the register until it is \gls{dead} --- a value is dead when it is guaranteed that it will never be used again.
However, there are often not enough registers available in an architecture to keep all live values in register.
Therefore, the compiler performs analysis to decide how to judiciously select some live values to be temporarily \glslink{spill}{spilled} to memory in order to temporarily free registers for new values.
Thus, \glslink{spill}{spilling} occurs when new values must be brought from memory but all registers at that \gls{point} in the program have a \gls{live} value.
For small matrix-multiplication kernels, all values may be resident in register simultaneously or may only be used once and thus do no remain \gls{live} across future loads.
Efficient spilling requires careful scheduling but in the case of \gls{mma} the problem is made more difficult by the following issues.

\subsubsection{Framework Vector Loads}
The implementation described in \rcha{method} is integrated within a pre-existing framework that provides the default vectorisation lowering method described in \rsec{matMulInt}.
This framework contains data structures to support operations with matrices.
These data structures transform the load of a single long vector representing a flattened matrix into several vector loads representing the matrix's column or row vectors, depending on the original access order.
For example, a floating-point column-major matrix \mat{A}{4}{2} is originally represented by \code{<8 x float>} and then transformed into two \code{<4 x float>} column-vector loads.
After the transformation, these load instructions in the \gls{ir} remain consecutive and occur before any computation.
An examination of the assembly code generated by the backend reveals that such a program is translated by the backend into assembly that brings \emph{all} of the matrix's elements to registers immediately and consecutively.
Given that
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; or }}, label=(\arabic*), after={,}]
  \item the matrix may be larger than all available registers
  \item some registers may contain values from other matrices
\end{enumerate*}
many of the loaded values are immediately \glslink{spill}{spilled} back to memory.
These values must be reloaded later when they are needed, effectively tripling the time spent on memory accesses.
Memory operations are considerably slower than \gls{mma} operations, even though the values may be going to and from the L1 cache, and thus the runtime of the kernel becomes dominated by memory operations.

\begin{figure}[t]
  \centering
  \input{tex/figures/sinkFloats.pgf}
  \caption{The effect of load sinking on \code{float} and \code{i16} matrix multiplications.}
  \label{fig:floatSink}
\end{figure}

\begin{table}[t]
  \centering
  \makebox[\textwidth][c]{
    \begin{tabular}{| c | c | c | c |}
      \hline
      Name & $n$ & CPU Time (\SI{}{\textit{\nano\second}}) & Cycles\\\hline
      \code{float}, $8 \times 8 \times 16$, No Load Sinking & $25623185.92 \pm 623715.04$ & $54.84 \pm 1.39$ & $219.11 \pm 5.57$ \\\hline
      \code{float}, $8 \times 8 \times 16$, Load Sinking & $97308628.60 \pm 144463.04$ & $14.44 \pm 0.05$ & $57.67 \pm 0.19$ \\\hline
      \code{float}, $32 \times 8 \times 32$, No Load Sinking & $2146145.16 \pm 50283.11$ & $654.30 \pm 14.86$ & $2614.07 \pm 59.39$ \\\hline
      \code{float}, $32 \times 8 \times 32$, Load Sinking & $2352684.80 \pm 51746.61$ & $596.81 \pm 13.12$ & $2384.39 \pm 52.42$ \\\hline
      \code{i16}, $8 \times 8 \times 16$, No Load Sinking & $91419216.40 \pm 35690.47$ & $15.31 \pm 0.00$ & $61.18 \pm 0.02$ \\\hline
      \code{i16}, $8 \times 8 \times 16$, Load Sinking & $80431238.20 \pm 148030.83$ & $17.42 \pm 0.01$ & $69.61 \pm 0.03$ \\\hline
      \code{i16}, $32 \times 8 \times 32$, No Load Sinking & $3368892.16 \pm 18135.72$ & $415.63 \pm 2.21$ & $1660.53 \pm 8.85$ \\\hline
      \code{i16}, $32 \times 8 \times 32$, Load Sinking & $2230348.60 \pm 7729.44$ & $627.73 \pm 2.19$ & $2507.92 \pm 8.75$ \\\hline
    \end{tabular}
  }
  \caption{
    The effect of load sinking on \code{float} and \code{i16} matrix multiplications.
    See \rfig{floatSink} for graphical presentation.
    \bk{Is this better with just minimum iteration count?}
  }
  \label{tab:floatSink}
\end{table}

This spilling issue must be resolved by the backend scheduler\footnotemark: the scheduler needs to move loads closer to the \gls{point} in the program that contains the first use of the loaded value to reduce register pressure by using a technique called ``load sinking''.
\footnotetext{The backend design team is currently investigating how to implement this change to the scheduler.}
Load sinking may not reduce register pressure in many other programs, but in the case of this matrix-multiplication kernel it certainly does.
A workaround for the spilling resulting from unsunk loads, which can be used until the backend scheduler improves, consists of modifying the \gls{ir} by further breaking up the vector loads into smaller vector loads and sinking these smaller loads closer to the first use of the loaded values.

In preparation for the performance evaluation in this thesis, a short targeted \gls{ir} transformation was implemented using this tactic.
Its goal was to produce results that are more representative of the code that will be generated by a final production-ready backend.
Consider the pair of bars labeled ``8x8x16'' (\matmul{8}{8}{16}) for \code{float} (left) in \rfig{floatSink}.
This matrix size represents the best-case scenario for such an optimisation:
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}, label={\protect\circled{\arabic*}}, after={.}]
  \item $A$'s eight-element column-vector load can be broken up into two four-element vector loads, each of which fills an entire \gls{vsr} (similar logic applies to $B$'s rows)
  \item only a single load is necessary to create an outer-product operand (rank-one update)
  \item each operand is used only once during the accumulation step ($2+4=6$ registers \gls{live} at once)
\end{enumerate*}
\bk{If plots are updated, update these numbers.}
Given such a perfect case, the runtime of the original version (labeled ``No Load Sinking'') is roughly quartered (\rtab{floatSink}, rows 1 and 2) in the optimised version (labeled ``Load Sinking'').
If the operation were to have more accumulations, then this gap would continue to grow as more and more spills would be required in the unoptimised version.

However, by examining the bars labeled ``8x8x16'' for \code{i16} (right) in \rfig{floatSink} we can see that for smaller types load sinking actually causes a slight slow down (\rtab{floatSink}, rows 5 and 6).
This slowdown can be attributed to the invalidation of properties \circled{1} and \circled{2}: loading an \code{i16} operand no longer fills an entire register and two loads are necessary to create an operand for \code{i16} (rank-two update).
In fact, the assembly produced without load sinking is much closer to the handwritten version because it loads multiple vectors into a single register and uses these as source vectors for multiple shuffle instructions.
The load-sinking version loads only a single vector into a register, increasing register pressure, and uses each vector for only a single shuffle instruction.
The performance degradation because of load-sinking in this case is small only because the overall register pressure has been decreased by halving the element width.

\subsubsection{Operand Spilling and Rematerialisation}
After addressing the load-sinking issue in this way, another, separate but related, issue appears.
As can be seen from the pair of bars labeled ``32x8x32'' for \code{float} in \rfig{floatSink}, the same fix that produced significant speedup for a smaller kernel has significantly reduced in effectiveness for a larger kernel.
An examination of the assembly reveals that the large number of loads that led to spills at the beginning of the kernel prior to sinking the loads are indeed no longer there.
However, while there are no spills throughout the body of the smaller kernel, the larger kernel has multiple spills or reloads per set of outer products.
The issue here is that, for the larger kernel, property \circled{3} no longer holds true.

Consider the example in \rfig{opReuse}.
The figure shows an example large kernel ($16 \times 32$) in the style of \rfig{intrinsic}; to illustrate how spills occur, the figure does not divide individual elements nor does it show $B$.
On the right, darker lines divide the output into blocks of accumulators while the lighter lines divide blocks into the eight accumulators as in \rfig{intrinsic}.
Each of the smaller boxes on the left represents an operand composed of four \code{float} elements used in an outer-product computation.

An example operand from $A$, highlighted by parallel diagonal lines, must be used in the accumulation of the first set of accumulators, highlighted by a crosshatch pattern.
In the computation of the smaller kernel in \rfig{floatSink}, this value is now \gls{dead}.
The larger kernel, however, must use the value again in computing the second set of accumulators, highlighted by a dotted pattern.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[scale=1/2]
    \draw[xstep=1, ystep=2, shift={(0, 0)}] (0, 0) grid +(2, 8);
    \draw[pattern=north west lines, shift={(0, 6)}] (0, 0) rectangle +(1, 2);
    \draw[pattern=crosshatch, shift={(3, 6)}] (0, 0) rectangle +(8, 2);
 %   \draw[pattern={Hatch[angle=45,distance={8pt},xshift=.2pt, line width=1.25]}, shift={(3, 6)}] (0, 0) rectangle +(8, 2);
    \draw[pattern=dots, shift={(11, 6)}] (0, 0) rectangle +(8, 2);
 %   \draw[pattern={Dots[angle=45,distance={7pt},xshift=.1pt, radius=1.25]}, shift={(11, 6)}] (0, 0) rectangle +(8, 2);
    \draw[xstep=2, ystep=2, shift={(3, 0)}, color={black!20!white}] (0, 0) grid +(16, 8);
    \draw[xstep=8, ystep=4, shift={(3, 0)}] (0, 0) grid +(16, 8);
    \draw[decorate, decoration={brace, mirror}] (0, -0.3) -- node[below=2] {$K$} +(2, 0);
  \end{tikzpicture}
  \caption{Demonstration of operand reuse.}
  \label{fig:opReuse}
\end{figure}

According to constraint \fbox{5} in \rsec{baseCase}, spilling an accumulator is expensive, and therefore an efficient solution must finish computing a block of accumulators before moving on to a different area of the matrix.
Doing so requires that every operand along the dimension $K$ be brought to register before any operand can be reused with a new block of accumulators.
For instance, an accumulator layout like that in \rfig{opReuse} requires six operand registers for each accumulation to be performed.
Using only the 32 registers available while the accumulators are assembled, if $K \geq \left\lceil \frac{32}{6} \right\rceil = 6$, then there is not enough space for all values to remain in registers.
And thus the cause for the multiple spills and reloads throughout the computation becomes apparent: operands from $A$\footnotemark that are still \gls{live} are being forced out of registers and the compiler is choosing to spill them.
\footnotetext{The issue is identical for operands of $B$, though the reuse distance is greater.}

\bk{I've changed the discussion of rematerialisation, let me know if it's better.}
This code generation evokes the well-studied compiler technique ``\gls{rematerialisation}''.
Essentially, the compiler analysis must answer the question ``is it more efficient to store a value to memory and retrieve it later (\gls{spill}) or to recompute it later using the same process?''

In the case of floats, producing the operand is a simple load and therefore, if an operand \emph{must} be evicted from its register, \glslink{rematerialisation}{rematerialising} the value is a simple load.
Thus, the compiler's choice to spill the operand is incorrect and, because spills are saved on the stack, not only it is wasting memory by duplicating the value, but it is also potentially evicting values from the cache.
Regardless, the larger kernel's execution, optimised or not, is significantly influenced by memory operations, though the gains from reducing the initial spills still have significant impact.

For smaller datatypes, however, spilling is the correct choice.
Materialising an \code{i16} involves two loads and at least one instruction to shuffle the loaded data together.
Rematerialising the value in the future becomes a simple load, significantly reducing register pressure and required memory bandwidth.
Both the original version and load-sinking version correctly choose to spill their values instead of \glslink{rematerialisation}{rematerialising} them.
As with \code{float}, the increased number of memory operations explains the significant increase in execution time for the larger \code{i16} kernel in \rfig{floatSink}, but it does not, however, explain the large difference between the two version.
The next section investigates this discrepancy.

\subsubsection{Smaller Types}
While the issues affecting \code{float} matrices presented above remain roughly identical for \code{double}-typed operations, small datatypes have a separate source of slow down.
The difference in performance is actually an exacerbation of an already-discussed issue, namely the invalidation of properties \circled{1} and \circled{2}.
For \code{i16}, in the load-sinking version, a single 64-bit double-word load instruction loads four \code{i16} elements at the same time.
The original version loads 16 elements at a time using 256-bit vector-pair loads, meaning that the load-sinking version quadruples the number of loads required to load the values needed for the computation (excluding loads from the original spilling issue).
Changing the scheduling to remove the unnecessary loads and stores would likely lead to a much more significant the performance gap between the two versions.

This issue extends to the other data types, which perform rank-two, -four, or -eight updates.
The issue is aggravated when the number of memory requests increases because of the shrinking data width that require more vectors for shuffling.
For example, where, for \code{i16}, the load-sinking version takes roughly 114\% of the execution time of the original version, the same comparison for \code{i8} shows that the load-sinking version takes 292\% of the execution time of the original version.

A more complicated load breaking and sinking strategy may counteract this slowdown, but only to a certain degree.
Combining certain loads -- for example two four-element \code{i16} column loads, making a full \gls{vsr} -- would decrease register pressure because the upper and lower halves of the register can be used in separate shuffles.
However, this still does not quite match the 256-bit paired-vector load in terms of memory-request efficiency.
It is impossible for smaller types to use this 256-bit load because the columns of $A$ are consecutive: the paired-vector load would load values that are not used until the second row of accumulator blocks, inevitably introducing new spills.

The issue worsens for \code{i8} and \code{i4} where the eight consecutive vertical elements fill less and less of a \gls{vsr}.
For larger kernels, the smaller data types thus imply a increasing number of loads, further diluting the speedup gained from \gls{mma}.

\subsection{Shuffling}
For datatypes smaller than 32 bits, producing the appropriate operand for a rank-$r$ update requires several vectors be merged via shuffling.
The \gls{power} \gls{isa} offers several methods to move vector elements, ranging from register shifting and rotating combined with masking to efficient vector upper and lower half merges based on datatype size.
Transforming \gls{ir} statements in an efficient code for shuffling is an involved design process.

There are a multitude ways to express the same shuffling operation in \gls{ir}, each depending on multiple implementation choices made throughout the process.
Certain variants are interpreted by the backend and translated to efficient code while others are interpreted very conservatively.
\nelson{"This"}
This can be the difference between an idiomatic assembly of several instructions, often close to an expert's handwritten version, and the same process being expressed in hundreds or thousands of instructions, often with \glspl{spill} throughout.
Several factors that lead shuffling to cause performance degradation are discussed below.

\subsubsection{Datatype: \texorpdfstring{\code{half}}{half}}
\label{sec:halfShuffle}
The \code{i16} and \code{half} types have identical widths: a halfword.
Because \glspl{cpu} and their accompanying \glspl{isa} are type-agnostic and sensitive only to the width of a data element when implementing vector shuffles, shuffle code in the \gls{ir} that differs only in choice of 16-bit type should produce identical assembly.
However, in the assembly code currently generated  by the compiler backend  for \code{i16} executes roughly six vector loads and six vector permute instructions to produce all operands for a single accumulation in a full $2 \times 4$ accumulator layout.
For \code{half}, the generated assembly code has gone from six instructions to almost 150 instructions because thee computation is partially scalarised by moving elements from vector registers to general purpose registers and back again.

This explosion in code size is most apparent in the calculation epilogue where, in the \gls{ir}, the accumulators are disassembled and stored to memory.
First, the resulting accumulator vectors are truncated to the output data type.
They are then concatenated together into an output flattened matrix and stored to memory.
Concatenation is also achieved through shuffling and therefore experiences the same conservative code generation as before.
The added truncation operation worsens the effect.
All told, in the assembly code currently being generated the epilogue for \code{half} is roughly ten times the length of the same operation for \code{i16}.

\subsubsection{Datatype: \texorpdfstring{\code{i4}}{i4}}
For \code{i8}, degradations arise from \glspl{spill} due to extra register pressure but also from needing to merge four separate vectors (rank-four update).
Despite this, generated code is relatively close to the ideal, handwritten assembly.
As mentioned before, there exists instructions for merging vectors at a per-byte granularity, but not a at a sub-byte granularity as is required for \code{i4} code generation in the same style.
This does not mean that it is not possible to shuffle a vector of \code{i4} elements using the same \gls{ir} code generation, only that assembly generated by the backed will be significantly less performant.
Specifically, this includes vector rotations and scalarisation, with single bytes being moved individually to and from general purpose registers.
This slow down negates much of the speed up that could be achieved via the \gls{mma} rank-eight update.

\subsection{Solutions}
This section presents potential solutions though it is unlikely that the ideas introduced will solve the performance degradations without further investigation and cooperation with the backend developers.

\subsubsection{Spilling}
\label{sec:spillSolution}
The simplest solution to the noted \glslink{spill}{spilling} issues is for developers to focus on using only small kernels and, indeed, this is not so unreasonable a proposal as one might perceive it at first.
While \gls{llvm} necessitates that certain sequences of \gls{ir} always be able to be lowered to assembly, it does not require that the code be as performant as possible, as is demonstrated by the above issues with shuffling \code{i4} vectors.
This may seem like a defeatist notion but is actually a tried-and-true approach to performance in compilers: make the common case fast.

The average user interacts with a compiler through a high-level language, not its \gls{ir}.
Thus, the frontend will choose the fastest solution to implement a user's code; if the fastest implementation is a small kernel encapsulated by an outer kernel then that is what will be chosen.
With this notion, optimisation can be focused on this most common case for which a solution is known.
A larger inner-kernel will always remain usable but it will be less performant.

Furthermore, the most performant library implementations focus on using small unrolled assembly kernels, often computing kernels with dimensions of four to eight elements.
These small kernels are then surrounded by memory-managing outer kernels as described in \rsec{outerKernel}.
This is exactly what Kuzma \etal~\autocite{kuzma2021fast} do while using the inner kernel presented in this thesis with results comparable to libraries with kernels written in assembly.
In this way, using only smaller kernels can be seen as a well-worn path to performance.

If a larger kernel \emph{must} be used, significant investigation must be done to test load scheduling, value \gls{rematerialisation}, load breaking, and, if necessary, load recombination.
It is unlikely that such a solution will be found within the \gls{ir} because .
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}, label=(\arabic*), after={.}]
  \item it is in \gls{ssa} form which often folds identical values together without consideration for the times when a value is \gls{live}
  \item for better or worse, the backend does not have to respect \gls{ir} operation order
\end{enumerate*}
Such a solution will have to be found in the backend.

\subsubsection{Shuffling}
Unlike issues with spilling, it is likely that performance degradations caused by misbehaving shuffles can be solved by creating type specific shuffling code.
While injecting the proposed solution code into the original vectorisation framework is possible, it would be much more effective to first refactor the framework so such a modification is easier.

For \code{half} vectors, it may be possible to fool the compiler into generating code as is done for \code{i16} simply by reinterpreting data as \code{i16} via casts and manipulating the data in this state.
The cast is not a conversion, simply a reinterpretation of the bits in a register, meaningful only within the \gls{llvm} framework and producing no new instructions.
The data can again be casted back to \code{half} at no cost when it needs to be used in a computation.
In theory, this produces much better assembly but may also trigger a very conservative response in the compiler, worsening the code significantly with scalar element movement derived from a perceived conversion hazard.

The sub-byte granularity of \code{i4} vectors pose a different problem.
Performant shuffling code for \code{i4} in the same style as the larger types is an impossibility.
Instead, clever use of bitwise operations will have to explicitly be used to implement efficient element extraction and movement.
Bitwise operations can be applied across an entire vector register, removing the need for scalarisation and improving register usage by reducing the number of required intermediate registers.

\subsubsection{Evaluation Disclaimer}
All evaluation presented in future sections have no targeted optimisations applied.
No tested solution positively affected all cases.
Thus, all test cases use identical \gls{ir} generation logic with no special cases.
The presented results are therefore equally advantaged and disadvantaged in code generation.

\section{Varied Access Orders}
\label{sec:variedOrders}
As discussed in \rsec{outerKernel}, packing is incredibly important for matrix multiplication performance.
An important part of packing, beyond choosing dimensions, is the choice of data access order.
\rsec{registerArguments} shows that \gls{mma} is sensitive to the access order of elements in \glspl{vsr}.
This inevitably translates to a sensitivity to the access order of the matrices in memory that provide the arguments in register.

\subsection{Setup}
When using an unmodified version of \gls{llvm}, the access orders of $A$, $B$, and $C$ are tied together and must be equal.
Thus two test cases, one all-column-major ($\textrm{C} = \textrm{C} \times \textrm{C}$) and another all-row-major ($\textrm{R} = \textrm{R} \times \textrm{R}$), constitute the baseline for the experiment.
In order to test the performance of the hypothesised best mix of orientations, further modifications to the lowering framework were performed to enable the mixing of orientations.
With this possibility enabled, two cases are added to the comparison: both use the proposed optimal layout for $A$ and $B$ but vary the layout of $C$ ($\textrm{C} = \textrm{C} \times \textrm{R}$ and $\textrm{R} = \textrm{C} \times \textrm{R}$)

Each kernel uses a $2 \times 4$ accumulator setup and therefore, in keeping with the suggestion in \rsec{spillSolution}, uses a small $8 \times 16$ output.
The inner most dimension of the kernel is 32, making the overall computation \matmul{8}{32}{16}.
An inner dimension of 32 requires 32 accumulations but smaller datatypes perform rank-$r$ updates, effectively reducing the number of operations required to compute the kernel.
It may seem that 32 accumulations is relatively large, but~\autocite{kuzma2021fast} determines that, even given the poor load scheduling, up to 128 accumulations per kernel gives good speed up, with performance plateauing with any more accumulations.
Given the discussion in \rsec{orderExpectation}, more accumulations implies more opportunities for a ``good'' orientation to outperform a bad orientation, effectively highlighting differences in performance.

\subsection{Expectation}
\label{sec:orderExpectation}
For $A$, it is expected that data laid out in a column-major orientation is optimal.
For \code{float}, while technically a row-major matrix when in \gls{vsr}, is indistinguishable from a single column of four elements.
Thus, if $A$ is laid out in column-major order, an operand can be created with a single load and no waste.
However, if $A$ is laid out in row-major order, it is likely the case that $K \ne 1$ and thus the elements in the first column of $A$ are not consecutive in memory.
Therefore, to create an operand, four loads must be issued, one for each element of the operand.
Further overhead is incurred to move elements into the vector register either from scalar registers if the four scalar loads were issued or to shuffle elements from four vectors that were loaded.
Thus, a column-major order layout is highly preferred.
The same logic can be applied to $B$, except that a row-major orientation is preferred.

For smaller types, the logic is more complicated because the byte-length of the overall loaded data also shrinks.
In the interest of not repeating much of the discussion in \rsec{caveats}, the same logic as above can be applied while acknowledging that as register usage efficiency decreases speedups also decrease.

An interesting point of discussion exists for types \code{i8} and \code{i4} whose in-register dimensions are $4 \times 4$ and $4 \times 8$ respectively.
Because two \code{i4} elements are packed into a byte, an eight-element vector is only four bytes.
This means that both \code{i8} and \code{i4} require loading four four-byte vectors to provide the elements to fill a \gls{vsr}.
Therefore, there will always be four loads issued whether the matrix that is being extracted from has a row-major or column-major order.
In this case, it would seem that either orientation is therefore acceptable.
This is true in the case that operands are being built in isolation.
However, if one considers that several operands are being built simultaneously, loads can be combined to increase efficiency.
Shuffle instructions do not decrease, only the indices from which shuffles extract elements in a vector change.
This optimisation can have a small but noticeable impact on the results.

Naively, $C$ optimally has a row-major orientation.
This is simply because the simplest method of using \gls{mma} places the operand from $A$ as the first argument and the operand from $B$ as the second argument in the outer product instruction.
When used in this way, the elements in the accumulator are oriented in a row-major fashion and, after disassembling, there is no extra work required before storing the underlying vectors to memory.
Transposing the values so that they can be stored into a column-major output-matrix requires extra shuffles.
However, with the theorem discussed in \rsec{arbitraryOrder}, it is shown that reversing the order of the arguments causes the result in the accumulator to be equivalent but in a column-major order.
Thus, it is expected that either access order can be used for $C$ without loss of performance.

\subsection{Analysis}
\begin{figure}[t]
  \centering
  \input{tex/figures/accessOrder.pgf}
  \caption{The effect of matrix access order on performance.}
  \label{fig:accessOrder}
\end{figure}

\rfig{accessOrder} visualises the results for this experiment.
For \code{float}, \code{i16}, and \code{i8}, the numbers are inline with the hypothesised performance.
The two order-locked variants perform worse than the optimal layout.
Furthermore, between the two kernels whose $A$ and $B$ are laid out optimally, varying the access order of $C$ has no perceptible effect on performance.

Based on the discussion in \rsec{orderExpectation}, it is also unsurprising to see the $\textrm{R} = \textrm{R} \times \textrm{R}$ test case outperform the $\textrm{C} = \textrm{C} \times \textrm{C}$ case.
Given that the order of $C$ does no affect performance, the difference must be found in $A$ and $B$.
In each case, one matrix is laid out ideally while the other is not.
Because $B$ is twice the size of $A$, when it is laid out poorly the generated assembly suffers more than when a bad choice is made for $A$.
Examining the generated assembly shows that for \code{float} the column only variant performs roughly 450 more instructions of ``setup'' (loading and shuffling values) before executing the first outer-product instruction; the generated code is effectively identical afterwards.
This gap lessens as types become smaller due to the smaller overall amount of bytes being loaded, but the difference in performance remains.

\subsubsection{Evaluating \texorpdfstring{\code{half}}{half}}
As discussed in \rsec{halfShuffle}, \code{half} and \code{i16} can and should be manipulated in exactly the same manner.
The only difference in their generated assembly should be the outer product instruction which interprets a register as a \code{half} or \code{i16}.
The \gls{power} \gls{isa} indicates that the two different outer product instructions also have identical latencies.
Thus, given identical load, store, and shuffle schedules along with identical outer-product latencies, the performance of the two types should be identical in every instance.

However, given the results in \rfig{accessOrder}, this is clearly not the case.
\todo{Add table with numbers.}
The most performant case for \code{half}, $\textrm{C} = \textrm{C} \times \textrm{C}$, still requires roughly three times the cycles of \code{i16}'s least performant cast, also $\textrm{C} = \textrm{C} \times \textrm{C}$.
Results from test cases using \code{half} that do not follow the anticipated trend are a common thread in this chapter.
The cause is undoubtedly the same issue discussed in \rsec{halfShuffle}: a code length explosion due to extremely conservative assembly code generation.
For this reason, performance can be expected to drastically increase and trends expected to follow that of \code{i16} when this issue is resolved.
Such a change is thus likely to invalidate any of the results presented for \code{half} in this and future experiments.
Results will continue to be presented for completeness but will not be discussed unless necessary.
\bk{Is this an acceptable statement?}

\section{Varied Accumulator Layout}
The layout of accumulators can significantly affect the performance of a kernel.
Both the input and output memory efficiency as well as the in-register performance can degrade due to a suboptimal layout.

\subsection{Setup}
Intuitively, leaving an accumulator unused means unused functional units and therefore wasted performance; thus, any layout which aspires to be a contender for best performance must use all eight accumulators.
The implemented framework allows only for rectangular layouts.
Combining these two restrictions creates four test cases: two rectangular layouts, $2 \times 4$ and $4 \times 2$, and two linear layouts, $1 \times 8$ and $8 \times 1$.

Again, using the logic of \rsec{spillSolution}, the kernel size is fit to the accumulator layout.
A kernel's dimensions are therefore four times the length of the same dimension of the accumulator layout (\eg layout $8 \times 1$ has kernel dimensions $32 \times 4$).
As in the previous experiment, each kernel performs 32 accumulations, though smaller types perform fewer outer product instructions.
Therefore, the overall kernel size is \matmul{(\textrm{\code{V}} \times 4)}{32}{(\textrm{\code{H}} \times 4)} where \code{V} and \code{H} correspond to the number of vertical and horizontal accumulators in the layout as described in \rsec{baseCase}.
Additionally, using the result from \rsec{variedOrders}, all kernels use a $\textrm{R} = \textrm{C} \times \textrm{R}$ layout for their matrices.

\subsection{Expectation}
\label{sec:layoutExpectation}
First, the two rectangular layouts and the two linear layouts are expected to have equal performance.
Without loss of generality, one can abstractly think of executing an accumulation as performing loads from $A$, performing loads from $B$, performing shuffles for each if necessary, and then performing the outer product.
Transposing a layout means swapping the number of loads from $A$ and $B$ with the overall total remaining the same.
Likewise, the number of shuffles and outer products also remains the same.
In this way, the total work remains the same and therefore the performance should be identical.
Any difference in performance will certainly indicate a missed optimisation in one of the compilations.

\rsec{baseCase} discusses a proposed ideal layout for the eight accumulators available in \gls{power10} \gls{mma}.
The two proposed rectangular layouts, $2 \times 4$, and its transpose, $4 \times 2$, have the highest factor of reuse, with operands from one matrix being reused four times and an operand the other matrix being used twice.
Moreover, it effectively reduces register pressure by requiring only six elements to be \gls{live} before performing eight consecutive accumulations.
With 32 total registers, using only six per accumulation allows for the arguments of future accumulations to be brought to register ahead of time or even for multiple accumulations to be performed uninterrupted.

However, the argument in \rsec{baseCase} is made under the assumption that the most efficient schedule for outer-product instructions is a large set of loads followed by as many outer products as possible.
In this case, the more square layout should be more performant because it requires less operands per accumulation (six instead of eight) and therefore allows more accumulations to be setup simultaneously.
If this is not the case, and instead interleaving loads with outer products is more performant, then either set of layouts can potentially be more performant, as long as the latency for the larger number of loads required to setup the linear layout can be effecitively hidden.
In fact, if the linear layouts can activate the hardware prefetcher more readily, there is a potential for speedup that may not be found in the rectangular layouts.

\subsection{Analysis}
\begin{figure}[t]
  \centering
  \input{tex/figures/tightAccLayouts.pgf}
  \caption{The effect of accumulator layout on performance.}
  \label{fig:tightAccLayout}
\end{figure}

The results of the experiment are shown in \rfig{tightAccLayout}.
All three of \code{float}, \code{i16}, and \code{i8} meet the expectation that the rectangular layouts and linear layouts each have roughly identical performance.
\bk{
  It seems the more ``vertical'' of the layouts occasionally takes longer.
  It's not consistent though as for \code{float}, the $4 \times 2$ layout takes longer while the $8 \times 1$ is identical to its transpose.
  For \code{i16}, the $8 \times 1$ takes longer than its transpose and the rectangular layouts are indentical.
  I have no reason to believe it's due to ``extra loads'' and am inclined to blame it on load latencies.
  I'm not sure where to start investigating this beyond moving back to the simulator and collecting traces for analysis.
}
However, where \code{i16} and \code{i8} agree with the theory in \rsec{layoutExpectation}, \code{float} shows opposing results.

Initial analysis of this difference hoped that, for \code{float}, the $1 \times 8$ layout and its transpose had found a better load schedule or that it had activated hardware such as the prefetcher in a way that was not possible with the data types which load less data overall.
It is partially true that a better schedule was found, though not in the way that was anticipated.
Instead, the changed arguments have simplified the backend code generator's analysis, likely its \glslink{live}{liveness} analysis, allowing it to find ways to deconflict \gls{live} values in order to create a simpler load schedule.
This simplification has thus crucially resulted in the removal of the initial spills present in every test case presented so far.
In fact, given the kernel laid out for it, the assembly is very close to the optimal, handwritten assembly, lacking only the small optimisation to combine 128-bit vector loads which use consecutive addresses into a paired-vector 256-bit load.
For comparison's sake, activating the load sinking optimisation for the rectangular layouts produces assembly resembling almost exactly the assembly in the tested linear layouts\footnotemark.
\footnotetext{The load sinking assemblies also contain the more efficient paired-vector loads though it is likely due to an optimisation pass ordering issue, not deliberate choice.}
By examining the assembly for the rectangular test case with load sinking, however, it is easy to see that the total number of vectors loaded between accumulations is less than that in the linear layout.
In both cases the only instructions present are the loads and the outer products which means there are no instructions with which to hide a stall due to load latency.
Thus it can be expected that equivalently-scheduled \code{float} testcases would follow the expected trend because, with less loads, there is less likely to be a load-based stall.

Regarding \code{i16} and \code{i8}, the shuffles necessary to build operands serve to add extra complication to the \glslink{live}{liveness} analysis and thus the same simplified schedule cannot be found for the linear layouts.

\section{More experiments}
\begin{itemize}
  \item Comparison between:
  \begin{itemize}
    \item Matmul with default llvm code path.
    \item Matmul with default llvm intrinsic.
    \item Matmul with llvm intrinsic + mma.
    \item Matmul with libraries.
    \item Matmul on GPU?
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item Small matrices vs big matrices (call costs).
\end{itemize}

\end{document}
