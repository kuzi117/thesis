% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Evaluation}
\section{Experimental Methodology}
\begin{itemize}
  \item
    Machine info:
    \begin{itemize}
      \item CPU used (P10 obviously).
      \item Frequencies (if we can?).
      \item Cache properties (hopefully available).
      \item Main memory properties.
    \end{itemize}
  \item
    Experimental procedure.
    \begin{itemize}
      \item Run count.
      \item Run order.
    \end{itemize}
  \item
    LLVM info.
      \begin{itemize}
        \item All matrix inputs to \code{llvm.matrix.multiply} are column major.
        \item Could be row major.
      \end{itemize}
\end{itemize}

\section{MMA vs Vectorization}
\label{sec:mmaVsVec}
To begin, we consider a performance comparison between different methods of \gls{lowering} the \gls{llvm} intrinsic \code{llvm.matrix.multiply}.
The first lowering is the current state-of-the-art lowering provided by \gls{llvm} using \gls{vectorisation} while the second method uses the \gls{lowering} method contributed as part of this thesis (\todo{rsec method}).
Such a comparison will help us investigate several research questions:
\begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}]
  \item can \gls{mma} provide speedup over \gls{vectorisation}?
  \item which accumulator layout offers the greatest speedup?
  \item do some data types offer greater speedup?
\end{enumerate*}

We must first acknowledge restrictions on inputs as described in \rsec{restrictions}.
When working with each data type, the input matrices must have dimensions which are multiples of the appropriate argument dimension.
For example, when considering \code{xvi4ger8}, the $A$ matrix must have a vertical dimension which is a multiple of four and a horizontal dimension which is a multiple of eight.
For the $B$ matrix, the restrictions are transposed with the dimensions.

Further restriction comes from the hardware accumulator count.
Through the magic of an \gls{ir}, it is possible to have ``more'' than eight accumulators simultaneously active, but doing so will necessarily cause a resident accumulator to \gls{spill} to memory during \gls{lowering}.
Therefore, experimentation must also be restricted to eight or less accumulators so as to not cause unneccessary memory operations to confound results.
With this knowledge in mind, we perform two experiments in order to gain insight into the questions listed above.

\subsection{Multiple Layouts, Single Accumulation}
\label{sec:mlsa}
In the first, we vary the accumulator tiling layout but supply inputs which cause only a single operation to be accumulated into each accumulator.
We perform this test for all investigated data types (\todo{see table/section of types}) in combination with every layout whose dimensions are a power of two.
Specifically, given an \gls{mma} instruction whose input dimension is $(i, j)$ (\eg $(4, 8)$ for \code{i4}, see \rtab{mmainsts}), to create accumulator layout $m \times n$:

\begin{equation}
  \begin{split}
    a = i * m, \text{ } b &= i * n, \text{ } c = j\\
    \underset{(a \times c)}{A} \times \underset{(c \times b)}{B} &= \underset{(a \times b)}{C}
  \end{split}
\end{equation}

\begin{table}[t]
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline
    $1 \times 1$ & \matmul{4}{8}{4} \\\hline
    $2 \times 1$ & \matmul{8}{8}{4} \\\hline
    $1 \times 2$ & \matmul{4}{8}{8} \\\hline
    $2 \times 2$ & \matmul{8}{8}{8} \\\hline
    $4 \times 1$ & \matmul{16}{8}{4} \\\hline
  \end{tabular}
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline
    $1 \times 4$ & \matmul{4}{8}{16} \\\hline
    $4 \times 2$ & \matmul{16}{8}{8} \\\hline
    $2 \times 4$ & \matmul{8}{8}{16} \\\hline
    $8 \times 1$ & \matmul{32}{8}{4} \\\hline
    $1 \times 8$ & \matmul{4}{8}{32} \\\hline
  \end{tabular}
  \caption[Experiment Setup: Multiple Layouts, Single Accumulation]{Accumulator layouts and the corresponding size-matched computation for the \code{i4} type.}
  \label{tab:mlsaSetup}
\end{table}

This equation is applied in \rtab{mlsaSetup} to generate all of the tested layouts for the \code{i4} type.
For example, using a tiling layout of $4 \times 1$ accumulators with the \code{i4} type requires a computation of \matmul{(4 \times 4)}{8}{(4 \times 1)}.

\todo{Find a better way to refer to the contributed method.}
The expectation for performance within the provided \gls{lowering} method is that the greatest speedup should be achieved when using all eight accumulators in a $4 \times 2$ or $2 \times 4$ layout.
When comparing the two implementations, for small data types (less than 32 bits), there is a potential that vectorisation is slower than the contributed method, even with a suboptimal accumulator configuration.
A na\"ive implementation will not see the potential for concatenating two input vectors while performing a product and will therefore waste computation potential by not filling all register elements.
Concretely, using a $1 \times 1$ layout of accumulators with the \code{i16} type produces the computation \matmul{4}{2}{4}.
Even though the entire matrix $A$ fits into a single vector register ($4 \times 2 \times 16 \text{ bits} = 128\text{ bits}$), a single column of $A$ fills only half ($4 \times 16 \text{ bits} = 64 \text{ bits}$).
Given this, the implementation using \gls{mma} will use the whole input in a single operation while the \gls{vectorisation} option will use half of the input in multiple computations.

\subsubsection{Analysis}
\label{sec:mlsaAnalysis}
\begin{itemize}
  \item Examining only the mma speedups should show an optimal layout using all eight accumulators in some orientation
  \item
    Comparison between mma and vectorisation of types <32 bit: vectorisation should be slower due to extra/wasted operations since column size doesn't match vector size.
    If mma \emph{is} slower then the cost of the operation is too high while vectorisation could overcome wasted data through pipelining.
  \item
    Comparison between mma and vectorisation of types >=32 bit: vectorisation should be faster for small accumulator layouts but slower for full accumulator layouts.
    Reference optimal layout size from previous examination.
\end{itemize}

\subsection{Multiple Layouts, Multiple Accumulation}
\label{sec:mlma}
In a second experiment, we find the lowest common multiple of vector register length and dimension induced by accumulator layout and use this as the input dimension.
In this way we attempt to remove all disadvantages to the \gls{vectorisation} method while still maintaining a low computation count for the contributed method.
Again, we perform this for all investigated data types in combination with every layout whose dimensions are a power of two.
To produce the dimensions for a given given accumulator layout $m \times n$, with \gls{mma} instruction who input dimension is $(i, j)$ and data type size in bits is $s$:

\begin{equation}
  \begin{split}
    \text{vreg\_elems} &= 128\text{ bits} / s\\
    \text{a} &= \text{lcm}(\text{vreg\_elems}, i * m)\\
    \text{b} &= \text{lcm}(\text{vreg\_elems}, i * n)\\
    \text{c} &= \text{lcm}(\text{vreg\_elems}, j)\\
    \underset{(a \times c)}{A} \times \underset{(c \times b)}{B} &= \underset{(a \times b)}{C}\\
  \end{split}
\end{equation}

\begin{table}[t]
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline$1 \times 1$ & \matmul{8}{8}{8} \\\hline
    $2 \times 1$ & \matmul{8}{8}{8} \\\hline
    $1 \times 2$ & \matmul{8}{8}{8} \\\hline
    $2 \times 2$ & \matmul{8}{8}{8} \\\hline
    $4 \times 1$ & \matmul{16}{8}{8} \\\hline
  \end{tabular}
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline
    $1 \times 4$ & \matmul{8}{8}{16} \\\hline
    $4 \times 2$ & \matmul{16}{8}{8} \\\hline
    $2 \times 4$ & \matmul{8}{8}{16} \\\hline
    $8 \times 1$ & \matmul{32}{8}{8} \\\hline
    $1 \times 8$ & \matmul{8}{8}{32} \\\hline
  \end{tabular}
  \caption[Experiment Setup: Multiple Layouts, Multiple Accumulation]{Accumulator layouts and the corresponding vector-size-matched computation for the \code{half} type.}
  \label{tab:mlmaSetup}
\end{table}

Applying this to the \code{half} type as an example, we produce the dimensions seen in \rtab{mlmaSetup}.
We can see that for small accumulator layout dimensions, the computation size is dictacted by the vector size ($8 \times 16 \text{ bits} = 128 \text{ bits} = \text{vreg\_size}$).
The larger accumulator layout dimensions, however, eventually override this ($32 \times 16 \text{ bits} = 512 \text{ bits} = 4 \times \text{vreg\_size}$).

This experiment expects to see the \gls{vectorisation} method perform better overall due to better register usage.
The provided method has slightly more complicated predictions.
Small accumulator layouts for small types should perform worse due to the serialisation of accumulations along both dimensions with reduced amortisation through pipelining accumulation into multiple accumulators.
This effect should be reduced as type size increases, eventually making inputs nearly the same size as in \rsec{mlsa} and thereby reducing the number of serialised accumulations.
Therefore, results for larger types should reduce to close to those seen in \rsec{mlsaAnalysis}.

\subsubsection{Analysis}
\begin{itemize}
  \item Results should match expectations, any deviations are interesting talking points.
\end{itemize}

\subsection{Multiple layouts, Fixed Size}
\bk{
  Is there value in doing the same iteration of layout sizes but with a fixed input size.
  For example, for \code{float} choose \matmul{64}{8}{64} which offers a horizontal and vertical accumulator tiling of (16, 16) and an accumulation count of 8.
  This would magnify the speedup from using multiple accumulators but also magnify the slowdown from the serialisation of small layouts.
}

\section{More experiments}
\begin{itemize}
  \item Comparison between:
  \begin{itemize}
    \item Matmul with default llvm code path.
    \item Matmul with default llvm intrinsic.
    \item Matmul with llvm intrinsic + mma.
    \item Matmul with libraries.
    \item Matmul on GPU?
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item Small matrices vs big matrices (call costs).
\end{itemize}

\end{document}
