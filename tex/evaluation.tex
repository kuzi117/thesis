% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Evaluation}
\label{cha:evaluation}
This section presents an evaluation of the implementation developed in \rcha{method}.
The results presented also represent one of the first evaluations of \gls{power10}'s \gls{mma} facility.
Given that the evaluation was performed on test~\bk{There's a better word for this, I'm sure.} hardware, the results are expected to be representative of the hardware that will eventually be available to consumers but not exactly the same.
This could be due to anything from frequency changes to firmware changes or actual silicon changes.

\section{Experimental Methodology}
\begin{itemize}
  \item
    Machine info:
    \begin{itemize}
      \item CPU used (P10 obviously).
      \item Frequencies (if we can?).
      \item Cache properties (hopefully available).
      \item Main memory properties.
    \end{itemize}
  \item
    Experimental procedure.
    \begin{itemize}
      \item Run count.
      \item Run order.
    \end{itemize}
  \item
    LLVM info.
      \begin{itemize}
        \item All matrix inputs to \code{llvm.matrix.multiply} are column major.
        \item Could be row major.
      \end{itemize}
\end{itemize}

\section{Caveats}
The results presented in \todo{link future results section} are unexpected and less than ideal.
However, the cause of the lackluster performance, in general, is not a result of the implementation presented in \rcha{method}.
Instead, missed performance can be traced to several factors in the backend code generator as well as limitations inherent in the surrounding framework.

\nelson{You are  using lots of passive voice. Try change the sentences  to active voice. They will become clearer. I changed them in this paragraph.}
Before presenting the results, this section explains sources of performance degradations.
The results are counterintuitive, but they reveal issues that are addressable and represent opportunities for improving code generation for the entire \gls{power} software stack.
The MMA kernel is a new type of kernel generated by the backend and therefore the assembly code generation has not been  as tuned as other kernels have been.
It was only after stripping away certain bottlenecks present in the original kernel that we learn which other bottlenecks remain.

\subsection{Spilling}
\glslink{spill}{Spilling} occurs when new values must be brought from memory but all register's at that \gls{point} in the program have a \gls{live} value.
To make space for the new value, one register must be \glslink{spill}{spilled} to memory.
For small kernels, all values may be resident in register simultaneously or may only be used once and so do no remain live across future loads.
There is not enough register space for large kernels to have all operands and results simultaneously in register.
This issue is solveable with careful scheduling but is exacerbated by the following issues.

\subsubsection{Framework Vector Loads}
\label{sec:test}
The implementation described in \rcha{method} integrates itself within a pre-existing framework which provides the default vectorisation lowering method described in \rsec{matMulInt}.
The data structures for working with matrices that were created as part of the framework take the load of a single long vector representing a flattened matrix and replace it with several vector loads representing the matrix's major axis values.
For example, a floating-point column-major matrix \mat{A}{4}{2} will originally be represented by \code{<8 x float>} and will be transformed into two \code{<4 x float>} column-vector loads.
Regardless of the form this loading takes -- be it one large load or several smaller, consecutive loads -- this load occurs in the IR before all computation.
By examining the generated assembly, \bk{Should I attach an appendix with example code? It seems excessive.} we can, for large kernels, see that this sort of code has been interpreted by the backend as a request to bring \emph{all} of the values to register immediately.
This translates to the entire matrix being loaded consecutively and, because the matrix may be larger than all available registers and values from other matrices must be present, it results in these values immediately being \glslink{spill}{spilled} back to memory.
These values must be reloaded later when it is their time to be used, effectively tripling the kernel's time spent on memory accesses.
The memory operations are considerably slower than \gls{mma} operations, even though the values may be going to and from L1 cache, and thus the runtime of the kernel becomes dominated by memory operations.

\begin{figure}
  \centering
  \input{tex/figures/sinkFloats.pgf}
  \caption{The effect of load sinking on floating point matrix multiplications.}
  \label{fig:floatSink}
\end{figure}

This issue should be a consideration of the backend scheduler\footnotemark: moving loads for values that are not needed at the current \gls{point} in the schedule to another \gls{point} closer to its first use where there is potentially less register pressure (load ``sinking'').
\footnotetext{This issue is actively being investigated as part of future work.}
In general, for the average program, this may not be the case, but in the case of this matrix multiplication kernel it is well within reason.
As a stop-gap measure, this can be addressed in the IR by breaking up larger loads and sinking them closer to their uses.

A short, specifically-targeted optimisation was created to implement this idea to test the feasibility of presenting more accurate\footnotemark results in this manuscripts; a representative sample of the effects are shown in \rfig{floatSink}.
\footnotetext{Here, ``accurate results'' means ``optimal results given a more well-behaved backend code generator''.}
Consider the pair of bars labeled ``8x8x16'' (\matmul{8}{8}{16}).
This represents the best case scenario for such an optimisation:
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}, label=\textbf{(\arabic*)}, after={.}]
  \item $A$'s eight-element column-vector load can be broken up in to two four-element vector loads, each of which fills an entire VSR (similar logic applies to $B$'s rows)
  \item only a single load is necessary to create an outer product operand (rank-one update)
  \item each operand is used only during its accumulation step ($2+4=6$ registers \gls{live} at once)
\end{enumerate*}
\bk{If plots are updated, update these numbers.}
Given such a perfect case, the runtime of the original verison (labeled ``No Opt'') is roughly quartered\footnotemark in the optimised version (labeled ``Sink'').
If the operation were to have more accumulations then this gap would continue to grow as more and more spills are required in the unoptimised version.
\footnotetext{Changed from 113.6 ns to 28.5 ns on average.}

\subsubsection{Operand Spilling and Rematerialisation}
In solving the first issue, another, separate but related issue is born.
As can be seen from the pair of bars labeled ``32x8x32'', the same fix that worked for a smaller kernel produced a roughly 12\% increase\footnotemark in runtime for the larger kernel.
\footnotetext{Changed from 710.4 ns to 796.0 ns on average.}
Once again, by examining the assembly, it is easy to determine that original issue derived from the framework that created a large number of loads and spills in the beginning of the kernel remains solved.
However, where there once was no spills throughout the body of the kernel, the larger kernel has multiple spills or reloads per set of outer products.
The issue here is that, in the larger kernel, \textbf{(3)} no longer holds true.

For example, in \rfig{opReuse}, an operand from $A$ (diagonal lines) must be used in the accumulation of the first set of accumulators (crosshatch).
The first pair of bars in \rfig{floatSink} stop here.
The larger kernel, however, must use the value again in computing the second set of accumulators (dots).

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[scale=1/2]
    \draw[xstep=1, ystep=2, shift={(0, 0)}] (0, 0) grid +(2, 8);
    \draw[pattern=north west lines, shift={(0, 6)}] (0, 0) rectangle +(1, 2);
    \draw[pattern=crosshatch, xstep=2, ystep=2, shift={(3, 6)}] (0, 0) rectangle +(8, 2);
 %   \draw[pattern={Hatch[angle=45,distance={8pt},xshift=.2pt, line width=1.25]}, xstep=2, ystep=2, shift={(3, 6)}] (0, 0) rectangle +(8, 2);
 \draw[pattern=dots, xstep=2, ystep=2, shift={(11, 6)}] (0, 0) rectangle +(8, 2);
 %   \draw[pattern={Dots[angle=45,distance={7pt},xshift=.1pt, radius=1.25]}, xstep=2, ystep=2, shift={(11, 6)}] (0, 0) rectangle +(8, 2);
    \draw[xstep=2, ystep=2, shift={(3, 0)}, color={black!20!white}] (0, 0) grid +(16, 8);
    \draw[xstep=8, ystep=4, shift={(3, 0)}] (0, 0) grid +(16, 8);
    \draw[decorate, decoration={brace, mirror}] (0, -0.3) -- node[below=2] {$K$} +(2, 0);
  \end{tikzpicture}
  \caption{Demonstration of operand reuse.}
  \label{fig:opReuse}
\end{figure}

According to constraint \fbox{5} in \rsec{baseCase}, spilling an accumulator is expensive, and we must therefore fully compute an accumulator before moving on.
This means that every operand along the dimension $K$ will be brought to register before an operand will be reused.
Given only 32 registers and requiring six for each accumulation, if $K \geq \left\lceil \frac{32}{6} \right\rceil = 6$, then there will not be enough space for all values to remain in register\footnotemark.
\footnotetext{The issue is identical for operands of $B$, though the reuse distance is greater.}
And thus the cause becomes apparent: operands that are still \gls{live} are being forced out of registers and the compiler is choosing to spill them.

This is the well studied compiler problem of ``\gls{rematerialisation}''.
Essentially, the compiler must answer the question ``is it more efficient to store a value to memory and retrieve it later (\gls{spill}) or to recompute it later using the same process?''
In the case of floats, producing the operand is a simple load and therefore, if an operand \emph{must} be evicted from its register, \glslink{rematerialisation}{rematerialising} the value is a simple load.
Thus, the compiler's choice to spill the operand is incorrect and, because spills are saved on the stack, not only is it wasting memory by duplicating the value, it is potentially evicting values from the cache.

\subsubsection{Smaller Types}
While the issues presented above remain roughly identical for \code{double}-typed operations, small datatypes have several extra confounding factors.
First to consider is that the smaller datatypes, as described in \rsec{dataTypes}, require that two, four, or eight vectors be merged (shuffled) in order to create operands for the specific rank-$r$ update.
These vectors are not contiguous in memory which counteracts condition \textbf{(2)} that applied originally to \code{float}-typed matrices.
Together, these differences increase the cost of \gls{rematerialisation}, though it remains unclear at which point this is enough to offset the cost of \glslink{spill}{spilling}.
Investigating such a difference requires non-negligible changes in the backend making it firmly outside the scope of this thesis.

Complicating things further is that \textbf{(1)} is also no longer true.
A four-element \code{i16} vector only fills half of a VSR; usage only decreases for the smaller types.
Thus, register pressure actually increases as the data size decreases because a register is required to hold each of the unique loads that will be used for merging.

A more complicated load breaking and sinking strategy may counteract this, but only to a certain degree.
Combining certain loads -- for example two four-element \code{i16} column loads, making a full VSR -- would decrease register pressure as shuffles could use the upper and lower halves of the register for separate shuffles.
However, combining four four-element \code{i8} column loads begins to span multiple blocks of accumulators, as can be seen in \rfig{opReuse}.
This causes the VSR to be \gls{live} much longer as the upper two quarters are required for a different set of accumulators, likely introducing new spills.
Combining enough loads eventually returns to loading the entire column or row vector and allows the previously described issues to appear again.

\subsection{Shuffling}
For datatypes smaller than 32 bits, producing the appropriate operand for a rank-$r$ update requires several vectors be merged via shuffling.
The \gls{power} \gls{isa} offers several methods of moving vector elements, ranging from register shifting and rotating combined with masking to efficient vector upper and lower half merges based on datatype size.
Unfortunately, producing efficient code for shuffles in assembly from \gls{ir} is difficult.

There are a multitude of ways to express the same shuffling operation in \gls{ir}, each depending on multiple implementation choices made throughout the process.
Certain variants will be interpreted by the backend and translated to efficient code while others are interpreted very conservatively.
This can be the difference between an idiomatic assembly of several instructions, often close to an expert's handwritten version, and the same process being expressed in hundreds or thousands of instructions, often with \glspl{spill} throughout.
Several factors which cause degraded performance from shuffling are discussed below.

\subsubsection{Datatype: \texorpdfstring{\code{half}}{half}}
\bk{
  I think this issue is likely to affect \code{bfloat} as well but when I went to try it last night (I'd never compiled any \code{bfloat} code before) it crashed.
  I've spoken to Nemanja and it seems to be a backend issue; I've created a github issue for it and the backend team will look at it shortly.
}
The \code{i16} and \code{half} types have identical widths: a half word.
Because \glspl{cpu} and their accompanying \glspl{isa} are type-agnostic and sensitive only to the width of a data element when implementing vector shuffles, shuffle code in the \gls{ir} which differs only in choice of 16-bit type should produce identical assembly.
This is demonstrably untrue, however.
The assembly code generated for \code{i16} executes roughly six vector loads and six vector permute instructions to produce all operands for a single accumulation in a full $2 \times 4$ accumulator layout.
For \code{half}, the assembly has extended from six instructions to almost 150 and has been at least partially scalarised, moving elements from vector registers to general purpose registers and back again.

This explosion in code size is most apparent in the calculation epilogue where, in the \gls{ir}, the accumulators are disassembled and stored to memory.
First, the resulting accumulator vectors are truncated to the output data type.
They are then concatenated together into a output flattened matrix and stored to memory.
Concatenation is also achieved through shuffling and therefore experiences the same conservative code generation as before.
The added truncation operation worsens the effect.
All told, the epilogue for \code{half} is roughly ten times the length of same operation for \code{i16}.
\bk{
  I'm concerned there's an extra factor here in the truncation operation that needs to be considered.
  Going from \code{i32} to \code{i16} uses a ``vector pack'' instruction which is effectively just takes two vectors and truncates every word to a half-word and packs both into a single vector.
  I can't find a similar operation for the conversion from \code{f32} to \code{f16} which is a significantly more complicated process because the exponent bias is different.
  This might mean that it \emph{must} be scalarised which, of course, entails a longer epilogue.
}

\subsubsection{Datatype: \texorpdfstring{\code{i8}}{i8}}
\bk{Discuss issues about merging four vectors simultaneously.}

\subsubsection{Datatype: \texorpdfstring{\code{i4}}{i4}}
\bk{Discuss issues about merging elements that have sub-byte length.}

\subsection{Solutions}
\bk{Discuss how these issues might be best addressed.}

\section{MMA vs Vectorization}
\label{sec:mmaVsVec}
To begin, we consider a performance comparison between different methods of \gls{lowering} the \gls{llvm} intrinsic \code{llvm.matrix.multiply}.
The first lowering is the current state-of-the-art lowering provided by \gls{llvm} using \gls{vectorisation} while the second method uses the \gls{lowering} method contributed as part of this thesis (\todo{rsec method}).
Such a comparison will help us investigate several research questions:
\begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}]
  \item can \gls{mma} provide speedup over \gls{vectorisation}?
  \item which accumulator layout offers the greatest speedup?
  \item do some data types offer greater speedup?
\end{enumerate*}

We must first acknowledge restrictions on inputs as described in \rsec{restrictions}.
When working with each data type, the input matrices must have dimensions which are multiples of the appropriate argument dimension.
For example, when considering \code{xvi4ger8}, the $A$ matrix must have a vertical dimension which is a multiple of four and a horizontal dimension which is a multiple of eight.
For the $B$ matrix, the restrictions are transposed with the dimensions.

Further restriction comes from the hardware accumulator count.
Through the magic of an \gls{ir}, it is possible to have more than eight accumulators simultaneously active, but doing so will necessarily cause a resident accumulator to \gls{spill} to memory during \gls{lowering}.
Therefore, experimentation must also be restricted to eight or less accumulators so as to not cause unneccessary memory operations to confound results.
With this knowledge in mind, we perform two experiments in order to gain insight into the questions listed above.

\subsection{Multiple Layouts, Single Accumulation}
\label{sec:mlsa}
In the first, we vary the accumulator tiling layout but supply inputs which cause only a single operation to be accumulated into each accumulator.
We perform this test for all investigated data types (\todo{see table/section of types}) in combination with every layout whose dimensions are a power of two.
Specifically, given an \gls{mma} instruction whose input dimension is $(i, j)$ (\eg $(4, 8)$ for \code{i4}, see \rtab{mmaInsts}), to create accumulator layout $m \times n$:

\begin{equation}
  \begin{split}
    a = i * m, \text{ } b &= i * n, \text{ } c = j\\
    \underset{(a \times c)}{A} \times \underset{(c \times b)}{B} &= \underset{(a \times b)}{C}
  \end{split}
\end{equation}

\begin{table}[t]
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline
    $1 \times 1$ & \matmul{4}{8}{4} \\\hline
    $2 \times 1$ & \matmul{8}{8}{4} \\\hline
    $1 \times 2$ & \matmul{4}{8}{8} \\\hline
    $2 \times 2$ & \matmul{8}{8}{8} \\\hline
    $4 \times 1$ & \matmul{16}{8}{4} \\\hline
  \end{tabular}
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline
    $1 \times 4$ & \matmul{4}{8}{16} \\\hline
    $4 \times 2$ & \matmul{16}{8}{8} \\\hline
    $2 \times 4$ & \matmul{8}{8}{16} \\\hline
    $8 \times 1$ & \matmul{32}{8}{4} \\\hline
    $1 \times 8$ & \matmul{4}{8}{32} \\\hline
  \end{tabular}
  \caption[Experiment Setup: Multiple Layouts, Single Accumulation]{Accumulator layouts and the corresponding size-matched computation for the \code{i4} type.}
  \label{tab:mlsaSetup}
\end{table}

This equation is applied in \rtab{mlsaSetup} to generate all of the tested layouts for the \code{i4} type.
For example, using a tiling layout of $4 \times 1$ accumulators with the \code{i4} type requires a computation of \matmul{(4 \times 4)}{8}{(4 \times 1)}.

\todo{Find a better way to refer to the contributed method.}
The expectation for performance within the provided \gls{lowering} method is that the greatest speedup should be achieved when using all eight accumulators in a $4 \times 2$ or $2 \times 4$ layout.
When comparing the two implementations, for small data types (less than 32 bits), there is a potential that vectorisation is slower than the contributed method, even with a suboptimal accumulator configuration.
A na\"ive implementation will not see the potential for concatenating two input vectors while performing a product and will therefore waste computation potential by not filling all register elements.
Concretely, using a $1 \times 1$ layout of accumulators with the \code{i16} type produces the computation \matmul{4}{2}{4}.
Even though the entire matrix $A$ fits into a single vector register ($4 \times 2 \times 16 \text{ bits} = 128\text{ bits}$), a single column of $A$ fills only half ($4 \times 16 \text{ bits} = 64 \text{ bits}$).
Given this, the implementation using \gls{mma} will use the whole input in a single operation while the \gls{vectorisation} option will use half of the input in multiple computations.

\subsubsection{Analysis}
\label{sec:mlsaAnalysis}
\begin{itemize}
  \item Examining only the mma speedups should show an optimal layout using all eight accumulators in some orientation
  \item
    Comparison between mma and vectorisation of types $<32$ bit: vectorisation should be slower due to extra/wasted operations since column size doesn't match vector size.
    If mma \emph{is} slower then the cost of the operation is too high while vectorisation could overcome wasted data through pipelining.
  \item
    Comparison between mma and vectorisation of types $\geq 32$ bit: vectorisation should be faster for small accumulator layouts but slower for full accumulator layouts.
    Reference optimal layout size from previous examination.
\end{itemize}

\subsection{Multiple Layouts, Multiple Accumulation}
\label{sec:mlma}
In a second experiment, we find the lowest common multiple of vector register length and dimension induced by accumulator layout and use this as the input dimension.
In this way we attempt to remove all disadvantages to the \gls{vectorisation} method while still maintaining a low computation count for the contributed method.
Again, we perform this for all investigated data types in combination with every layout whose dimensions are a power of two.
To produce the dimensions for a given given accumulator layout $m \times n$, with \gls{mma} instruction who input dimension is $(i, j)$ and data type size in bits is $s$:

\begin{equation}
  \begin{split}
    \text{vreg\_elems} &= 128\text{ bits} / s\\
    \text{a} &= \text{lcm}(\text{vreg\_elems}, i * m)\\
    \text{b} &= \text{lcm}(\text{vreg\_elems}, i * n)\\
    \text{c} &= \text{lcm}(\text{vreg\_elems}, j)\\
    \underset{(a \times c)}{A} \times \underset{(c \times b)}{B} &= \underset{(a \times b)}{C}\\
  \end{split}
\end{equation}

\begin{table}[t]
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline$1 \times 1$ & \matmul{8}{8}{8} \\\hline
    $2 \times 1$ & \matmul{8}{8}{8} \\\hline
    $1 \times 2$ & \matmul{8}{8}{8} \\\hline
    $2 \times 2$ & \matmul{8}{8}{8} \\\hline
    $4 \times 1$ & \matmul{16}{8}{8} \\\hline
  \end{tabular}
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline
    $1 \times 4$ & \matmul{8}{8}{16} \\\hline
    $4 \times 2$ & \matmul{16}{8}{8} \\\hline
    $2 \times 4$ & \matmul{8}{8}{16} \\\hline
    $8 \times 1$ & \matmul{32}{8}{8} \\\hline
    $1 \times 8$ & \matmul{8}{8}{32} \\\hline
  \end{tabular}
  \caption[Experiment Setup: Multiple Layouts, Multiple Accumulation]{Accumulator layouts and the corresponding vector-size-matched computation for the \code{half} type.}
  \label{tab:mlmaSetup}
\end{table}

Applying this to the \code{half} type as an example, we produce the dimensions seen in \rtab{mlmaSetup}.
We can see that for small accumulator layout dimensions, the computation size is dictacted by the vector size ($8 \times 16 \text{ bits} = 128 \text{ bits} = \text{vreg\_size}$).
The larger accumulator layout dimensions, however, eventually override this ($32 \times 16 \text{ bits} = 512 \text{ bits} = 4 \times \text{vreg\_size}$).

This experiment expects to see the \gls{vectorisation} method perform better overall due to better register usage.
The provided method has slightly more complicated predictions.
Small accumulator layouts for small types should perform worse due to the serialisation of accumulations along both dimensions with reduced amortisation through pipelining accumulation into multiple accumulators.
This effect should be reduced as type size increases, eventually making inputs nearly the same size as in \rsec{mlsa} and thereby reducing the number of serialised accumulations.
Therefore, results for larger types should reduce to close to those seen in \rsec{mlsaAnalysis}.

\subsubsection{Analysis}
\begin{itemize}
  \item Results should match expectations, any deviations are interesting talking points.
\end{itemize}

\subsection{Multiple layouts, Fixed Size}
\bk{
  Is there value in doing the same iteration of layout sizes but with a fixed input size.
  For example, for \code{float} choose \matmul{64}{8}{64} which offers a horizontal and vertical accumulator tiling of (16, 16) and an accumulation count of 8.
  This would magnify the speedup from using multiple accumulators but also magnify the slowdown from the serialisation of small layouts.
}

\section{More experiments}
\begin{itemize}
  \item Comparison between:
  \begin{itemize}
    \item Matmul with default llvm code path.
    \item Matmul with default llvm intrinsic.
    \item Matmul with llvm intrinsic + mma.
    \item Matmul with libraries.
    \item Matmul on GPU?
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item Small matrices vs big matrices (call costs).
\end{itemize}

\end{document}
