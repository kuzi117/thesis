% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Evaluation}
\label{cha:evaluation}
This section presents an evaluation of the implementation developed in \rcha{method}.
The results presented also represent one of the first evaluations of \gls{power10}'s \gls{mma} facility.
Given that the evaluation was performed on test~\bk{There's a better word for this, I'm sure.} hardware, the results are expected to be representative of the hardware that will eventually be available to consumers but not exactly the same.
This could be due to anything from frequency changes to firmware changes or actual silicon changes.

\section{Experimental Methodology}
\begin{itemize}
  \item
    Machine info:
    \begin{itemize}
      \item CPU used (P10 obviously).
      \item Frequencies (if we can?).
      \item Cache properties (hopefully available).
      \item Main memory properties.
    \end{itemize}
  \item
    Experimental procedure.
    \begin{itemize}
      \item Run count.
      \item Run order.
    \end{itemize}
  \item
    LLVM info.
      \begin{itemize}
        \item All matrix inputs to \code{llvm.matrix.multiply} are column major.
        \item Could be row major.
      \end{itemize}
\end{itemize}

\section{Caveats}
The results presented in \todo{link future results section} are unexpected and less than ideal.
However, the cause of the lackluster performance, in general, is not a result of the implementation presented in \rcha{method}.
Instead, missed performance can be traced to several factors in the backend code generator as well as limitations inherent in the surrounding framework.

Before presenting the results, the sources of performance degredations are explained.
While the results are counterintuitive, it must be stressed that these factors are addressable and represent opportunties for improving code generation for the entire \gls{power} ecosystem.
Because this is a new type of kernel being generated by the backend, the assembly code generation has not had the chance to be improved as other kernels have been.
It is only after stripping away certain bottlenecks present in the original kernel that we learn which other bottlenecks remain.

\subsection{Spilling}
\glslink{spill}{Spilling} occurs when new values must be brought from memory but all register's at that \gls{point} in the program have a \gls{live} value.
To make space for the new value, one register must be \glslink{spill}{spilled} to memory.
For small kernels, all values may be resident in register simultaneously or may only be used once and so do no remain live across future loads.
There is not enough register space for large kernels to have all operands and results simultaneously in register.
This issue is solveable with careful scheduling but is exacerbated by the following issues.

\subsubsection{Framework Vector Loads}
\label{sec:test}
The implementation described in \rcha{method} integrates itself within a pre-existing framework which provides the default vectorisation lowering method described in \rsec{matMulInt}.
The data structures for working with matrices that were created as part of the framework take the load of a single long vector representing a flattened matrix and replace it with several vector loads representing the matrix's major axis values.
For example, a floating-point column-major matrix \mat{A}{4}{2} will originally be represented by \code{<8 x float>} and will be transformed into two \code{<4 x float>} column-vector loads.
Regardless of the form this loading takes -- be it one large load or several smaller, consecutive loads -- this load occurs in the IR before all computation.
By examining the generated assembly, \bk{Should I attach an appendix with example code? It seems excessive.} we can, for large kernels, see that this sort of code has been interpreted by the backend as a request to bring \emph{all} of the values to register immediately.
This translates to the entire matrix being loaded consecutively and, because the matrix may be larger than all available registers and values from other matrices must be present, it results in these values immediately being \glslink{spill}{spilled} back to memory.
These values must be reloaded later when it is their time to be used, effectively tripling the kernel's time spent on memory accesses.
The memory operations are considerably slower than \gls{mma} operations, even though the values may be going to and from L1 cache, and thus the runtime of the kernel becomes dominated by memory operations.

\begin{figure}
  \centering
  \input{figures/sinkFloats.pgf}
  \caption{The effect of load sinking on floating point matrix multiplications.}
  \label{fig:floatSink}
\end{figure}

This issue should be a consideration of the backend scheduler\footnotemark: moving loads for values that are not needed at the current \gls{point} in the schedule to another \gls{point} closer to its first use where there is potentially less register pressure (load ``sinking'').
\footnotetext{This issue is actively being investigated as part of future work.}
In general, for the average program, this may not be the case, but in the case of this matrix multiplication kernel it is well within reason.
As a stop-gap measure, this can be addressed in the IR by breaking up larger loads and sinking them closer to their uses.

A short, specifically-targeted optimisation was created to implement this idea to test the feasibility of presenting more accurate\footnotemark results in this manuscripts; a representative sample of the effects are shown in \rfig{floatSink}.
\footnotetext{Here, ``accurate results'' means ``optimal results given a more well-behaved backend code generator''.}
Consider the pair of bars labeled ``8x8x16'' (\matmul{8}{8}{16}).
This represents the best case scenario for such an optimisation:
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}, label=\textbf{(\arabic*)}, after={.}]
  \item $A$'s eight-element column-vector load can be broken up in to two four-element vector loads, each of which fills an entire VSR (similar logic applies to $B$'s rows)
  \item only a single load is necessary to create an outer product operand (rank-one update)
  \item each operand is used only during its accumulation step ($2+4=6$ registers \gls{live} at once)
\end{enumerate*}
\bk{If plots are updated, update these numbers.}
Given such a perfect case, the runtime of the original verison (labeled ``No Opt'') is roughly quartered\footnotemark in the optimised version (labeled ``Sink'').
If the operation were to have more accumulations then this gap would continue to grow as more and more spills are required in the unoptimised version.
\footnotetext{Changed from 113.6 ns to 28.5 ns on average.}

\subsubsection{Operand Spilling and Rematerialisation}
In solving the first issue, another, separate but related issue is born.
As can be seen from the pair of bars labeled ``32x8x32'', the same fix that worked for a smaller kernel produced a roughly 12\% increase\footnotemark in runtime for the larger kernel.
\footnotetext{Changed from 710.4 ns to 796.0 ns on average.}
Once again, by examining the assembly, it is easy to determine that original issue derived from the framework that created a large number of loads and spills in the beginning of the kernel remains solved.
However, where there once was no spills throughout the body of the kernel, the larger kernel has multiple spills or reloads per set of outer products.
The issue here is that, in the larger kernel, \textbf{(3)} no longer holds true.

For example, in \rfig{opReuse}, an operand from $A$ (diagonal lines) must be used in the accumulation of the first set of accumulators (crosshatch).
The first pair of bars in \rfig{floatSink} stop here.
The larger kernel, however, must use the value again in computing the second set of accumulators (dots).

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[scale=1/2]
    \draw[xstep=1, ystep=2, shift={(0, 0)}] (0, 0) grid +(2, 8);
    \draw[pattern=north west lines, shift={(0, 6)}] (0, 0) rectangle +(1, 2);
    \draw[pattern={Hatch[angle=45,distance={8pt},xshift=.2pt, line width=1.25]}, xstep=2, ystep=2, shift={(3, 6)}] (0, 0) rectangle +(8, 2);
    \draw[pattern={Dots[angle=45,distance={7pt},xshift=.1pt, radius=1.25]}, xstep=2, ystep=2, shift={(11, 6)}] (0, 0) rectangle +(8, 2);
    \draw[xstep=2, ystep=2, shift={(3, 0)}, color={black!20!white}] (0, 0) grid +(16, 8);
    \draw[xstep=8, ystep=4, shift={(3, 0)}] (0, 0) grid +(16, 8);
    \draw[decorate, decoration={brace, mirror}] (0, -0.3) -- node[below=2] {$K$} +(2, 0);
  \end{tikzpicture}
  \caption{Demonstration of operand reuse.}
  \label{fig:opReuse}
\end{figure}

According to constraint \fbox{5} in \rsec{baseCase}, spilling an accumulator is expensive, and we must therefore fully compute an accumulator before moving on.
This means that every operand along the dimension $K$ will be brought to register before an operand will be reused.
Given only 32 registers and requiring six for each accumulation, if $K \geq \left\lceil \frac{32}{6} \right\rceil = 6$, then there will not be enough space for all values to remain in register\footnotemark.
\footnotetext{The issue is identical for operands of $B$, though the reuse distance is greater.}
And thus the cause becomes apparent: operands that are still \gls{live} are being forced out of registers and the compiler is choosing to spill them.

This is the well studied compiler problem of ``\gls{rematerialisation}''.
Essentially, the compiler must answer the question ``is it more efficient to store a value to memory and retrieve it later (\gls{spill}) or to recompute it later using the same process?''
In the case of floats, producing the operand is a simple load and therefore, if an operand \emph{must} be evicted from its register, \glslink{rematerialisation}{rematerialising} the value is a simple load.
Thus, the compiler's choice to spill the operand is incorrect and, because spills are saved on the stack, not only is it wasting memory by duplicating the value, it is potentially evicting values from the cache.

\section{MMA vs Vectorization}
\label{sec:mmaVsVec}
To begin, we consider a performance comparison between different methods of \gls{lowering} the \gls{llvm} intrinsic \code{llvm.matrix.multiply}.
The first lowering is the current state-of-the-art lowering provided by \gls{llvm} using \gls{vectorisation} while the second method uses the \gls{lowering} method contributed as part of this thesis (\todo{rsec method}).
Such a comparison will help us investigate several research questions:
\begin{enumerate*}[itemjoin*={{ and }}, label=\textbf{(\arabic*)}]
  \item can \gls{mma} provide speedup over \gls{vectorisation}?
  \item which accumulator layout offers the greatest speedup?
  \item do some data types offer greater speedup?
\end{enumerate*}

We must first acknowledge restrictions on inputs as described in \rsec{restrictions}.
When working with each data type, the input matrices must have dimensions which are multiples of the appropriate argument dimension.
For example, when considering \code{xvi4ger8}, the $A$ matrix must have a vertical dimension which is a multiple of four and a horizontal dimension which is a multiple of eight.
For the $B$ matrix, the restrictions are transposed with the dimensions.

Further restriction comes from the hardware accumulator count.
Through the magic of an \gls{ir}, it is possible to have more than eight accumulators simultaneously active, but doing so will necessarily cause a resident accumulator to \gls{spill} to memory during \gls{lowering}.
Therefore, experimentation must also be restricted to eight or less accumulators so as to not cause unneccessary memory operations to confound results.
With this knowledge in mind, we perform two experiments in order to gain insight into the questions listed above.

\subsection{Multiple Layouts, Single Accumulation}
\label{sec:mlsa}
In the first, we vary the accumulator tiling layout but supply inputs which cause only a single operation to be accumulated into each accumulator.
We perform this test for all investigated data types (\todo{see table/section of types}) in combination with every layout whose dimensions are a power of two.
Specifically, given an \gls{mma} instruction whose input dimension is $(i, j)$ (\eg $(4, 8)$ for \code{i4}, see \rtab{mmaInsts}), to create accumulator layout $m \times n$:

\begin{equation}
  \begin{split}
    a = i * m, \text{ } b &= i * n, \text{ } c = j\\
    \underset{(a \times c)}{A} \times \underset{(c \times b)}{B} &= \underset{(a \times b)}{C}
  \end{split}
\end{equation}

\begin{table}[t]
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline
    $1 \times 1$ & \matmul{4}{8}{4} \\\hline
    $2 \times 1$ & \matmul{8}{8}{4} \\\hline
    $1 \times 2$ & \matmul{4}{8}{8} \\\hline
    $2 \times 2$ & \matmul{8}{8}{8} \\\hline
    $4 \times 1$ & \matmul{16}{8}{4} \\\hline
  \end{tabular}
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline
    $1 \times 4$ & \matmul{4}{8}{16} \\\hline
    $4 \times 2$ & \matmul{16}{8}{8} \\\hline
    $2 \times 4$ & \matmul{8}{8}{16} \\\hline
    $8 \times 1$ & \matmul{32}{8}{4} \\\hline
    $1 \times 8$ & \matmul{4}{8}{32} \\\hline
  \end{tabular}
  \caption[Experiment Setup: Multiple Layouts, Single Accumulation]{Accumulator layouts and the corresponding size-matched computation for the \code{i4} type.}
  \label{tab:mlsaSetup}
\end{table}

This equation is applied in \rtab{mlsaSetup} to generate all of the tested layouts for the \code{i4} type.
For example, using a tiling layout of $4 \times 1$ accumulators with the \code{i4} type requires a computation of \matmul{(4 \times 4)}{8}{(4 \times 1)}.

\todo{Find a better way to refer to the contributed method.}
The expectation for performance within the provided \gls{lowering} method is that the greatest speedup should be achieved when using all eight accumulators in a $4 \times 2$ or $2 \times 4$ layout.
When comparing the two implementations, for small data types (less than 32 bits), there is a potential that vectorisation is slower than the contributed method, even with a suboptimal accumulator configuration.
A na\"ive implementation will not see the potential for concatenating two input vectors while performing a product and will therefore waste computation potential by not filling all register elements.
Concretely, using a $1 \times 1$ layout of accumulators with the \code{i16} type produces the computation \matmul{4}{2}{4}.
Even though the entire matrix $A$ fits into a single vector register ($4 \times 2 \times 16 \text{ bits} = 128\text{ bits}$), a single column of $A$ fills only half ($4 \times 16 \text{ bits} = 64 \text{ bits}$).
Given this, the implementation using \gls{mma} will use the whole input in a single operation while the \gls{vectorisation} option will use half of the input in multiple computations.

\subsubsection{Analysis}
\label{sec:mlsaAnalysis}
\begin{itemize}
  \item Examining only the mma speedups should show an optimal layout using all eight accumulators in some orientation
  \item
    Comparison between mma and vectorisation of types $<32$ bit: vectorisation should be slower due to extra/wasted operations since column size doesn't match vector size.
    If mma \emph{is} slower then the cost of the operation is too high while vectorisation could overcome wasted data through pipelining.
  \item
    Comparison between mma and vectorisation of types $\geq 32$ bit: vectorisation should be faster for small accumulator layouts but slower for full accumulator layouts.
    Reference optimal layout size from previous examination.
\end{itemize}

\subsection{Multiple Layouts, Multiple Accumulation}
\label{sec:mlma}
In a second experiment, we find the lowest common multiple of vector register length and dimension induced by accumulator layout and use this as the input dimension.
In this way we attempt to remove all disadvantages to the \gls{vectorisation} method while still maintaining a low computation count for the contributed method.
Again, we perform this for all investigated data types in combination with every layout whose dimensions are a power of two.
To produce the dimensions for a given given accumulator layout $m \times n$, with \gls{mma} instruction who input dimension is $(i, j)$ and data type size in bits is $s$:

\begin{equation}
  \begin{split}
    \text{vreg\_elems} &= 128\text{ bits} / s\\
    \text{a} &= \text{lcm}(\text{vreg\_elems}, i * m)\\
    \text{b} &= \text{lcm}(\text{vreg\_elems}, i * n)\\
    \text{c} &= \text{lcm}(\text{vreg\_elems}, j)\\
    \underset{(a \times c)}{A} \times \underset{(c \times b)}{B} &= \underset{(a \times b)}{C}\\
  \end{split}
\end{equation}

\begin{table}[t]
  \centering
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline$1 \times 1$ & \matmul{8}{8}{8} \\\hline
    $2 \times 1$ & \matmul{8}{8}{8} \\\hline
    $1 \times 2$ & \matmul{8}{8}{8} \\\hline
    $2 \times 2$ & \matmul{8}{8}{8} \\\hline
    $4 \times 1$ & \matmul{16}{8}{8} \\\hline
  \end{tabular}
  \begin{tabular}{| c | c | c |}
    \hline
    {\small Acc. Layout} & Computation \\\hline
    $1 \times 4$ & \matmul{8}{8}{16} \\\hline
    $4 \times 2$ & \matmul{16}{8}{8} \\\hline
    $2 \times 4$ & \matmul{8}{8}{16} \\\hline
    $8 \times 1$ & \matmul{32}{8}{8} \\\hline
    $1 \times 8$ & \matmul{8}{8}{32} \\\hline
  \end{tabular}
  \caption[Experiment Setup: Multiple Layouts, Multiple Accumulation]{Accumulator layouts and the corresponding vector-size-matched computation for the \code{half} type.}
  \label{tab:mlmaSetup}
\end{table}

Applying this to the \code{half} type as an example, we produce the dimensions seen in \rtab{mlmaSetup}.
We can see that for small accumulator layout dimensions, the computation size is dictacted by the vector size ($8 \times 16 \text{ bits} = 128 \text{ bits} = \text{vreg\_size}$).
The larger accumulator layout dimensions, however, eventually override this ($32 \times 16 \text{ bits} = 512 \text{ bits} = 4 \times \text{vreg\_size}$).

This experiment expects to see the \gls{vectorisation} method perform better overall due to better register usage.
The provided method has slightly more complicated predictions.
Small accumulator layouts for small types should perform worse due to the serialisation of accumulations along both dimensions with reduced amortisation through pipelining accumulation into multiple accumulators.
This effect should be reduced as type size increases, eventually making inputs nearly the same size as in \rsec{mlsa} and thereby reducing the number of serialised accumulations.
Therefore, results for larger types should reduce to close to those seen in \rsec{mlsaAnalysis}.

\subsubsection{Analysis}
\begin{itemize}
  \item Results should match expectations, any deviations are interesting talking points.
\end{itemize}

\subsection{Multiple layouts, Fixed Size}
\bk{
  Is there value in doing the same iteration of layout sizes but with a fixed input size.
  For example, for \code{float} choose \matmul{64}{8}{64} which offers a horizontal and vertical accumulator tiling of (16, 16) and an accumulation count of 8.
  This would magnify the speedup from using multiple accumulators but also magnify the slowdown from the serialisation of small layouts.
}

\section{More experiments}
\begin{itemize}
  \item Comparison between:
  \begin{itemize}
    \item Matmul with default llvm code path.
    \item Matmul with default llvm intrinsic.
    \item Matmul with llvm intrinsic + mma.
    \item Matmul with libraries.
    \item Matmul on GPU?
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item Small matrices vs big matrices (call costs).
\end{itemize}

\end{document}
