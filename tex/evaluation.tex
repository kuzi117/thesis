% A workaround to allow relative paths in included subfiles
% that are to be compiled separately
% See https://tex.stackexchange.com/questions/153312/subfiles-inside-a-subfile-using-relative-paths
\providecommand{\main}{..}
\documentclass[\main/thesis.tex]{subfiles}

\begin{document}

\chapter{Evaluation}
\label{cha:evaluation}
This chapter presents an evaluation of the implementation developed in \rcha{method}.
The results presented also represent one of the first evaluations of \gls{power10}'s \gls{mma} facility.
Given that the evaluation was performed on test~\bk{There's a better word for this, I'm sure.} hardware, the results are expected to be representative of the hardware that will eventually be available to consumers but not exactly the same.
This could be due to anything from frequency changes to firmware changes or actual silicon changes.

\section{Experimental Setup}
This section presents details on resources used and processes performed to produce the results in \rsec{caveats} and onwards.

\subsection{Machine Details}
Experiments were conducted on a \gls{power10} machine provided by \gls{ibm}.
The processor is not yet available to the public but relevant details are presented in \rtab{machineInfo}.
The listed core counts are per socket while the thread counts are per core.
The machine runs Linux with a 64-bit kernel at version 5.10.0-17496-g41bc5268c5e8.

\begin{table}[t]
  \centering
  \begin{tabular}{c | c}
    & IBM POWER10\\\hline
    Cores/Threads & 15/8\\
    L1i cache & 720KiB\\
    L1d cache & 480KiB\\
    Frequency & 3.65 GHz\\
  \end{tabular}
  \caption{POWER10 machine info.}
  \label{tab:machineInfo}
\end{table}

\subsection{Compilation}
All binaries are compiled with Clang version 13.0.0\footnotemark at the highest optimisation level (\code{-O3}) and are tuned to the processor (\code{-mcpu=pwr10}).
\footnotetext{At time of writing this is a development version.}
Because support for \gls{power10} was not available in mainstream \gls{llvm} when this work began, it has been implemented as part of \glslink{ibm}{IBM's} variant of \gls{llvm} which did have support.
This variant contains several platform specific, targeted optimisations focused on improving \gls{power} code; these have all been disabled so that results are representative of mainstream \gls{llvm}.

\subsection{Experimental Methodology}
\begin{figure}[t]
  \centering
  \begin{tikzpicture}[scale=1]
    \draw[decorate, decoration={brace}, align=center, shift={(0, 0)}] (0, 0) -- node[above=2] {$n_0$ iters} +(2, 0);
    \draw[decorate, decoration={brace}, align=center, shift={(2.125, 0)}] (0, 0) -- node[above=2] {$n_1$ iters} +(2, 0);
    \node[shift={(4.5, 0)}] {$\ldots$};
    \draw[decorate, decoration={brace}, align=center, shift={(4.875, 0)}] (0, 0) -- node[above=2] {$n_{24}$ iters} +(2, 0);
    \draw[decorate, decoration={brace}, align=center, shift={(1, .8)}] (0, 0) -- node[above=2] {25 repeats} +(4.875, 0);
    \foreach \x in { 0, 2.125, 4.875 }
    {
      \pgfmathsetmacro{\xStart}{\x + 0.1}
      \foreach \i in {0,...,18}
      {
        \draw[shift={(\xStart + \i * 0.1, -0.1)}] (0, 0) -- +(0, -0.3);
      }
    }
  \end{tikzpicture}
  \caption{Experiment execution process.}
  \label{fig:execution}
\end{figure}

Every measurement presented in this chapter was produced using the Google\texttrademark{} benchmark library\footnotemark~\autocite{googlebench}.
\footnotetext{At commit ab74ae5e104f72fa957c1712707a06a781a974a6.}
A sinlge measurement is the result of timing the execution of a kernel many times until statistical stability is obtained.
The framework implements a method of measuring a kernel by placing it in a loop whose exit condition is dynamically determined by the benchmark being executed.
The loop will only exit when the kernel been executed enough times.

To achieve this, a loop trip count, $n$, is determined by
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}, label=(\arabic*), after={\footnotemark.}]
  \item timing several executions of the benchmark
  \item multiplying the required time (set at one second\footnote{One second is double the default, suggested half-second time-limit.}) by $1.4$
  \item dividing by the average execution time of those executions
\end{enumerate*}
\footnotetext{This process can be verified in any upcoming table using $n \times \text{cpuTime} \approx 1.4e9$.}
The true process is more iterative and tries mulitple values in order to produce a conservative estimate for benchmarks with variation in their execution times.
For the kernels benchmarked in this chapter, which have very little variation, this is an accurate summary of the process.

\bk{Double check these numbers when they're finalised.}
While a single second may seem short, for the longest-running kernel presented (\atilde\SI{81}{\textit{\micro\second}}), this still results in roughly 17000 executions.
For the shortest-running kernel (\atilde\SI{14}{\textit{\nano\second}}), the kernel is executed nearly 100 million times.

In \rfig{execution}, producing one measurement (\ie completing $n$ iterations of the loop) corresponds to a single of the lower braces labeled ``$n_i$ iters''.
A total of 25 measurements are produced in this way, corresponding to the upper brace in \rfig{execution}, to reduce the impact of system fluctuations in any execution.
Repetitions vary $n$ slightly because the timing iterations may produce slightly different base times, changing the iteration count.

The framework presents statistics for each of the 10 loops including wall clock time, \gls{cpu} time, and cycles.
\bk{I think I can add instructions retired if it's a good idea.}
It also presents overall statistics between each of the ten runs: mean, median, and standard deviation.
These overall statistics are the values presented throughout the coming sections.
Vertical bars in plots represent the overall mean while error bars represent the 95\% confidence interval derived from the standard deviation.
Tables accompany each plot with execution count ($n$), mean \gls{cpu} time, mean cycles, and their respective confidence intervals.

\subsection{Types Missing From Analysis}
Three types are missing from the following analysis for different reasons.
The \code{i4} and \code{bfloat} types use the same \gls{ir} code generation code paths as every other type except \code{double}.
Given that the implemented logic works for \code{i8}, \code{i16}, \code{half}, and \code{float}, it is likely that the code generated for them is functional and as performant as the other types.
However, while translating to assembly, the backend produces either incorrect code (\code{i4}) or an error (\code{bfloat}).
Both issues have been acknowledged by the developer team and will be fixed in future versions of \gls{llvm}.

For \code{double}, the changed operation dimensions cause a signficant divergence in code generation logic that otherwise applies to all other types usable with \gls{mma}.
Due to the constraints of the surrounding framework, it was decided early on that development efforts should be focused on an algorithm that worked for six of the seven types.
The method presented in \rcha{method} can easily be applied to \code{float} but its \gls{lowering} implementation will be a noticeable variant of the original code.

\section{Caveats}
\label{sec:caveats}
The results presented in \todo{link future results section} are unexpected and less than ideal.
However, the cause of the lackluster performance, in general, is not a result of the implementation presented in \rcha{method}.
Instead, missed performance can be traced to several factors in the backend code generator as well as limitations inherent in the surrounding framework.

Before presenting the results, this section explains sources of performance degradations.
The results are counterintuitive, but the issues they reveal are addressable in the future and represent opportunities for improving code generation for the entire \gls{power} software stack.
The presented \gls{mma} kernel is a new type of kernel never before seen by the backend and therefore the assembly code generation has not been as tuned as other kernels have been.
Only after stripping away certain bottlenecks present in the naive kernel can we learn about new issues that are still to be addressed by the backend design team.

\subsection{Spilling}
In compiler terminology, a \gls{live} value is a value that may be used by a statement in the future.
Ideally, once a value is brought from memory into a register or it is computed and placed into a register, it should remain in the register until it is \gls{dead} --- a value is dead when it is guaranteed that it will never be used again.
However, there are often not enough registers available in an architecture to keep all live values in register.
Therefore, the compiler performs analysis to decide how to judiciously select some live values to be temporarily \glslink{spill}{spilled} to memory in order to temporarily free registers for new values.
Thus, \glslink{spill}{spilling} occurs when new values must be brought from memory but all registers at that \gls{point} in the program have a \gls{live} value.
For small matrix-multiplication kernels, all values may be resident in register simultaneously or may only be used once and thus do no remain \gls{live} across future loads.
Efficient spilling requires careful scheduling but in the case of \gls{mma} the problem is made more difficult by the following issues.

\subsubsection{Framework Vector Loads}
The implementation described in \rcha{method} is integrated within a pre-existing framework that provides the default vectorisation lowering method described in \rsec{matMulInt}.
This framework contains data structures to support operations with matrices.
These data structures transform the load of a single long vector representing a flattened matrix into several vector loads representing the matrix's column or row vectors, depending on the original access order.
For example, a floating-point column-major matrix \mat{A}{4}{2} is originally represented by \code{<8 x float>} and then transformed into two \code{<4 x float>} column-vector loads.
After the transformation, these load instructions in the \gls{ir} remain consecutive and occur before any computation.
An examination of the assembly code generated by the backend reveals that such a program is translated by the backend into assembly that brings \emph{all} of the matrix's elements to registers immediately and consecutively.
Given that
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; or }}, label=(\arabic*), after={,}]
  \item the matrix may be larger than all available registers
  \item some registers may contain values from other matrices
\end{enumerate*}
many of the loaded values are immediately \glslink{spill}{spilled} back to memory.
These values must be reloaded later when they are needed, effectively tripling the time spent on memory accesses.
Memory operations are considerably slower than \gls{mma} operations, even though the values may be going to and from the L1 cache, and thus the runtime of the kernel becomes dominated by memory operations.

\begin{figure}[t]
  \centering
  \input{tex/figures/sinkFloats.pgf}
  \caption{The effect of load sinking on \code{float} and \code{i16} matrix multiplications.}
  \label{fig:floatSink}
\end{figure}

\begin{table}[t]
  \centering
  \makebox[\textwidth][c]{
    \begin{tabular}{| c | c | c | c |}
      \hline
      Name & $n$ & CPU Time (\SI{}{\textit{\nano\second}}) & Cycles\\\hline
      \code{float}, $8 \times 8 \times 16$, No Load Sinking & $25623185.92 \pm 623715.04$ & $54.84 \pm 1.39$ & $219.11 \pm 5.57$ \\\hline
      \code{float}, $8 \times 8 \times 16$, Load Sinking & $97308628.60 \pm 144463.04$ & $14.44 \pm 0.05$ & $57.67 \pm 0.19$ \\\hline
      \code{float}, $32 \times 8 \times 32$, No Load Sinking & $2146145.16 \pm 50283.11$ & $654.30 \pm 14.86$ & $2614.07 \pm 59.39$ \\\hline
      \code{float}, $32 \times 8 \times 32$, Load Sinking & $2352684.80 \pm 51746.61$ & $596.81 \pm 13.12$ & $2384.39 \pm 52.42$ \\\hline
      \code{i16}, $8 \times 8 \times 16$, No Load Sinking & $91419216.40 \pm 35690.47$ & $15.31 \pm 0.00$ & $61.18 \pm 0.02$ \\\hline
      \code{i16}, $8 \times 8 \times 16$, Load Sinking & $80431238.20 \pm 148030.83$ & $17.42 \pm 0.01$ & $69.61 \pm 0.03$ \\\hline
      \code{i16}, $32 \times 8 \times 32$, No Load Sinking & $3368892.16 \pm 18135.72$ & $415.63 \pm 2.21$ & $1660.53 \pm 8.85$ \\\hline
      \code{i16}, $32 \times 8 \times 32$, Load Sinking & $2230348.60 \pm 7729.44$ & $627.73 \pm 2.19$ & $2507.92 \pm 8.75$ \\\hline
    \end{tabular}
  }
  \caption{
    The effect of load sinking on \code{float} and \code{i16} matrix multiplications.
    See \rfig{floatSink} for graphical presentation.
    \bk{Is this better with just minimum iteration count?}
  }
  \label{tab:floatSink}
\end{table}

This spilling issue must be resolved by the backend scheduler\footnotemark: the scheduler needs to move loads closer to the \gls{point} in the program that contains the first use of the loaded value to reduce register pressure by using a technique called ``load sinking''.
\footnotetext{The backend design team is currently investigating how to implement this change to the scheduler.}
Load sinking may not reduce register pressure in many other programs, but in the case of this matrix-multiplication kernel it certainly does.
A workaround for the spilling resulting from unsunk loads, which can be used until the backend scheduler improves, consists of modifying the \gls{ir} by further breaking up the vector loads into smaller vector loads and sinking these smaller loads closer to the first use of the loaded values.

In preparation for the performance evaluation in this thesis, a short targeted \gls{ir} transformation was implemented using this tactic.
Its goal was to produce results that are more representative of the code that will be generated by a final production-ready backend.
Consider the pair of bars labeled ``8x8x16'' (\matmul{8}{8}{16}) for \code{float} (left) in \rfig{floatSink}.
This matrix size represents the best case scenario for such an optimisation:
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}, label={\protect\circled{\arabic*}}, after={.}]
  \item $A$'s eight-element column-vector load can be broken up into two four-element vector loads, each of which fills an entire \gls{vsr} (similar logic applies to $B$'s rows)
  \item only a single load is necessary to create an outer-product operand (rank-one update)
  \item each operand is used only once during the accumulation step ($2+4=6$ registers \gls{live} at once)
\end{enumerate*}
\bk{If plots are updated, update these numbers.}
Given such a perfect case, the runtime of the original version (labeled ``No Load Sinking'') is roughly quartered (\rtab{floatSink}, rows 1 and 2) in the optimised version (labeled ``Load Sinking'').
If the operation were to have more accumulations, then this gap would continue to grow as more and more spills would be required in the unoptimised version.

However, by examining the bars labeled ``8x8x16'' for \code{i16} (right) in \rfig{floatSink} we can see that for smaller types this optimisation actually causes a slight slow down (\rtab{floatSink}, rows 5 and 6).
This can be attributed to invalidating properties \circled{1} and \circled{2}: loading an \code{i16} operand no longer fills an entire register and two loads are necessary to create an operand for \code{i16} (rank-two update).
In fact, the assembly produced by the unoptimised version is much closer to the optimal handwritten version because it loads multiple vectors into a single register and uses these as source vectors for multiple shuffle instructions.
The ``optimised'' version loads only a single vector into a register, increasing register pressure, and uses each vector for only a single shuffle instruction.
It is only because the overall register pressure has been decreased by halving the element width that the performance of these two kernels show only a slight difference.

\subsubsection{Operand Spilling and Rematerialisation}
After addressing the load-sinking issue in this way, another, separate but related, issue appears.
As can be seen from the pair of bars labeled ``32x8x32'' for \code{float} in \rfig{floatSink}, the same fix that produced significant speedup for a smaller kernel has significantly reduced in effectiveness for a larger kernel.
An examination of the assembly reveals that the large number of loads that led to spills at the beginning of the kernel prior to sinking the loads are indeed no longer there.
However, while there are no spills throughout the body of the smaller kernel, the larger kernel has multiple spills or reloads per set of outer products.
The issue here is that, for the larger kernel, property \circled{3} no longer holds true.

Consider the example in \rfig{opReuse}.
The figure shows an example large kernel ($16 \times 32$) in the style of \rfig{intrinsic}; to illustrate how spills occur, the figure does not divide individual elements nor does it show $B$.
On the right, darker lines divide the output into blocks of accumulators while the lighter lines divide blocks into the eight accumulators as in \rfig{intrinsic}.
Each of the smaller boxes on the left represents an operand composed of four \code{float} elements used in an outer-product computation.

An example operand from $A$, highlighted by parallel diagonal lines, must be used in the accumulation of the first set of accumulators, highlighted by a crosshatch pattern.
In the computation of the smaller kernel in \rfig{floatSink}, this value is now \gls{dead}.
The larger kernel, however, must use the value again in computing the second set of accumulators, highlighted by a dotted pattern.

\begin{figure}[t]
  \centering
  \begin{tikzpicture}[scale=1/2]
    \draw[xstep=1, ystep=2, shift={(0, 0)}] (0, 0) grid +(2, 8);
    \draw[pattern=north west lines, shift={(0, 6)}] (0, 0) rectangle +(1, 2);
    \draw[pattern=crosshatch, xstep=2, ystep=2, shift={(3, 6)}] (0, 0) rectangle +(8, 2);
 %   \draw[pattern={Hatch[angle=45,distance={8pt},xshift=.2pt, line width=1.25]}, xstep=2, ystep=2, shift={(3, 6)}] (0, 0) rectangle +(8, 2);
    \draw[pattern=dots, xstep=2, ystep=2, shift={(11, 6)}] (0, 0) rectangle +(8, 2);
 %   \draw[pattern={Dots[angle=45,distance={7pt},xshift=.1pt, radius=1.25]}, xstep=2, ystep=2, shift={(11, 6)}] (0, 0) rectangle +(8, 2);
    \draw[xstep=2, ystep=2, shift={(3, 0)}, color={black!20!white}] (0, 0) grid +(16, 8);
    \draw[xstep=8, ystep=4, shift={(3, 0)}] (0, 0) grid +(16, 8);
    \draw[decorate, decoration={brace, mirror}] (0, -0.3) -- node[below=2] {$K$} +(2, 0);
  \end{tikzpicture}
  \caption{Demonstration of operand reuse.}
  \label{fig:opReuse}
\end{figure}

According to constraint \fbox{5} in \rsec{baseCase}, spilling an accumulator is expensive, and therefore an efficient solution must finish computing a block of accumulators before moving on to a different area of the matrix.
\nelson{Not clear what "before an operand" means here.}
\bk{
  Is this clear now?
  There was a typo before but I've also cleared up some other things.
}
Doing so requires that every operand along the dimension $K$ be brought to register before an operand can be reused with a new block of accumulators.
For instance, an accumulator layout like that in \rfig{opReuse} requires six operand registers for each accumulation to be performed.
Using only the 32 registers available while the accumulators are assembled, if $K \geq \left\lceil \frac{32}{6} \right\rceil = 6$, then there is not enough space for all values to remain in registers.
And thus the cause for the multiple spills and reloads throughout the computation becomes apparent: operands from $A$\footnotemark that are still \gls{live} are being forced out of registers and the compiler is choosing to spill them.
\footnotetext{The issue is identical for operands of $B$, though the reuse distance is greater.}

\bk{I've changed the discussion of rematerialisation, let me know if it's better.}
This code generation evokes the well-studied compiler technique ``\gls{rematerialisation}''.
Essentially, the compiler analysis must answer the question ``is it more efficient to store a value to memory and retrieve it later (\gls{spill}) or to recompute it later using the same process?''

In the case of floats, producing the operand is a simple load and therefore, if an operand \emph{must} be evicted from its register, \glslink{rematerialisation}{rematerialising} the value is a simple load.
Thus, the compiler's choice to spill the operand is incorrect and, because spills are saved on the stack, not only is it wasting memory by duplicating the value, it is potentially evicting values from the cache.
Regardless, the larger kernel's execution, optimised or not, is signficantly influenced by memory operations, though the gains from reducing the initial spills still have significant impact.

For smaller datatypes however, spilling is the correct choice.
Materialising an \code{i16} involves two loads and at least one instruction to shuffle the loaded data together.
Rematerialising the value in the future becomes a simple load, significantly reducing register pressure and required memory bandwidth.
Both the unoptimised and load-sinking version correctly choose to spill their values instead of \glslink{rematerialisation}{rematerialising} them.
As with \code{float}, the increased number of memory operations explains the significant increase in execution time for the larger \code{i16} kernel in \rfig{floatSink}, but it does not, however, explain the large difference between the two version.
The next section investigates this discrepancy.

\subsubsection{Smaller Types}
While the issues affecting \code{float} matrices presented above remain roughly identical for \code{double}-typed operations, small datatypes have a separate source of slow down.
The difference in performance is actually an exacerbation of an already-discussed issue, namely the invalidation of properties \circled{1} and \circled{2}.
For \code{i16}, in the load-sinking version, loads are performed four \code{i16} elements at the same time using the 64-bit doubleword load.
The unoptimised version loads 16 elements at a time using 256-bit vector-pair loads, meaning that the load-sinking version quadruples the number of ``necessary'' loads (excluding loads from the original spilling issue).
If the scheduling were fixed, removing the unnecessary loads and stores, it is likely the performance gap would be much more significant.

This issue extends to the other data types which perform rank-two, -four, or -eight updates.
The issue is aggravated as the number of memory requests increase due to shrinking data width and a greater required number of vectors for shuffling.
For example, where, for \code{i16}, the load-sinking version has roughly 114\% of the execution time of the unoptimised version, the same comparison for \code{i8} shows the load-sinking version has 292\% of the unoptimised version's execution time.

A more complicated load breaking and sinking strategy may counteract this slowdown, but only to a certain degree.
Combining certain loads -- for example two four-element \code{i16} column loads, making a full \gls{vsr} -- would decrease register pressure because the upper and lower halves of the register can be used in separate shuffles.
However, this still does not quite match the 256-bit paired-vector load in terms of memory-request efficiency.
It is impossible for smaller types to use this 256-bit load because the columns of $A$ are consecutive: the paired-vector load would load values that are not used until the second row of accumulator blocks, inevitably introducing new spills.

The issue worsens for \code{i8} and \code{i4} where the eight consecutive vertical elements fill less and less of a \gls{vsr}.
For larger kernels, the smaller data types thus imply a increasing number of loads, further diluting the speedup gained from \gls{mma}.

\subsection{Shuffling}
For datatypes smaller than 32 bits, producing the appropriate operand for a rank-$r$ update requires several vectors be merged via shuffling.
The \gls{power} \gls{isa} offers several methods to move vector elements, ranging from register shifting and rotating combined with masking to efficient vector upper and lower half merges based on datatype size.
Transforming \gls{ir} statements in an efficient code for shuffling is an involved design process.

There are a multitude ways to express the same shuffling operation in \gls{ir}, each depending on multiple implementation choices made throughout the process.
Certain variants are interpreted by the backend and translated to efficient code while others are interpreted very conservatively.
\nelson{"This"}
This can be the difference between an idiomatic assembly of several instructions, often close to an expert's handwritten version, and the same process being expressed in hundreds or thousands of instructions, often with \glspl{spill} throughout.
Several factors that lead shuffling to cause performance degradation are discussed below.

\subsubsection{Datatype: \texorpdfstring{\code{half}}{half}}
\bk{
  I think this issue is likely to affect \code{bfloat} as well but when I went to try it last night (I'd never compiled any \code{bfloat} code before) it crashed.
  I've spoken to Nemanja and it seems to be a backend issue; I've created a github issue for it and the backend team will look at it shortly.
}
\nelson{It this paragraph simply stating the currently the shuffling is incorrect?}
The \code{i16} and \code{half} types have identical widths: a half word.
Because \glspl{cpu} and their accompanying \glspl{isa} are type-agnostic and sensitive only to the width of a data element when implementing vector shuffles, shuffle code in the \gls{ir} that differs only in choice of 16-bit type should produce identical assembly.
This is demonstrably untrue, however.
The assembly code generated for \code{i16} executes roughly six vector loads and six vector permute instructions to produce all operands for a single accumulation in a full $2 \times 4$ accumulator layout.
For \code{half}, the assembly has extended from six instructions to almost 150 and has been at least partially scalarised, moving elements from vector registers to general purpose registers and back again.

This explosion in code size is most apparent in the calculation epilogue where, in the \gls{ir}, the accumulators are disassembled and stored to memory.
First, the resulting accumulator vectors are truncated to the output data type.
They are then concatenated together into a output flattened matrix and stored to memory.
Concatenation is also achieved through shuffling and therefore experiences the same conservative code generation as before.
The added truncation operation worsens the effect.
All told, the epilogue for \code{half} is roughly ten times the length of same operation for \code{i16}.
\bk{
  I'm concerned there's an extra factor here in the truncation operation that needs to be considered.
  Going from \code{i32} to \code{i16} uses a ``vector pack'' instruction which is effectively just takes two vectors and truncates every word to a half-word and packs both into a single vector.
  I can't find a similar operation for the conversion from \code{f32} to \code{f16} which is a significantly more complicated process because the exponent bias is different.
  This might mean that it \emph{must} be scalarised which, of course, entails a longer epilogue.
}

\subsubsection{Datatype: \texorpdfstring{\code{i4}}{i4}}
For \code{i8}, degradations arise from \glspl{spill} due to extra register pressure but also from needing to merge four separate vectors (rank-four update).
Despite this, generated code is relatively close to the ideal, handwritten assembly.
As mentioned before, there exists instructions for merging vectors at a per-byte granularity, but not a at a sub-byte granularity as is required for \code{i4} code generation in the same style.
This does not mean that it is not possible to shuffle a vector of \code{i4} elements using the same \gls{ir} code generation, only that assembly generated by the backed will be significantly less performant.
Specifically, this includes vector rotations and scalarisation, with single bytes being moved individually to and from general purpose registers.
This slow down negates much of the speed up that could be achieved via the \gls{mma} rank-eight update.

\subsection{Solutions}
This section presents potential solutions though it is unlikely that the ideas introduced will solve the performance degradations without further investigation and cooperation with the backend developers.

\subsubsection{Spilling}
The simplest solution to the noted \glslink{spill}{spilling} issues is for developers to focus on using only small kernels and, indeed, this is not so unreasonable a proposal as one might perceive it at first.
While \gls{llvm} necessitates that certain sequences of \gls{ir} always be able to be lowered to assembly, it does not require that the code be as performant as possible, as is demonstrated by the above issues with shuffling \code{i4} vectors.
This may seem like a defeatist notion but is actually a tried-and-true approach to performance in compilers: make the common case fast.

The average user interacts with a compiler through a high-level language, not its \gls{ir}.
Thus, the frontend will choose the fastest solution to implement a user's code; if the fastest implementation is a small kernel encapsulated by an outer kernel then that is what will be chosen.
With this notion, optimisation can be focused on this most common case for which a solution is known.
A larger kernel will always remain usable but it will be less performant.

Furthermore, the most performant library implementations focus on using small unrolled assembly kernels, often computing kernels with dimensions of four to eight elements.
These small kernels are then surrounded by memory-managing outer kernels as described in \rsec{outerKernel}.
This is exactly what Kuzma \etal~\autocite{kuzma2021fast} do while using the inner kernel presented in this thesis with results comparable to libraries with kernels written in assembly.
In this way, using only smaller kernels can be seen as a well-worn path to performance.

If a larger kernel \emph{must} be used, significant investigation must be done to test load scheduling and to know the cost of operand \gls{rematerialisation}.
It is unlikely that such a solution will be found within the \gls{ir} because it is in \gls{ssa} form.
Because
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}, label=(\arabic*), after={,}]
  \item \gls{ssa} makes it very easy to trace the process behind generating a value, easily identifying identical values
  \item \gls{llvm} automatically folds identical values together
\end{enumerate*}
it is impossible to ``duplicate'' a value in order to manually \glslink{rematerialisation}{rematerialise} a value.
Such a solution will have to be found in the backend.

\subsubsection{Shuffling}
Unlike issues with spilling, it is likely that performance degradations caused by misbehaving shuffles can be solved by creating type specific shuffling code.
While injecting the proposed solution code into the original vectorisation framework is possible, it would be much more effective to first refactor the framework so such a modification is easier.

For \code{half} vectors, it may be possible to fool the compiler into generating code as is done for \code{i16} simply by reinterpreting data as \code{i16} via casts and manipulating the data in this state.
The cast is not a conversion, simply a reinterpretation of the bits in a register, meaningful only within the \gls{llvm} framework and producing no new instructions.
The data can again be casted back to \code{half} at no cost when it needs to be used in a computation.
In theory, this produces much better assembly but may also trigger a very conservative response in the compiler, worsening the code significantly with scalar element movement derived from a perceived conversion hazard.

The sub-byte granularity of \code{i4} vectors pose a different problem.
Performant shuffling code for \code{i4} is nearly an impossibility.
Instead, clever use of bitwise operations will have to explicitly be used to implement efficient element extraction and movement.
Depending on the implementation of cost analysis, this may or may not improve the cost of \gls{rematerialisation}, potentially reducing spills in larger kernels.

\subsubsection{Evaluation Disclaimer}
All evaluation presented in future sections have no targeted optimisations applied.
No tested solution positively affected all cases.
Thus, all test cases use identical \gls{ir} generation logic with no special cases.
The presented results are therefore equally advantaged and disadvantaged in code generation.

\section{More experiments}
\begin{itemize}
  \item Comparison between:
  \begin{itemize}
    \item Matmul with default llvm code path.
    \item Matmul with default llvm intrinsic.
    \item Matmul with llvm intrinsic + mma.
    \item Matmul with libraries.
    \item Matmul on GPU?
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item Small matrices vs big matrices (call costs).
\end{itemize}

\end{document}
